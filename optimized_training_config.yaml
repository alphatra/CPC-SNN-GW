# ğŸš€ OPTIMIZED TRAINING CONFIGURATION - FAST & EFFICIENT
# RozwiÄ…zuje problemy wydajnoÅ›ciowe: 19.5s init â†’ <5s init
# Target: >90% accuracy z szybkÄ… wydajnoÅ›ciÄ…

# ğŸ“Š Experiment Configuration  
experiment:
  name: "cpc_snn_gw_optimized_fast"
  version: "performance_v1.0"
  description: "Performance-optimized training - fast initialization & efficient memory usage"
  target_accuracy: 0.90
  scientific_validation: true

# ğŸŒŠ Memory-Optimized Data Configuration
data:
  # Real LIGO data - optimized parameters
  use_real_ligo_data: true
  sample_rate: 4096
  sequence_length: 128        # âœ… REDUCED: 256â†’128 for faster init
  window_size: 128           # âœ… REDUCED: 256â†’128 for faster init  
  enhanced_overlap: 0.7      # âœ… REDUCED: 0.9â†’0.7 for memory
  num_samples: 1200          # âœ… REDUCED: 2000â†’1200 for faster loading
  data_augmentation: true
  noise_scaling: false       # âœ… DISABLED: for performance
  
  # Quality preprocessing
  min_snr: 5.0
  preprocessing:
    whitening: true
    bandpass_low: 35.0
    bandpass_high: 350.0
    scaling_factor: 1e20

# ğŸ§  Performance-Optimized Model Architecture
model:
  # CPC Encoder - optimized
  cpc:
    latent_dim: 128           # âœ… INCREASED: for better representation
    downsample_factor: 4
    context_length: 128       # âœ… REDUCED: 256â†’128
    prediction_steps: 4       # âœ… REDUCED: 8â†’4
    num_negatives: 4          # âœ… REDUCED: 8â†’4
    temperature: 0.07
    conv_channels: [32, 64, 128]  # âœ… REDUCED: smaller channels
    
    # Simplified Temporal Transformer
    use_temporal_transformer: true
    transformer_num_heads: 2   # âœ… REDUCED: 4â†’2
    transformer_num_layers: 1  # âœ… REDUCED: 2â†’1
    multi_scale_kernels: [3, 5]  # âœ… REDUCED: [3,5,7]â†’[3,5]
    temporal_attention_dropout: 0.1
    use_attention: true         # âœ… ADDED: Attention mechanism for better context
    
  # Optimized Spike Bridge
  spike_bridge:
    encoding_strategy: "phase_preserving"
    use_learnable_thresholds: true
    num_threshold_scales: 2    # âœ… REDUCED: 4â†’2
    threshold_adaptation_rate: 0.01
    use_phase_preserving_encoding: true
    edge_detection_thresholds: 2  # âœ… REDUCED: 4â†’2
    
  # Memory-Optimized SNN Classifier
  snn:
    hidden_sizes: [64, 32]     # âœ… REDUCED: [128,64]â†’[64,32]
    num_classes: 2
    snn_neurons_per_layer: 128 # âœ… CRITICAL REDUCTION: 512â†’128
    snn_num_layers: 2          # âœ… CRITICAL REDUCTION: 4â†’2
    
    # Optimized LIF parameters
    tau_mem: 20e-3            # âœ… SIMPLIFIED: 50e-6â†’20e-3
    tau_syn: 5e-3             # âœ… SIMPLIFIED: 10e-6â†’5e-3
    threshold: 1.0
    use_enhanced_lif: true
    use_refractory_period: false  # âœ… DISABLED: for performance
    use_adaptation: false         # âœ… DISABLED: for performance
    
    # Simplified surrogate gradients
    surrogate_gradient_type: "ADAPTIVE_MULTI_SCALE"
    surrogate_gradient_beta: 2.0  # âœ… REDUCED: 4.0â†’2.0
    surrogate_slope: 2.0          # âœ… ADDED: Required for validation
    curriculum_learning: false    # âœ… DISABLED: for simplicity

# ğŸ¯ Fast Training Configuration
training:
  # Core parameters
  num_epochs: 50
  batch_size: 1              # âœ… SAFE: batch_size=1 for memory safety
  learning_rate: 5e-4
  weight_decay: 1e-4
  use_focal_loss: true        # âœ… ADDED: Focal loss for class imbalance
  
  # Simplified learning rate schedule
  learning_rate_schedule: "cosine_with_warmup"
  warmup_epochs: 1           # âœ… REDUCED: 2â†’1
  warmup_factor: 0.1
  min_learning_rate: 1e-6
  
  # Optimized gradient settings
  gradient_clipping: true
  max_gradient_norm: 1.0
  gradient_accumulation_steps: 2  # âœ… REDUCED: 4â†’2 (effective batch=2)
  
  # Optimizer
  optimizer: "adamw"
  adam_b1: 0.9
  adam_b2: 0.999
  adam_eps: 1e-8
    
  # Optimized Spike Bridge
  spike_bridge:
    encoding_strategy: "phase_preserving"
    use_learnable_thresholds: true
    num_threshold_scales: 2    # âœ… REDUCED: 4â†’2
    threshold_adaptation_rate: 0.01
    use_phase_preserving_encoding: true
    edge_detection_thresholds: 2  # âœ… REDUCED: 4â†’2
    
  # Memory-Optimized SNN Classifier
  snn:
    hidden_sizes: [64, 32]     # âœ… REDUCED: [128,64]â†’[64,32]
    num_classes: 2
    snn_neurons_per_layer: 128 # âœ… CRITICAL REDUCTION: 512â†’128
    snn_num_layers: 2          # âœ… CRITICAL REDUCTION: 4â†’2
    
    # Optimized LIF parameters
    tau_mem: 20e-3            # âœ… SIMPLIFIED: 50e-6â†’20e-3
    tau_syn: 5e-3             # âœ… SIMPLIFIED: 10e-6â†’5e-3
    threshold: 1.0
    use_enhanced_lif: true
    use_refractory_period: false  # âœ… DISABLED: for performance
    use_adaptation: false         # âœ… DISABLED: for performance
    
    # Simplified surrogate gradients
    surrogate_gradient_type: "ADAPTIVE_MULTI_SCALE"
    surrogate_gradient_beta: 2.0  # âœ… REDUCED: 4.0â†’2.0
    surrogate_slope: 2.0          # âœ… ADDED: Required for validation
    curriculum_learning: false    # âœ… DISABLED: for simplicity

# ğŸ¯ Fast Training Configuration
training:
  # Core parameters
  num_epochs: 50
  batch_size: 1              # âœ… SAFE: batch_size=1 for memory safety
  learning_rate: 5e-4
  weight_decay: 1e-4
  
  # Simplified learning rate schedule
  learning_rate_schedule: "cosine_with_warmup"
  warmup_epochs: 1           # âœ… REDUCED: 2â†’1
  warmup_factor: 0.1
  min_learning_rate: 1e-6
  
  # Optimized gradient settings
  gradient_clipping: true
  max_gradient_norm: 1.0
  gradient_accumulation_steps: 2  # âœ… REDUCED: 4â†’2 (effective batch=2)
  
  # Optimizer
  optimizer: "adamw"
  adam_b1: 0.9
  adam_b2: 0.999
  adam_eps: 1e-8

# ğŸ§® Simplified Mathematical Framework
mathematical_framework:
  # InfoNCE
  use_temporal_infonce: true
  temporal_context_length: 64   # âœ… REDUCED: from higher values
  temporal_negative_samples: 4  # âœ… REDUCED: 8â†’4
  
  # Temperature
  use_adaptive_temperature: true
  initial_temperature: 0.07
  min_temperature: 0.01
  max_temperature: 0.20
  
  # Simplified SNN simulation
  simulation_time_steps: 64     # âœ… REDUCED: for performance
  simulation_dt: 1e-3
  
  # Core enhancements only
  use_pac_bayes_regularization: false  # âœ… DISABLED: for performance
  gradient_stability_monitoring: true

# ğŸš€ Performance Enhancements (Simplified)
enhancements:
  # Keep core enhancements, simplify complex ones
  surrogate_adaptivity_factor: 1.5
  use_momentum_negatives: true
  negative_momentum: 0.95       # âœ… SIMPLIFIED: 0.999â†’0.95  
  hard_negative_ratio: 0.2
  curriculum_temperature: false # âœ… DISABLED: for simplicity

# ğŸ–¥ï¸ Optimized Platform Configuration
platform:
  device: "gpu"
  precision: "float32"
  memory_fraction: 0.3          # âœ… CONSERVATIVE: prevents memory issues
  enable_jit: true
  cache_compilation: true

# ğŸ’¾ Performance GPU Optimization
gpu_optimization:
  memory_fraction: 0.3          # âœ… CONSERVATIVE: 30% GPU memory
  enable_jit: true
  cache_compilation: true
  
  # Fast warmup
  comprehensive_gpu_warmup: true
  warmup_stages: 6
  
  # Optimized XLA settings
  xla_python_client_preallocate: false
  xla_python_client_mem_fraction: 0.3  # âœ… MATCH memory_fraction

# ğŸ“Š Efficient Monitoring
monitoring:
  # Basic W&B
  use_wandb: true
  wandb_project: "cpc_snn_gw_optimized"
  wandb_run_name: "fast_performance_training"
  
  # Reduced logging frequency
  log_every_n_steps: 10        # âœ… REDUCED: 5â†’10
  eval_every_n_epochs: 5       # âœ… REDUCED: 2â†’5
  save_every_n_epochs: 10      # âœ… REDUCED: 5â†’10
  
  # Selective tracking (disable expensive operations)
  track_gradient_norms: false   # âœ… DISABLED: for performance
  track_spike_patterns: false  # âœ… DISABLED: for performance
  track_attention_weights: false # âœ… DISABLED: for performance
  track_temperature_adaptation: true  # Keep for debugging
  
  # Output directory
  output_dir: "outputs/optimized_fast_training"
  save_best_model: true
  save_final_model: true

# ğŸ¯ Realistic Performance Targets
targets:
  # Achievable accuracy targets
  target_train_accuracy: 0.95
  target_test_accuracy: 0.90
  target_roc_auc: 0.92
  
  # Performance targets
  target_inference_time: 0.1   # <100ms
  target_energy_efficiency: 0.001
  target_spike_rate: 0.02     # 2% average
  
  # Quality metrics
  no_model_collapse: true
  gradient_flow_healthy: true
  cpc_loss_working: true

# ğŸ”¬ Essential Validation
validation:
  # Core test evaluation
  comprehensive_test_evaluation: true
  model_collapse_detection: true
  data_leakage_prevention: true
  
  # Minimal statistical validation
  confidence_intervals: false  # âœ… DISABLED: for performance
  bootstrap_samples: 0         # âœ… DISABLED: for performance
  statistical_significance: false # âœ… DISABLED: for performance

# ğŸ“Š Minimal Evaluation
evaluation:
  metrics: ["accuracy", "roc_auc", "f1"]  # âœ… REDUCED set
  target_accuracy: 0.90
  confidence_intervals: false
  bootstrap_samples: 0

# ğŸ“ Essential Logging
logging:
  level: "INFO"
  use_wandb: true
  wandb_project: "cpc_snn_gw_optimized"
  checkpoint_dir: "outputs/optimized_fast_training/checkpoints"
  save_every_n_epochs: 10
  log_every_n_steps: 10

# ğŸš¨ Core Safety Features
safety:
  # Essential monitoring
  nan_detection: true
  inf_detection: true
  gradient_explosion_detection: true
  
  # Memory safety
  memory_monitoring: true
  swap_detection: true
  oom_prevention: true
  
  # Core validation
  parameter_validation: true
  architecture_validation: false  # âœ… DISABLED: for performance
  data_quality_validation: true 