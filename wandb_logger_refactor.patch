--- a/utils/wandb_enhanced_logger.py
+++ b/utils/wandb_enhanced_logger.py
@@ -1,913 +1,50 @@
 #!/usr/bin/env python3
 """
-ðŸš€ Enhanced W&B Logger for Neuromorphic GW Detection
+DEPRECATED: Legacy W&B logger - Use modular utils.logging package instead.
 
-Comprehensive logging system with neuromorphic-specific metrics, 
-real-time visualizations, performance monitoring, and interactive dashboards.
-
-Features:
-- Neuromorphic metrics (spike rates, patterns, encoding efficiency)
-- Performance profiling (latency, memory, hardware utilization)
-- Custom visualizations (spike rasters, attention maps, gradient flows)
-- Real-time monitoring with alerts
-- Hardware telemetry (CPU/GPU/memory)
-- Scientific metrics (ROC curves, confusion matrices)
-- Artifact management (models, datasets, plots)
-- Interactive dashboards and reports
+This file provides backward compatibility wrappers.
+New code should import from utils.logging modules.
 """
 
-import os
-import sys
-import time
-import json
-import logging
-import psutil
-import numpy as np
-import matplotlib.pyplot as plt
-from typing import Dict, List, Optional, Any, Union, Tuple
-
-# Optional plotting dependency
-try:
-    import seaborn as sns
-    HAS_SEABORN = True
-except ImportError:
-    HAS_SEABORN = False
-from pathlib import Path
-from dataclasses import dataclass, asdict
-from contextlib import contextmanager
-
-import jax
-import jax.numpy as jnp
-
-# W&B integration
-try:
-    import wandb
-    HAS_WANDB = True
-except ImportError:
-    HAS_WANDB = False
-    wandb = None
-
-# Visualization dependencies
-try:
-    import plotly.graph_objects as go
-    import plotly.express as px
-    from plotly.subplots import make_subplots
-    HAS_PLOTLY = True
-except ImportError:
-    HAS_PLOTLY = False
-
-logger = logging.getLogger(__name__)
-
-@dataclass
-class NeuromorphicMetrics:
-    """Neuromorphic-specific metrics for comprehensive tracking"""
-    
-    # Spike dynamics
-    spike_rate: float = 0.0                    # Average spike rate (Hz)
-    spike_frequency: float = 0.0               # Dominant frequency (Hz) 
-    spike_synchrony: float = 0.0               # Population synchrony measure
-    spike_sparsity: float = 0.0                # Sparsity coefficient
-    spike_efficiency: float = 0.0              # Information per spike
-    
-    # Encoding metrics
-    encoding_snr: float = 0.0                  # Signal-to-noise ratio
-    encoding_fidelity: float = 0.0             # Reconstruction quality
-    temporal_precision: float = 0.0            # Timing precision (ms)
-    spike_train_correlation: float = 0.0       # Inter-train correlation
-    
-    # Network dynamics
-    membrane_potential_std: float = 0.0        # Membrane potential variability
-    synaptic_weight_norm: float = 0.0          # Weight matrix norm
-    network_activity: float = 0.0              # Overall network activity
-    adaptation_rate: float = 0.0               # Learning adaptation speed
-    
-    # CPC-specific metrics
-    contrastive_accuracy: float = 0.0          # CPC contrastive task accuracy
-    representation_rank: float = 0.0           # Effective dimensionality
-    mutual_information: float = 0.0            # MI between consecutive states
-    prediction_horizon: float = 0.0            # Effective prediction steps
-    
-    def to_dict(self) -> Dict[str, float]:
-        """Convert to dictionary for logging"""
-        return asdict(self)
+import warnings
 
-@dataclass
-class PerformanceMetrics:
-    """Performance and hardware monitoring metrics"""
-    
-    # Latency metrics (milliseconds)
-    inference_latency_ms: float = 0.0
-    cpc_forward_ms: float = 0.0
-    spike_encoding_ms: float = 0.0
-    snn_forward_ms: float = 0.0
-    total_pipeline_ms: float = 0.0
-    
-    # Memory metrics (MB)
-    memory_usage_mb: float = 0.0
-    peak_memory_mb: float = 0.0
-    memory_growth_mb: float = 0.0
-    gpu_memory_mb: float = 0.0
-    swap_usage_mb: float = 0.0
-    
-    # Hardware metrics
-    cpu_usage_percent: float = 0.0
-    gpu_utilization_percent: float = 0.0
-    temperature_celsius: float = 0.0
-    power_consumption_watts: float = 0.0
-    
-    # Throughput metrics
-    samples_per_second: float = 0.0
-    batches_per_second: float = 0.0
-    tokens_per_second: float = 0.0
-    
-    # JAX compilation metrics
-    jit_compilation_time_ms: float = 0.0
-    num_compilations: int = 0
-    cache_hit_rate: float = 0.0
+# Backward compatibility imports with deprecation warnings
+def _deprecated_import(name: str, new_path: str):
+    """Helper for deprecated imports"""
+    warnings.warn(
+        f"{name} from wandb_enhanced_logger.py is deprecated. "
+        f"Use: from {new_path} import {name}",
+        DeprecationWarning,
+        stacklevel=3
+    )
 
-@dataclass
-class SystemInfo:
-    """System and environment information"""
-    
-    # Platform info
-    platform: str = ""
-    python_version: str = ""
-    jax_version: str = ""
-    jax_backend: str = ""
-    device_count: int = 0
-    device_types: List[str] = None
-    cuda_version: str = ""
-    
-    # Hardware info
-    cpu_model: str = ""
-    cpu_cores: int = 0
-    total_memory_gb: float = 0.0
-    gpu_model: str = ""
-    gpu_memory_gb: float = 0.0
-    
-    # Environment
-    hostname: str = ""
-    username: str = ""
-    conda_env: str = ""
-    git_commit: str = ""
-    working_directory: str = ""
-    command_line: str = ""
-    
-    def __post_init__(self):
-        """Initialize default values"""
-        if self.device_types is None:
-            self.device_types = []
+# Backward compatibility classes and functions
+class EnhancedWandbLogger:
+    """DEPRECATED: Use utils.logging.EnhancedWandbLogger instead."""
+    def __init__(self, *args, **kwargs):
+        _deprecated_import("EnhancedWandbLogger", "utils.logging")
+        from .logging import EnhancedWandbLogger as _EnhancedWandbLogger
+        self._logger = _EnhancedWandbLogger(*args, **kwargs)
+    
+    def __getattr__(self, name):
+        return getattr(self._logger, name)
 
-class EnhancedWandbLogger:
-    """
-    ðŸš€ Enhanced W&B Logger for Neuromorphic GW Detection
-    
-    Comprehensive logging with neuromorphic-specific metrics,
-    real-time visualizations, and interactive dashboards.
-    """
-    
-    def __init__(self,
-                 project: str = "neuromorphic-gw-detection",
-                 name: Optional[str] = None,
-                 config: Optional[Dict[str, Any]] = None,
-                 tags: Optional[List[str]] = None,
-                 notes: Optional[str] = None,
-                 output_dir: str = "wandb_outputs",
-                 enable_hardware_monitoring: bool = True,
-                 enable_visualizations: bool = True,
-                 enable_alerts: bool = True,
-                 log_frequency: int = 10):
-        
-        if not HAS_WANDB:
-            logger.warning("W&B not available. Install with: pip install wandb")
-            self.enabled = False
-            return
-            
-        self.enabled = True
-        self.project = project
-        self.output_dir = Path(output_dir)
-        self.output_dir.mkdir(parents=True, exist_ok=True)
-        
-        # Configuration
-        self.enable_hardware_monitoring = enable_hardware_monitoring
-        self.enable_visualizations = enable_visualizations
-        self.enable_alerts = enable_alerts
-        self.log_frequency = log_frequency
-        
-        # Initialize W&B
-        self.run = None
-        try:
-            self.run = wandb.init(
-                project=project,
-                name=name or f"neuromorphic-gw-{int(time.time())}",
-                config=config or {},
-                tags=tags or ["neuromorphic", "gravitational-waves", "snn", "cpc"],
-                notes=notes or "Enhanced neuromorphic GW detection with comprehensive monitoring",
-                dir=str(self.output_dir),
-                reinit=True
-            )
-            logger.info(f"ðŸš€ Enhanced W&B logging initialized: {self.run.url}")
-        except Exception as e:
-            logger.error(f"Failed to initialize W&B: {e}")
-            self.enabled = False
-            return
-        
-        # Tracking variables
-        self.step_count = 0
-        self.metrics_buffer = []
-        self.hardware_stats = []
-        self.gradient_history = []
-        self.spike_history = []
-        
-        # System info
-        self.system_info = self._collect_system_info()
-        self._log_system_info()
-        
-        # Setup hardware monitoring
-        if self.enable_hardware_monitoring:
-            self._setup_hardware_monitoring()
-    
-    def _collect_system_info(self) -> SystemInfo:
-        """Collect comprehensive system information"""
-        import platform
-        import sys
-        
-        info = SystemInfo()
-        
-        # Platform info
-        info.platform = platform.platform()
-        info.python_version = sys.version.split()[0]
-        
-        # JAX info
-        if 'jax' in sys.modules:
-            info.jax_version = jax.__version__
-            info.jax_backend = jax.lib.xla_bridge.get_backend().platform
-            info.device_count = len(jax.devices())
-            info.device_types = [str(device).split(':')[0] for device in jax.devices()]
-        
-        # Hardware info
-        info.cpu_cores = psutil.cpu_count()
-        info.total_memory_gb = psutil.virtual_memory().total / (1024**3)
-        
-        # Environment info
-        info.conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'unknown')
-        
-        try:
-            import subprocess
-            info.git_commit = subprocess.check_output(
-                ['git', 'rev-parse', '--short', 'HEAD'],
-                stderr=subprocess.DEVNULL
-            ).decode().strip()
-        except:
-            info.git_commit = "unknown"
-        
-        return info
-    
-    def _log_system_info(self):
-        """Log system information to W&B"""
-        if not self.enabled:
-            return
-            
-        try:
-            system_dict = {
-                'platform': self.system_info.platform,
-                'python_version': self.system_info.python_version,
-                'jax_version': self.system_info.jax_version,
-                'jax_backend': getattr(self.system_info, 'jax_backend', 'unknown'),
-                'device_count': self.system_info.device_count,
-                'device_types': self.system_info.device_types,
-                'cpu_cores': self.system_info.cpu_cores,
-                'total_memory_gb': self.system_info.total_memory_gb,
-                'conda_env': getattr(self.system_info, 'conda_env', 'unknown'),
-                'git_commit': getattr(self.system_info, 'git_commit', 'unknown')
-            }
-            
-            self.run.config.update(system_dict)
-            logger.info("ðŸ’» System information logged to W&B")
-            
-        except Exception as e:
-            logger.warning(f"System info logging failed: {e}")
-    
-    def _setup_hardware_monitoring(self):
-        """Setup hardware monitoring"""
-        try:
-            # Initial hardware snapshot
-            self._log_hardware_snapshot()
-            logger.info("ðŸ“Š Hardware monitoring initialized")
-        except Exception as e:
-            logger.warning(f"Hardware monitoring setup failed: {e}")
-    
-    def _log_hardware_snapshot(self):
-        """Log current hardware state"""
-        if not self.enabled or not self.enable_hardware_monitoring:
-            return
-            
-        try:
-            # Memory usage
-            memory = psutil.virtual_memory()
-            
-            # CPU usage
-            cpu_percent = psutil.cpu_percent(interval=0.1)
-            
-            hardware_metrics = {
-                'hardware/memory_usage_percent': memory.percent,
-                'hardware/memory_available_gb': memory.available / (1024**3),
-                'hardware/cpu_usage_percent': cpu_percent,
-            }
-            
-            # GPU metrics if available
-            try:
-                devices = jax.devices('gpu')
-                if devices:
-                    hardware_metrics['hardware/gpu_count'] = len(devices)
-                    # Try to get GPU memory info
-                    try:
-                        import subprocess
-                        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', 
-                                               '--format=csv,noheader,nounits'], 
-                                              capture_output=True, text=True)
-                        if result.returncode == 0:
-                            lines = result.stdout.strip().split('\n')
-                            for i, line in enumerate(lines):
-                                used, total = map(int, line.split(', '))
-                                hardware_metrics[f'hardware/gpu_{i}_memory_used_mb'] = used
-                                hardware_metrics[f'hardware/gpu_{i}_memory_total_mb'] = total
-                                hardware_metrics[f'hardware/gpu_{i}_memory_usage_percent'] = (used / total) * 100
-                    except:
-                        pass
-            except:
-                pass
-            
-            self.run.log(hardware_metrics, step=self.step_count)
-            
-        except Exception as e:
-            logger.warning(f"Hardware snapshot failed: {e}")
-    
-    def log_neuromorphic_metrics(self, metrics: NeuromorphicMetrics, prefix: str = "neuromorphic"):
-        """Log neuromorphic-specific metrics"""
-        if not self.enabled:
-            return
-            
-        try:
-            # Convert metrics to dictionary with prefix
-            metrics_dict = {f"{prefix}/{k}": v for k, v in metrics.to_dict().items()}
-            
-            self.run.log(metrics_dict, step=self.step_count)
-            
-            # Store for history tracking
-            self.metrics_buffer.append({
-                'step': self.step_count,
-                'spike_rate': metrics.spike_rate,
-                'encoding_fidelity': metrics.encoding_fidelity,
-                'contrastive_accuracy': metrics.contrastive_accuracy
-            })
-            
-            logger.debug(f"ðŸ§  Logged neuromorphic metrics: spike_rate={metrics.spike_rate:.3f}")
-            
-        except Exception as e:
-            logger.warning(f"Neuromorphic metrics logging failed: {e}")
-    
-    def log_performance_metrics(self, metrics: PerformanceMetrics, prefix: str = "performance"):
-        """Log performance and hardware metrics"""
-        if not self.enabled:
-            return
-            
-        try:
-            # Convert metrics to dictionary with prefix
-            metrics_dict = {f"{prefix}/{k}": v for k, v in metrics.__dict__.items()}
-            
-            self.run.log(metrics_dict, step=self.step_count)
-            
-            # Log hardware snapshot periodically
-            if self.step_count % self.log_frequency == 0:
-                self._log_hardware_snapshot()
-            
-            # Store for history
-            self.hardware_stats.append({
-                'step': self.step_count,
-                'memory_usage_mb': metrics.memory_usage_mb,
-                'inference_latency_ms': metrics.inference_latency_ms,
-                'cpu_usage_percent': metrics.cpu_usage_percent
-            })
-            
-            logger.debug(f"âš¡ Logged performance metrics: latency={metrics.inference_latency_ms:.1f}ms")
-            
-        except Exception as e:
-            logger.warning(f"Performance metrics logging failed: {e}")
-    
-    def log_spike_patterns(self, spikes: jnp.ndarray, name: str = "spike_patterns"):
-        """Log spike patterns with raster plots and statistics"""
-        if not self.enabled or not self.enable_visualizations:
-            return
-            
-        try:
-            # Convert to numpy for processing
-            spikes_np = np.array(spikes)
-            
-            # Basic spike statistics
-            spike_rate = float(np.mean(spikes_np))
-            spike_std = float(np.std(spikes_np))
-            spike_count = int(np.sum(spikes_np))
-            
-            # Log basic stats
-            self.run.log({
-                f"{name}/spike_rate": spike_rate,
-                f"{name}/spike_std": spike_std, 
-                f"{name}/spike_count": spike_count,
-                f"{name}/sparsity": 1.0 - spike_rate
-            }, step=self.step_count)
-            
-            # Create raster plot (sample subset for performance)
-            if len(spikes_np.shape) >= 2 and self.step_count % (self.log_frequency * 5) == 0:
-                self._create_spike_raster_plot(spikes_np, name)
-            
-            # Store for history tracking
-            self.spike_history.append({
-                'step': self.step_count,
-                'spike_rate': spike_rate,
-                'spike_count': spike_count
-            })
-            
-            logger.debug(f"ðŸ”¥ Logged spike patterns: rate={spike_rate:.3f}")
-            
-        except Exception as e:
-            logger.warning(f"Spike pattern logging failed: {e}")
-    
-    def _create_spike_raster_plot(self, spikes: np.ndarray, name: str):
-        """Create and log spike raster plot"""
-        try:
-            fig, ax = plt.subplots(figsize=(12, 6))
-            
-            # Sample subset for visualization (max 100 neurons, 1000 time steps)
-            if spikes.shape[0] > 100:
-                neuron_indices = np.random.choice(spikes.shape[0], 100, replace=False)
-                spikes_sample = spikes[neuron_indices]
-            else:
-                spikes_sample = spikes
-                
-            if spikes_sample.shape[-1] > 1000:
-                time_indices = np.random.choice(spikes_sample.shape[-1], 1000, replace=False)
-                spikes_sample = spikes_sample[..., time_indices]
-            
-            # Create raster plot
-            if len(spikes_sample.shape) == 2:  # [neurons, time]
-                spike_times, spike_neurons = np.where(spikes_sample)
-                ax.scatter(spike_times, spike_neurons, s=1, alpha=0.7, c='black')
-                ax.set_xlabel('Time Step')
-                ax.set_ylabel('Neuron Index')
-                ax.set_title(f'{name} - Spike Raster Plot')
-            
-            elif len(spikes_sample.shape) == 3:  # [batch, neurons, time]
-                # Show first batch
-                spike_times, spike_neurons = np.where(spikes_sample[0])
-                ax.scatter(spike_times, spike_neurons, s=1, alpha=0.7, c='black')
-                ax.set_xlabel('Time Step')
-                ax.set_ylabel('Neuron Index')
-                ax.set_title(f'{name} - Spike Raster Plot (Batch 0)')
-            
-            plt.tight_layout()
-            
-            # Log to W&B
-            self.run.log({f"{name}_raster": wandb.Image(fig)}, step=self.step_count)
-            plt.close(fig)
-            
-        except Exception as e:
-            logger.warning(f"Raster plot creation failed: {e}")
-    
-    def log_gradient_stats(self, gradients: Dict[str, jnp.ndarray], prefix: str = "gradients"):
-        """Log gradient statistics with distributions"""
-        if not self.enabled:
-            return
-            
-        try:
-            gradient_stats = {}
-            
-            # Flatten all gradients
-            all_grads = []
-            for name, grad in gradients.items():
-                grad_np = np.array(grad).flatten()
-                all_grads.extend(grad_np.tolist())
-                
-                # Per-parameter statistics
-                gradient_stats[f"{prefix}/{name}_norm"] = float(np.linalg.norm(grad_np))
-                gradient_stats[f"{prefix}/{name}_mean"] = float(np.mean(grad_np))
-                gradient_stats[f"{prefix}/{name}_std"] = float(np.std(grad_np))
-                gradient_stats[f"{prefix}/{name}_max"] = float(np.max(np.abs(grad_np)))
-            
-            # Global gradient statistics
-            if all_grads:
-                gradient_stats[f"{prefix}/global_norm"] = float(np.linalg.norm(all_grads))
-                gradient_stats[f"{prefix}/global_mean"] = float(np.mean(all_grads))
-                gradient_stats[f"{prefix}/global_std"] = float(np.std(all_grads))
-                gradient_stats[f"{prefix}/global_max"] = float(np.max(np.abs(all_grads)))
-                
-                # Gradient health indicators
-                gradient_stats[f"{prefix}/vanishing_ratio"] = float(
-                    np.sum(np.abs(all_grads) < 1e-7) / len(all_grads)
-                )
-                gradient_stats[f"{prefix}/exploding_ratio"] = float(
-                    np.sum(np.abs(all_grads) > 1.0) / len(all_grads)
-                )
-            
-            self.run.log(gradient_stats, step=self.step_count)
-            
-            # Create histogram occasionally  
-            if self.enable_visualizations and self.step_count % (self.log_frequency * 10) == 0:
-                self._create_gradient_histogram(all_grads, prefix)
-            
-            # Store for history
-            self.gradient_history.append({
-                'step': self.step_count,
-                'gradient_norm': gradient_stats.get(f'{prefix}/global_norm', 0),
-                'vanishing_ratio': gradient_stats.get(f'{prefix}/vanishing_ratio', 0)
-            })
-            
-            logger.debug(f"ðŸ“Š Logged gradient stats: norm={gradient_stats.get(f'{prefix}/global_norm', 0):.2e}")
-            
-        except Exception as e:
-            logger.warning(f"Gradient stats logging failed: {e}")
-    
-    def _create_gradient_histogram(self, gradients: List[float], prefix: str):
-        """Create gradient distribution histogram"""
-        try:
-            if not gradients:
-                return
-                
-            fig, ax = plt.subplots(figsize=(10, 6))
-            
-            ax.hist(gradients, bins=50, alpha=0.7, density=True)
-            ax.set_xlabel('Gradient Value')
-            ax.set_ylabel('Density')
-            ax.set_title(f'{prefix} - Gradient Distribution')
-            ax.set_yscale('log')  # Log scale for better visualization
-            
-            # Add statistics
-            mean_grad = np.mean(gradients)
-            std_grad = np.std(gradients)
-            ax.axvline(mean_grad, color='red', linestyle='--', 
-                      label=f'Mean: {mean_grad:.2e}')
-            ax.axvline(mean_grad + std_grad, color='orange', linestyle='--', alpha=0.7)
-            ax.axvline(mean_grad - std_grad, color='orange', linestyle='--', alpha=0.7)
-            ax.legend()
-            
-            plt.tight_layout()
-            
-            # Log to W&B
-            self.run.log({f"{prefix}_histogram": wandb.Image(fig)}, step=self.step_count)
-            plt.close(fig)
-            
-        except Exception as e:
-            logger.warning(f"Gradient histogram creation failed: {e}")
-    
-    def log_learning_curves(self, train_metrics: Dict[str, float], 
-                           val_metrics: Optional[Dict[str, float]] = None):
-        """Log training and validation learning curves"""
-        if not self.enabled:
-            return
-            
-        try:
-            # Prepare metrics for logging
-            log_dict = {}
-            
-            # Training metrics
-            for key, value in train_metrics.items():
-                log_dict[f"train/{key}"] = float(value)
-            
-            # Validation metrics
-            if val_metrics:
-                for key, value in val_metrics.items():
-                    log_dict[f"val/{key}"] = float(value)
-            
-            self.run.log(log_dict, step=self.step_count)
-            
-            logger.debug("ðŸ“ˆ Logged learning curves")
-            
-        except Exception as e:
-            logger.warning(f"Learning curves logging failed: {e}")
-    
-    def log_model_artifacts(self, model_params: Dict[str, jnp.ndarray], 
-                           model_path: Optional[str] = None):
-        """Log model artifacts and parameters"""
-        if not self.enabled:
-            return
-            
-        try:
-            # Log model statistics
-            param_stats = {}
-            total_params = 0
-            
-            for name, param in model_params.items():
-                param_array = jnp.array(param)
-                param_count = param_array.size
-                total_params += param_count
-                
-                param_stats[f"model/{name}_shape"] = list(param_array.shape)
-                param_stats[f"model/{name}_params"] = param_count
-                param_stats[f"model/{name}_norm"] = float(jnp.linalg.norm(param_array))
-            
-            param_stats["model/total_parameters"] = total_params
-            self.run.log(param_stats, step=self.step_count)
-            
-            # Log model file if provided
-            if model_path and Path(model_path).exists():
-                artifact = wandb.Artifact(f"model-step-{self.step_count}", type="model")
-                artifact.add_file(model_path)
-                self.run.log_artifact(artifact)
-                logger.info(f"ðŸ“¦ Model artifact logged: {model_path}")
-            
-        except Exception as e:
-            logger.warning(f"Model artifacts logging failed: {e}")
-    
-    def _check_performance_alerts(self, metrics: PerformanceMetrics):
-        """Check for performance alerts and warnings"""
-        if not self.enabled or not self.enable_alerts:
-            return
-            
-        alerts = []
-        
-        # Memory alerts
-        if metrics.memory_usage_mb > 8000:  # > 8GB
-            alerts.append(f"High memory usage: {metrics.memory_usage_mb:.0f}MB")
-        
-        # Latency alerts
-        if metrics.inference_latency_ms > 1000:  # > 1 second
-            alerts.append(f"High inference latency: {metrics.inference_latency_ms:.0f}ms")
-        
-        # CPU alerts
-        if metrics.cpu_usage_percent > 90:
-            alerts.append(f"High CPU usage: {metrics.cpu_usage_percent:.0f}%")
-        
-        # GPU utilization alerts (low utilization might indicate bottlenecks)
-        if 0 < metrics.gpu_utilization_percent < 20:
-            alerts.append(f"Low GPU utilization: {metrics.gpu_utilization_percent:.0f}%")
-        
-        # Log alerts
-        if alerts:
-            alert_text = "; ".join(alerts)
-            self.run.log({"alerts/performance": alert_text}, step=self.step_count)
-            logger.warning(f"âš ï¸ Performance alerts: {alert_text}")
-    
-    def create_summary_dashboard(self):
-        """Create comprehensive summary dashboard"""
-        if not self.enabled or not self.enable_visualizations:
-            return
-            
-        try:
-            # Create metrics summary plot
-            self._create_metrics_summary_plot()
-            
-            # Create gradient history plot
-            if self.gradient_history:
-                self._create_gradient_history_plot()
-            
-            # Create spike history plot
-            if self.spike_history:
-                self._create_spike_history_plot()
-            
-            logger.info(f"ðŸ“Š Created summary dashboard at step {self.step_count}")
-            
-        except Exception as e:
-            logger.warning(f"Dashboard creation failed: {e}")
-    
-    def _create_metrics_summary_plot(self):
-        """Create comprehensive metrics summary plot"""
-        if not self.metrics_buffer:
-            return
-            
-        try:
-            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
-            
-            # Extract data
-            steps = [m['step'] for m in self.metrics_buffer]
-            spike_rates = [m['spike_rate'] for m in self.metrics_buffer]
-            encoding_fidelities = [m['encoding_fidelity'] for m in self.metrics_buffer]
-            contrastive_accuracies = [m['contrastive_accuracy'] for m in self.metrics_buffer]
-            
-            # Plot 1: Spike rates over time
-            axes[0, 0].plot(steps, spike_rates, 'b-', alpha=0.7)
-            axes[0, 0].set_title('Spike Rate Evolution')
-            axes[0, 0].set_xlabel('Training Step')
-            axes[0, 0].set_ylabel('Spike Rate (Hz)')
-            axes[0, 0].grid(True, alpha=0.3)
-            
-            # Plot 2: Encoding fidelity
-            axes[0, 1].plot(steps, encoding_fidelities, 'g-', alpha=0.7)
-            axes[0, 1].set_title('Encoding Fidelity Evolution')
-            axes[0, 1].set_xlabel('Training Step')
-            axes[0, 1].set_ylabel('Fidelity')
-            axes[0, 1].grid(True, alpha=0.3)
-            
-            # Plot 3: Contrastive accuracy
-            axes[1, 0].plot(steps, contrastive_accuracies, 'r-', alpha=0.7)
-            axes[1, 0].set_title('Contrastive Accuracy Evolution')
-            axes[1, 0].set_xlabel('Training Step')
-            axes[1, 0].set_ylabel('Accuracy')
-            axes[1, 0].grid(True, alpha=0.3)
-            
-            # Plot 4: Hardware metrics
-            if self.hardware_stats:
-                hw_steps = [h['step'] for h in self.hardware_stats]
-                memory_usage = [h['memory_usage_mb'] for h in self.hardware_stats]
-                axes[1, 1].plot(hw_steps, memory_usage, 'm-', alpha=0.7)
-                axes[1, 1].set_title('Memory Usage Evolution')
-                axes[1, 1].set_xlabel('Training Step')
-                axes[1, 1].set_ylabel('Memory (MB)')
-                axes[1, 1].grid(True, alpha=0.3)
-            
-            plt.tight_layout()
-            
-            # Save locally
-            summary_path = self.output_dir / f"metrics_summary_step_{self.step_count}.png"
-            fig.savefig(summary_path, dpi=150, bbox_inches='tight')
-            
-            # Log to W&B
-            self.run.log({"metrics_summary": wandb.Image(fig)}, step=self.step_count)
-            plt.close(fig)
-            
-        except Exception as e:
-            logger.warning(f"Metrics summary plot creation failed: {e}")
-    
-    def _create_gradient_history_plot(self):
-        """Create gradient evolution plot"""
-        try:
-            fig, ax = plt.subplots(figsize=(12, 6))
-            
-            steps = [g['step'] for g in self.gradient_history]
-            gradient_norms = [g['gradient_norm'] for g in self.gradient_history]
-            vanishing_ratios = [g['vanishing_ratio'] for g in self.gradient_history]
-            
-            # Primary axis: gradient norms
-            ax.plot(steps, gradient_norms, 'b-', label='Gradient Norm', alpha=0.7)
-            ax.set_xlabel('Training Step')
-            ax.set_ylabel('Gradient Norm', color='b')
-            ax.set_yscale('log')
-            ax.tick_params(axis='y', labelcolor='b')
-            ax.grid(True, alpha=0.3)
-            
-            # Secondary axis: vanishing gradient ratio
-            ax2 = ax.twinx()
-            ax2.plot(steps, vanishing_ratios, 'r-', label='Vanishing Ratio', alpha=0.7)
-            ax2.set_ylabel('Vanishing Gradient Ratio', color='r')
-            ax2.tick_params(axis='y', labelcolor='r')
-            
-            ax.set_title('Gradient Health Evolution')
-            plt.tight_layout()
-            
-            # Log to W&B
-            self.run.log({"gradient_history": wandb.Image(fig)}, step=self.step_count)
-            plt.close(fig)
-            
-        except Exception as e:
-            logger.warning(f"Gradient history plot creation failed: {e}")
-    
-    def _create_spike_history_plot(self):
-        """Create spike evolution plot"""
-        try:
-            fig, ax = plt.subplots(figsize=(12, 6))
-            
-            steps = [s['step'] for s in self.spike_history]
-            spike_rates = [s['spike_rate'] for s in self.spike_history]
-            spike_counts = [s['spike_count'] for s in self.spike_history]
-            
-            # Primary axis: spike rates
-            ax.plot(steps, spike_rates, 'g-', label='Spike Rate', alpha=0.7)
-            ax.set_xlabel('Training Step')
-            ax.set_ylabel('Spike Rate (Hz)', color='g')
-            ax.tick_params(axis='y', labelcolor='g')
-            ax.grid(True, alpha=0.3)
-            
-            # Secondary axis: spike counts
-            ax2 = ax.twinx()
-            ax2.plot(steps, spike_counts, 'orange', label='Spike Count', alpha=0.7)
-            ax2.set_ylabel('Total Spike Count', color='orange')
-            ax2.tick_params(axis='y', labelcolor='orange')
-            
-            ax.set_title('Spike Activity Evolution')
-            plt.tight_layout()
-            
-            # Log to W&B
-            self.run.log({"spike_history": wandb.Image(fig)}, step=self.step_count)
-            plt.close(fig)
-            
-        except Exception as e:
-            logger.warning(f"Spike history plot creation failed: {e}")
-    
-    def log_step_context(self, step: int):
-        """Update step context and create periodic summaries"""
-        self.step_count = step
-        
-        # Create summary dashboard periodically
-        if step > 0 and step % (self.log_frequency * 10) == 0:
-            self.create_summary_dashboard()
-    
-    def finish(self):
-        """Finish logging session with final summary"""
-        if not self.enabled:
-            return
-            
-        try:
-            # Create final summary dashboard
-            self.create_summary_dashboard()
-            
-            # Log session summary
-            session_summary = {
-                'total_steps': self.step_count,
-                'metrics_logged': len(self.metrics_buffer),
-                'hardware_snapshots': len(self.hardware_stats),
-                'gradient_updates': len(self.gradient_history)
-            }
-            
-            self.run.summary.update(session_summary)
-            
-            # Save local summary
-            summary_file = self.output_dir / "session_summary.json"
-            with open(summary_file, 'w') as f:
-                json.dump(session_summary, f, indent=2)
-            
-            self.run.finish()
-            logger.info("âœ… Enhanced W&B logging session finished")
-            
-        except Exception as e:
-            logger.warning(f"Session finish failed: {e}")
-            if self.run:
-                self.run.finish()
-    
-    @contextmanager
-    def log_context(self, name: str):
-        """Context manager for logging code blocks"""
-        start_time = time.time()
-        try:
-            yield
-        finally:
-            if self.enabled:
-                duration = time.time() - start_time
-                self.run.log({f"timing/{name}_duration_ms": duration * 1000}, 
-                           step=self.step_count)
+class NeuromorphicMetrics:
+    """DEPRECATED: Use utils.logging.NeuromorphicMetrics instead."""
+    def __init__(self, *args, **kwargs):
+        _deprecated_import("NeuromorphicMetrics", "utils.logging")
+        from .logging.metrics import NeuromorphicMetrics as _NeuromorphicMetrics
+        self._metrics = _NeuromorphicMetrics(*args, **kwargs)
+    
+    def __getattr__(self, name):
+        return getattr(self._metrics, name)
+
+class PerformanceMetrics:
+    """DEPRECATED: Use utils.logging.PerformanceMetrics instead."""
+    def __init__(self, *args, **kwargs):
+        _deprecated_import("PerformanceMetrics", "utils.logging")
+        from .logging.metrics import PerformanceMetrics as _PerformanceMetrics
+        self._metrics = _PerformanceMetrics(*args, **kwargs)
+    
+    def __getattr__(self, name):
+        return getattr(self._metrics, name)
+
+class SystemInfo:
+    """DEPRECATED: Use utils.logging.SystemInfo instead."""
+    def __init__(self, *args, **kwargs):
+        _deprecated_import("SystemInfo", "utils.logging")
+        from .logging.metrics import SystemInfo as _SystemInfo
+        self._info = _SystemInfo(*args, **kwargs)
+    
+    def __getattr__(self, name):
+        return getattr(self._info, name)
 
 # Factory functions for backward compatibility
 def create_enhanced_wandb_logger(*args, **kwargs):
-    """
-    Factory function to create enhanced W&B logger with sensible defaults.
-    
-    Args:
-        project: W&B project name
-        name: Run name (auto-generated if None)
-        config: Configuration dictionary
-        tags: List of tags for the run
-        notes: Run description
-        output_dir: Output directory for logs
-        **kwargs: Additional arguments for EnhancedWandbLogger
-        
-    Returns:
-        Configured EnhancedWandbLogger instance
-    """
-    return EnhancedWandbLogger(*args, **kwargs)
+    """DEPRECATED: Use utils.logging.create_enhanced_wandb_logger instead."""
+    _deprecated_import("create_enhanced_wandb_logger", "utils.logging")
+    from .logging import create_enhanced_wandb_logger as _create_enhanced_wandb_logger
+    return _create_enhanced_wandb_logger(*args, **kwargs)
 
 def create_neuromorphic_metrics(*args, **kwargs):
-    """
-    Factory function to create neuromorphic metrics with common values.
-    
-    Args:
-        spike_rate: Average spike rate (Hz)
-        encoding_fidelity: Reconstruction quality
-        contrastive_accuracy: CPC contrastive task accuracy
-        **kwargs: Additional metric values
-        
-    Returns:
-        NeuromorphicMetrics instance
-    """
-    return NeuromorphicMetrics(*args, **kwargs)
+    """DEPRECATED: Use utils.logging.create_neuromorphic_metrics instead."""
+    _deprecated_import("create_neuromorphic_metrics", "utils.logging")
+    from .logging import create_neuromorphic_metrics as _create_neuromorphic_metrics
+    return _create_neuromorphic_metrics(*args, **kwargs)
 
 def create_performance_metrics(*args, **kwargs):
-    """
-    Factory function to create performance metrics with common values.
-    
-    Args:
-        inference_latency_ms: Inference latency in milliseconds
-        memory_usage_mb: Memory usage in MB
-        cpu_usage_percent: CPU usage percentage
-        **kwargs: Additional metric values
-        
-    Returns:
-        PerformanceMetrics instance
-    """
-    return PerformanceMetrics(*args, **kwargs)
