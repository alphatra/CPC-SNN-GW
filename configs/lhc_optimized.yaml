# =============================================================================
# LHC-OPTIMIZED CONFIGURATION
# Based on Dillon et al. "Anomaly detection with spiking neural networks for LHC physics"
# arXiv:2508.00063v1 [hep-ph] 31 Jul 2025
# =============================================================================

# System Configuration
system:
  data_dir: "/teamspace/studios/this_studio/data"
  device: "auto"
  memory_fraction: 0.6
  enable_xla: true
  enable_jit: true

# Data Configuration  
data:
  # ✅ LHC PREPROCESSING: Minimal preprocessing like LHC paper
  sample_rate: 4096
  segment_length: 8.0
  overlap: 0.5
  
  # ✅ LHC NORMALIZATION: Linear rescaling (max=1.0, min_nonzero=0.1)
  normalization: "lhc_style"  # Linear rescaling like LHC paper
  apply_whitening: true
  
  # ✅ LHC DOWNSAMPLING: Target reasonable input dimension
  downsample_target_t: 1024  # Similar to LHC 19D → scale appropriately
  
  # Dataset paths
  dataset_4_path: "/teamspace/studios/this_studio/data/dataset-4"
  mlgwsc_data_path: "/teamspace/studios/this_studio/data/mlgwsc-1"
  
  # Current dataset files (gen48h)
  data_files:
    train_background: "train_background_gen48h_01.hdf"
    train_foreground: "train_foreground_gen48h_01.hdf" 
    train_injections: "train_injections_gen48h_01.hdf"
    val_background: "val_background_gen48h_01.hdf"
    val_foreground: "val_foreground_gen48h_01.hdf"
    val_injections: "val_injections_gen48h_01.hdf"

# =============================================================================
# LHC-OPTIMIZED MODEL CONFIGURATION
# =============================================================================
model:
  # ✅ LHC CPC: Simplified for efficiency
  cpc:
    latent_dim: 256
    num_layers: 2  # Reduced from 3 for efficiency
    hidden_dim: 256  # Reduced from 512
    dropout_rate: 0.0  # No dropout in LHC paper
    use_layer_norm: false  # Simpler architecture
    
  # ✅ LHC-OPTIMIZED SNN: Based on paper parameters
  snn:
    type: "lhc_optimized"  # Use new LHC-optimized implementation
    num_classes: 2  # binary: noise vs signal
    
    # ✅ LHC ARCHITECTURE: Small, efficient architecture
    # Original LHC: (19, 24, 12) → latent(4) → (12, 24, 19)
    # Adapted for GW: scaled appropriately
    hidden_sizes: [64, 32]  # Smaller architecture for efficiency
    latent_dim: 4  # ✅ LHC OPTIMAL: Small latent space
    
    # ✅ LHC TIME STEPS: 5-10 optimal range
    time_steps: 5  # ✅ LHC OPTIMAL: Start with 5 (can test 7, 10)
    
    # ✅ LHC THRESHOLD: 1.2 optimal
    threshold: 1.2  # ✅ LHC OPTIMAL: Higher than current 0.55
    
    # ✅ LHC MEMBRANE DYNAMICS: 0.9 beta decay
    beta: 0.9  # ✅ LHC OPTIMAL: Membrane potential decay factor
    tau_mem: 20.0  # Keep reasonable membrane time constant
    tau_syn: 10.0  # Keep reasonable synaptic time constant
    
    # ✅ LHC SURROGATE GRADIENTS: Hard sigmoid
    surrogate_gradient: "hard_sigmoid"  # ✅ LHC OPTIMAL
    surrogate_beta: 4.0  # ✅ LHC OPTIMAL: Same as current
    
    # ✅ LHC OUTPUT: No spiking on output layer
    output_layer_threshold: 12.0  # Very high to prevent output spiking
    
    # ✅ LHC RESET: Subtract threshold (not hard reset)
    membrane_reset: "subtract"  # ✅ LHC OPTIMAL
    
    # Monitoring parameters
    spike_regularization_target: 0.20  # Expect ~20% spike rate
    adaptive_threshold: false  # Keep threshold fixed for consistency
    
  # ✅ LHC BRIDGE: Simplified spike bridge
  bridge:
    encoding_type: "temporal_contrast"  # Simpler than multi-threshold
    spike_regularization: 0.001  # Minimal regularization
    temporal_encoding: true
    rate_coding_window: 8  # Smaller window for efficiency

# =============================================================================
# LHC-OPTIMIZED TRAINING CONFIGURATION  
# =============================================================================
training:
  # ✅ LHC TRAINING: Based on paper methodology
  batch_size: 8  # Small batches for stability
  learning_rate: 0.001  # ✅ LHC OPTIMAL: Adam with 0.001 LR
  num_epochs: 50  # Reasonable for testing
  
  # ✅ LHC OPTIMIZER: Adam with default parameters
  optimizer: "adam"
  optimizer_params:
    beta1: 0.9    # ✅ LHC OPTIMAL: Default Adam
    beta2: 0.999  # ✅ LHC OPTIMAL: Default Adam
    eps: 1.0e-8
  
  # ✅ LHC LOSS: MSE for reconstruction + classification
  loss_function: "mse_classification"  # Combine MSE + BCE like LHC
  
  # CPC parameters - simplified
  cpc_temperature: 0.20  # Higher than current 0.07 for stability
  cpc_aux_weight: 0.10   # Moderate CPC influence
  cpc_prediction_steps: 4  # Reduced from 12 for efficiency
  
  # Gradient clipping - conservative
  adaptive_grad_clip_threshold: 1.0  # Higher threshold for stability
  global_grad_clip_norm: 1.0
  per_module_grad_clip: false  # Simplified clipping
  
  # Early stopping
  early_stopping_patience: 15  # More patience for small architecture
  early_stopping_min_delta: 0.001
  
  # Learning rate scheduling
  use_lr_scheduler: true
  lr_scheduler_type: "reduce_on_plateau"
  lr_scheduler_params:
    factor: 0.5
    patience: 5
    min_lr: 1.0e-6

# =============================================================================
# LHC-OPTIMIZED EVALUATION CONFIGURATION
# =============================================================================
evaluation:
  # ✅ LHC EVALUATION: Similar to paper methodology
  eval_batch_size: 16  # Larger batches for evaluation
  metrics: ["accuracy", "auc", "precision", "recall", "f1"]
  
  # ✅ LHC VALIDATION: Cross-validation like approach
  validation_split: 0.2
  test_split: 0.2
  
  # Monitoring
  log_interval: 10
  save_interval: 5
  
  # ✅ LHC METRICS: Focus on anomaly detection metrics
  anomaly_detection_metrics: true
  roc_curve_analysis: true
  confusion_matrix: true

# =============================================================================
# WANDB CONFIGURATION
# =============================================================================
wandb:
  enabled: true
  project: "cpc-snn-gw-lhc-optimized"
  name: "lhc_t5_th1.2_beta0.9"  # Descriptive run name
  tags: ["lhc-optimized", "time_steps_5", "threshold_1.2", "beta_0.9"]
  
  # ✅ LHC MONITORING: Track key metrics from paper
  log_frequency: 1
  log_model: false  # Don't upload large models
  
  # Track LHC-specific metrics
  custom_metrics:
    - "spike_rate_mean"
    - "spike_rate_std" 
    - "active_neuron_ratio"
    - "latent_configurations"
    - "membrane_potential_stats"

# =============================================================================
# SYSTEM OPTIMIZATION
# =============================================================================
system_optimization:
  # ✅ LHC EFFICIENCY: Event-based processing preparation
  enable_event_based_processing: false  # Future implementation
  enable_sparse_computation: true
  
  # Memory optimization
  gradient_checkpointing: false  # Small model doesn't need it
  mixed_precision: false  # Keep full precision for stability
  
  # JAX optimization
  jax_config:
    platform_name: "cuda"  # or "cpu" for testing
    preallocate: false
    memory_fraction: 0.6
    
# =============================================================================
# EXPERIMENTAL FEATURES
# =============================================================================
experimental:
  # ✅ LHC FUTURE: Features from paper for future implementation
  enable_stdp_learning: false  # Future: Spike-timing dependent plasticity
  enable_binary_latent_optimization: false  # Future: Optimize 2^(T*Dz) space
  enable_reconstruction_loss: false  # Future: SNN-AE decoder
  
  # Comparison with current system
  compare_with_baseline: true
  baseline_time_steps: 32  # Current system
  baseline_threshold: 0.55  # Current system
  
  # A/B testing parameters
  test_multiple_time_steps: [5, 7, 10]  # LHC optimal range
  test_multiple_thresholds: [1.0, 1.2, 1.4]  # Around LHC optimal
