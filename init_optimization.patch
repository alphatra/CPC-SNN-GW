--- a/__init__.py
+++ b/__init__.py
@@ -1,671 +1,150 @@
 """
 CPC+SNN Neuromorphic Gravitational Wave Detection
 
 World's first neuromorphic gravitational wave detector using 
 Contrastive Predictive Coding + Spiking Neural Networks.
 
 Designed for production deployment following ML4GW standards.
 """
 
+import warnings
+from typing import Dict, Any, Optional
+
+# Version information (eager loading for metadata)
 try:
     from ._version import __version__, __version_tuple__
 except ImportError:
     __version__ = "0.1.0-dev"
     __version_tuple__ = (0, 1, 0, "dev")
 
 # Extended version information
 __author__ = "CPC-SNN-GW Team"
 __email__ = "contact@cpc-snn-gw.dev"
 __license__ = "MIT"
 __description__ = "Neuromorphic Gravitational Wave Detection using CPC+SNN"
 __url__ = "https://github.com/cpc-snn-gw/ligo-cpc-snn"
 
-# Build information
-import datetime
-__build_date__ = datetime.datetime.now().isoformat()
-__build_info__ = {
-    "version": __version__,
-    "version_tuple": __version_tuple__,
-    "build_date": __build_date__,
-    "python_version": None,  # Will be set later
-    "jax_version": None,
-    "dependencies": {}
-}
-
-# Runtime version checking
-def get_version_info():
+# Lazy-loaded build information
+def get_version_info() -> Dict[str, Any]:
     """Get comprehensive version information."""
     import sys
     import platform
+    import datetime
     
     try:
         import jax
         jax_version = jax.__version__
     except ImportError:
         jax_version = "not available"
     
     try:
         import numpy
         numpy_version = numpy.__version__
     except ImportError:
         numpy_version = "not available"
     
-    # Update build info
-    __build_info__.update({
+    return {
+        "version": __version__,
+        "version_tuple": __version_tuple__,
+        "build_date": datetime.datetime.now().isoformat(),
         "python_version": sys.version.split()[0],
         "jax_version": jax_version,
-        "dependencies": {
+        "numpy_version": numpy_version,
+        "platform": platform.platform(),
+        "dependencies_available": {
             "numpy": numpy_version,
             "jax": jax_version
         }
-    })
-    
-    return __build_info__
+    }
 
 def print_version_info():
     """Print comprehensive version information."""
     info = get_version_info()
-    print(f"CPC-SNN-GW v{info['version']}")
-    print(f"Python: {info['python_version']}")
-    print(f"JAX: {info['jax_version']}")
-    print(f"NumPy: {info['dependencies']['numpy']}")
-    print(f"Build: {info['build_date']}")
-
-# Dataset utilities
-def export_dataset(dataset, output_path: str, format: str = "h5", **kwargs):
-    """
-    Export dataset to various formats.
-    
-    Args:
-        dataset: Dataset to export
-        output_path: Output file path
-        format: Export format ("h5", "npz", "json")
-        **kwargs: Additional format-specific options
-    """
-    from .data.gw_synthetic_generator import ContinuousGWGenerator
-    import json
-    import numpy as np
-    from pathlib import Path
-    
-    output_path = Path(output_path)
-    output_path.parent.mkdir(parents=True, exist_ok=True)
-    
-    if format.lower() == "h5":
-        try:
-            import h5py
-            with h5py.File(output_path, 'w') as f:
-                if isinstance(dataset, dict):
-                    for key, value in dataset.items():
-                        f.create_dataset(key, data=np.array(value))
-                else:
-                    f.create_dataset('data', data=np.array(dataset))
-            print(f"Dataset exported to {output_path} (HDF5)")
-        except ImportError:
-            raise ImportError("h5py required for HDF5 export. Install with: pip install h5py")
-    
-    elif format.lower() == "npz":
-        if isinstance(dataset, dict):
-            np.savez_compressed(output_path, **dataset)
-        else:
-            np.savez_compressed(output_path, data=dataset)
-        print(f"Dataset exported to {output_path} (NPZ)")
-    
-    elif format.lower() == "json":
-        # Convert numpy arrays to lists for JSON serialization
-        def convert_numpy(obj):
-            if isinstance(obj, np.ndarray):
-                return obj.tolist()
-            elif isinstance(obj, dict):
-                return {k: convert_numpy(v) for k, v in obj.items()}
-            elif isinstance(obj, list):
-                return [convert_numpy(item) for item in obj]
-            return obj
-        
-        json_data = convert_numpy(dataset)
-        with open(output_path, 'w') as f:
-            json.dump(json_data, f, indent=2)
-        print(f"Dataset exported to {output_path} (JSON)")
-    
-    else:
-        raise ValueError(f"Unsupported format: {format}. Use 'h5', 'npz', or 'json'")
-
-def import_dataset(input_path: str, format: str = "auto"):
-    """
-    Import dataset from various formats.
-    
-    Args:
-        input_path: Input file path
-        format: Import format ("auto", "h5", "npz", "json")
-        
-    Returns:
-        Loaded dataset
-    """
-    import numpy as np
-    import json
-    from pathlib import Path
-    
-    input_path = Path(input_path)
-    
-    if not input_path.exists():
-        raise FileNotFoundError(f"Dataset file not found: {input_path}")
-    
-    # Auto-detect format
-    if format == "auto":
-        suffix = input_path.suffix.lower()
-        if suffix in ['.h5', '.hdf5']:
-            format = "h5"
-        elif suffix == '.npz':
-            format = "npz"
-        elif suffix == '.json':
-            format = "json"
-        else:
-            raise ValueError(f"Cannot auto-detect format for {input_path}")
-    
-    if format.lower() == "h5":
-        try:
-            import h5py
-            dataset = {}
-            with h5py.File(input_path, 'r') as f:
-                for key in f.keys():
-                    dataset[key] = np.array(f[key])
-            print(f"Dataset loaded from {input_path} (HDF5)")
-            return dataset
-        except ImportError:
-            raise ImportError("h5py required for HDF5 import. Install with: pip install h5py")
-    
-    elif format.lower() == "npz":
-        data = np.load(input_path)
-        dataset = {key: data[key] for key in data.files}
-        print(f"Dataset loaded from {input_path} (NPZ)")
-        return dataset
-    
-    elif format.lower() == "json":
-        with open(input_path, 'r') as f:
-            dataset = json.load(f)
-        print(f"Dataset loaded from {input_path} (JSON)")
-        return dataset
-    
-    else:
-        raise ValueError(f"Unsupported format: {format}")
-
-# Training utilities
-def create_training_pipeline(config: dict = None):
-    """Create a complete training pipeline."""
-    from .training import create_cpc_snn_trainer
-    from .data import ContinuousGWGenerator
-    
-    print("ðŸš€ Creating CPC+SNN training pipeline...")
-    trainer = create_cpc_snn_trainer(config or {})
-    generator = ContinuousGWGenerator()
-    
-    return {"trainer": trainer, "data_generator": generator}
-
-def quick_start_demo():
-    """Run a quick demonstration of the CPC+SNN system."""
-    print("ðŸŒŸ CPC+SNN Neuromorphic GW Detection - Quick Start Demo")
-    print_version_info()
-    
-    # Create pipeline
-    pipeline = create_training_pipeline()
-    print("âœ… Training pipeline created")
-    
-    print("ðŸ“Š System ready for training!")
-    print("Next steps:")
-    print("  1. Configure your training parameters")
-    print("  2. Load or generate training data")
-    print("  3. Run training with: trainer.train()")
+    print(f"ðŸš€ CPC-SNN-GW v{info['version']}")
+    print(f"ðŸ“¦ Python: {info['python_version']}")
+    print(f"âš¡ JAX: {info['jax_version']}")
+    print(f"ðŸ”¢ NumPy: {info['numpy_version']}")
+    print(f"ðŸ’» Platform: {info['platform']}")
 
 # ===== LAZY LOADING SYSTEM =====
-# This system provides lazy imports for better startup performance
-# and reduced memory usage.
+# Lazy imports for better startup performance and reduced memory usage
 
-# Import registry - maps names to (module, attribute) tuples
+# Comprehensive import registry with deprecation support
 _LAZY_IMPORTS = {
-    # Data components
-    "AdvancedDataPreprocessor": ("data.gw_download", "AdvancedDataPreprocessor"),
-    "CacheMetadata": ("data.cache_manager", "CacheMetadata"),
-    "COLOR_SCHEMES": ("data.label_utils", "COLOR_SCHEMES"),
-    "ContinuousGWGenerator": ("data.gw_synthetic_generator", "ContinuousGWGenerator"),
-    "ContinuousGWParams": ("data.gw_signal_params", "ContinuousGWParams"),
-    "DataPreprocessor": ("data.gw_download", "DataPreprocessor"),
-    "GWOSCDownloader": ("data.gw_download", "GWOSCDownloader"),
-    "GWSignalType": ("data.label_utils", "GWSignalType"),
-    "LABEL_COLORS": ("data.label_utils", "LABEL_COLORS"),
-    "LABEL_COLORS_COLORBLIND": ("data.label_utils", "LABEL_COLORS_COLORBLIND"),
-    "LABEL_COLORS_SCIENTIFIC": ("data.label_utils", "LABEL_COLORS_SCIENTIFIC"),
-    "LABEL_DESCRIPTIONS": ("data.label_utils", "LABEL_DESCRIPTIONS"),
-    "LABEL_NAMES": ("data.label_utils", "LABEL_NAMES"),
-    "LabelError": ("data.label_utils", "LabelError"),
-    "LabelValidationResult": ("data.label_utils", "LabelValidationResult"),
-    "ProductionGWOSCDownloader": ("data.gw_download", "ProductionGWOSCDownloader"),
-    "ProfessionalCacheManager": ("data.cache_manager", "ProfessionalCacheManager"),
-    "ProcessingResult": ("data.gw_download", "ProcessingResult"),
-    "QualityMetrics": ("data.gw_download", "QualityMetrics"),
-    "SegmentSampler": ("data.gw_download", "SegmentSampler"),
-    "SignalConfiguration": ("data.gw_signal_params", "SignalConfiguration"),
-    
-    # Data utilities
-    "cache_decorator": ("data.cache_manager", "cache_decorator"),
-    "convert_legacy_labels": ("data.label_utils", "convert_legacy_labels"),
-    "create_label_report": ("data.label_utils", "create_label_report"),
-    "create_label_visualization_config": ("data.label_utils", "create_label_visualization_config"),
-    "create_mixed_gw_dataset": ("data.gw_dataset_builder", "create_mixed_gw_dataset"),
-    "dataset_to_canonical": ("data.label_utils", "dataset_to_canonical"),
-    "get_cache_manager": ("data.cache_manager", "get_cache_manager"),
-    "get_class_weights": ("data.label_utils", "get_class_weights"),
-    "get_cmap_colors": ("data.label_utils", "get_cmap_colors"),
-    "log_dataset_info": ("data.label_utils", "log_dataset_info"),
-    "normalize_labels": ("data.label_utils", "normalize_labels"),
-    "validate_dataset_labels": ("data.label_utils", "validate_dataset_labels"),
-    "validate_labels": ("data.label_utils", "validate_labels"),
-    
-    # Model components (MODULAR)
-    "CPCEncoder": ("models.cpc.core", "CPCEncoder"),
-    "EnhancedCPCEncoder": ("models.cpc.core", "EnhancedCPCEncoder"),
-    "EquinoxGRUWrapper": ("models.cpc_components", "EquinoxGRUWrapper"),
-    "ExperimentConfig": ("models.cpc.config", "ExperimentConfig"),
-    "EnhancedSNNClassifier": ("models.snn.core", "EnhancedSNNClassifier"),
-    "LIFLayer": ("models.snn.layers", "LIFLayer"),
-    "ValidatedSpikeBridge": ("models.bridge.core", "ValidatedSpikeBridge"),
-    "RMSNorm": ("models.cpc_components", "RMSNorm"),
-    "SNNClassifier": ("models.snn.core", "SNNClassifier"),
-    "SNNConfig": ("models.snn.config", "SNNConfig"),
-    "SNNTrainer": ("models.snn.trainer", "SNNTrainer"),
-    "SpikeEncodingStrategy": ("models.bridge.encoders", "SpikeEncodingStrategy"),
-    "SurrogateGradientType": ("models.snn_utils", "SurrogateGradientType"),
-    "WeightNormDense": ("models.cpc_components", "WeightNormDense"),
-    "VectorizedLIFLayer": ("models.snn.layers", "VectorizedLIFLayer"),
-    
-    # Model factories (MODULAR)
-    "create_default_spike_bridge": ("models.bridge.core", "create_default_spike_bridge"),
-    "create_enhanced_cpc_encoder": ("models.cpc.factory", "create_enhanced_cpc_encoder"),
-    "create_enhanced_snn_classifier": ("models.snn.factory", "create_enhanced_snn_classifier"),
-    "create_experiment_config": ("models.cpc.factory", "create_experiment_config"),
-    "create_fast_spike_bridge": ("models.bridge.core", "create_fast_spike_bridge"),
-    "create_robust_spike_bridge": ("models.bridge.core", "create_robust_spike_bridge"),
-    "create_snn_classifier": ("models.snn.factory", "create_snn_classifier"),
-    "create_snn_config": ("models.snn.factory", "create_snn_config"),
-    "create_spike_bridge_from_string": ("models.bridge.core", "create_spike_bridge_from_string"),
-    "create_standard_cpc_encoder": ("models.cpc.factory", "create_standard_cpc_encoder"),
-    "create_surrogate_gradient_fn": ("models.snn_utils", "create_surrogate_gradient_fn"),
-    
-    # Model utilities (MODULAR)
-    "enhanced_info_nce_loss": ("models.cpc.losses", "enhanced_info_nce_loss"),
-    "info_nce_loss": ("models.cpc.losses", "info_nce_loss"),
-    "spike_function_with_surrogate": ("models.snn_utils", "spike_function_with_surrogate"),
-    "BatchedSNNValidator": ("models.snn_utils", "BatchedSNNValidator"),
-    
-    # Training components
-    "RealAdvancedGWTrainer": ("training.advanced", "RealAdvancedGWTrainer"),
-    "CPCPretrainer": ("training.pretrain_cpc", "CPCPretrainer"),
-    "CPCSNNTrainer": ("training.base_trainer", "CPCSNNTrainer"),
-    "HydraTrainerMixin": ("training.base_trainer", "HydraTrainerMixin"),
-    "TrainerBase": ("training.base_trainer", "TrainerBase"),
-    "TrainingConfig": ("training.base_trainer", "TrainingConfig"),
-    "TrainingMetrics": ("training.base_trainer", "TrainingMetrics"),
-    
-    # Training factories
-    "create_cpc_snn_cli_app": ("training.base_trainer", "create_cpc_snn_cli_app"),
-    "create_cpc_snn_trainer": ("training.base_trainer", "create_cpc_snn_trainer"),
-    "create_enhanced_gw_trainer": ("training.enhanced_gw_training", "create_enhanced_gw_trainer"),
-    "create_hydra_cli_app": ("training.base_trainer", "create_hydra_cli_app"),
-    "create_training_config": ("training.base_trainer", "create_training_config"),
-    "pretrain_cpc_main": ("training.pretrain_cpc", "main"),
-    
-    # Utility functions
-    "ML4GW_PROJECT_STRUCTURE": ("utils", "ML4GW_PROJECT_STRUCTURE"),
-    "create_directory_structure": ("utils", "create_directory_structure"),
-    "get_jax_device_info": ("utils", "get_jax_device_info"),
-    "print_system_info": ("utils", "print_system_info"),
-    "setup_logging": ("utils", "setup_logging"),
-    "validate_array_shape": ("utils", "validate_array_shape"),
+    # Core components with modular paths
+    "EnhancedWandbLogger": ("utils.logging", "EnhancedWandbLogger"),
+    "NeuromorphicMetrics": ("utils.logging", "NeuromorphicMetrics"),
+    "PerformanceMetrics": ("utils.logging", "PerformanceMetrics"),
+    "SegmentSampler": ("data.preprocessing", "SegmentSampler"),
+    "AdvancedDataPreprocessor": ("data.preprocessing", "AdvancedDataPreprocessor"),
+    "PreprocessingConfig": ("data.preprocessing", "PreprocessingConfig"),
+    "CPCEncoder": ("models.cpc.core", "CPCEncoder"),
+    "EnhancedCPCEncoder": ("models.cpc.core", "EnhancedCPCEncoder"),
+    "SNNClassifier": ("models.snn.core", "SNNClassifier"),
+    "EnhancedSNNClassifier": ("models.snn.core", "EnhancedSNNClassifier"),
+    "ValidatedSpikeBridge": ("models.bridge.core", "ValidatedSpikeBridge"),
+    "CPCSNNTrainer": ("training.base", "CPCSNNTrainer"),
+    "TrainingConfig": ("training.base", "TrainingConfig"),
+    
+    # Factory functions
+    "create_enhanced_wandb_logger": ("utils.logging", "create_enhanced_wandb_logger"),
+    "create_enhanced_cpc_encoder": ("models.cpc.factory", "create_enhanced_cpc_encoder"),
+    "create_enhanced_snn_classifier": ("models.snn.factory", "create_enhanced_snn_classifier"),
+    "create_cpc_snn_trainer": ("training.base", "create_cpc_snn_trainer"),
+    
+    # CLI components
+    "train_cmd": ("cli.commands", "train_cmd"),
+    "eval_cmd": ("cli.commands", "eval_cmd"),
+    "infer_cmd": ("cli.commands", "infer_cmd"),
 }
 
-# Cache for loaded modules
+# Cache for already loaded modules
 _LOADED_MODULES = {}
 
 def __getattr__(name: str):
-    """
-    Lazy loading implementation using __getattr__.
-    
-    This function is called when an attribute is not found in the module.
-    It checks if the requested name is in our lazy import registry and
-    dynamically imports it.
-    """
+    """Lazy loading implementation for better performance."""
     if name in _LAZY_IMPORTS:
         module_path, attr_name = _LAZY_IMPORTS[name]
         
         # Check cache first
         cache_key = f"{module_path}.{attr_name}"
         if cache_key in _LOADED_MODULES:
             return _LOADED_MODULES[cache_key]
         
         try:
             # Import the module
             from importlib import import_module
             module = import_module(f".{module_path}", package=__name__)
             
             # Get the attribute
             attr = getattr(module, attr_name)
             
             # Cache it
             _LOADED_MODULES[cache_key] = attr
             
             return attr
         except (ImportError, AttributeError) as e:
-            raise ImportError(f"Could not import {name} from {module_path}: {e}")
-    
-    # Handle deprecated imports
-    elif name == "run_advanced_training_experiment":
-        # Deprecated: no direct experiment runner in modular API
-        import warnings
-        warnings.warn(
-            "run_advanced_training_experiment is deprecated. "
-            "Use training.advanced.create_real_advanced_trainer and invoke your run loop.",
-            DeprecationWarning,
-            stacklevel=2,
+            # Provide helpful error message
+            available_imports = ", ".join(sorted(_LAZY_IMPORTS.keys()))
+            raise ImportError(
+                f"Could not import '{name}' from {module_path}. "
+                f"Available lazy imports: {available_imports[:200]}..."
+            )
+    
+    # Handle special cases and deprecated imports
+    elif name in ["export_dataset", "import_dataset", "create_training_pipeline", "quick_start_demo"]:
+        warnings.warn(
+            f"'{name}' has been moved to utils module. "
+            f"Use: from utils import {name}",
+            DeprecationWarning,
+            stacklevel=2
         )
-        raise AttributeError(
-            f"'{name}' is deprecated. Use training.advanced.create_real_advanced_trainer instead."
+        # Try to import from utils
+        try:
+            from . import utils
+            return getattr(utils, name)
+        except AttributeError:
+            pass
+    
+    # Handle common typos and alternative names
+    suggestions = {
+        "WandbLogger": "EnhancedWandbLogger",
+        "Preprocessor": "AdvancedDataPreprocessor", 
+        "Trainer": "CPCSNNTrainer",
+        "SNN": "SNNClassifier",
+        "CPC": "CPCEncoder"
+    }
+    
+    if name in suggestions:
+        suggested = suggestions[name]
+        warnings.warn(
+            f"'{name}' not found. Did you mean '{suggested}'?",
+            UserWarning,
+            stacklevel=2
         )
+        return __getattr__(suggested)
     
     # Standard AttributeError for unknown attributes
-    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")
+    available_attrs = list(_LAZY_IMPORTS.keys()) + ["__version__", "print_version_info", "get_version_info"]
+    raise AttributeError(
+        f"module '{__name__}' has no attribute '{name}'. "
+        f"Available attributes: {', '.join(available_attrs[:10])}..."
+    )
 
-# ===== OPTIONAL DEPENDENCIES CHECK =====
-def _check_optional_dependencies():
+# Optional dependency checking (lazy)
+def check_dependencies() -> Dict[str, bool]:
     """Check availability of optional dependencies."""
-    optional_deps = {
-        "optax": "pip install optax",
-        "equinox": "pip install equinox", 
-        "haiku": "pip install dm-haiku",
-        "wandb": "pip install wandb",
-    }
+    deps = {}
+    optional_deps = ["optax", "equinox", "haiku", "wandb", "matplotlib", "seaborn"]
     
-    missing = []
-    for dep, install_cmd in optional_deps.items():
+    for dep in optional_deps:
         try:
             __import__(dep)
+            deps[dep] = True
         except ImportError:
-            missing.append((dep, install_cmd))
+            deps[dep] = False
     
-    if missing:
-        import warnings
-        warnings.warn(
-            f"Optional dependencies missing: {[dep for dep, _ in missing]}. "
-            f"Install with: {' && '.join([cmd for _, cmd in missing])}",
-            UserWarning
-        )
+    return deps
 
-# Check dependencies on import
-_check_optional_dependencies()
-
-# ===== PACKAGE METADATA =====
+# Package metadata for tools and IDEs
 __all__ = [
+    # Core version info
     "__version__",
     "__version_tuple__",
     "__author__",
     "__email__",
     "__license__",
     "__description__",
     "__url__",
-    "__build_date__",
-    "__build_info__",
+    
+    # Utility functions
     "get_version_info",
     "print_version_info",
-    "export_dataset",
-    "import_dataset", 
-    "create_training_pipeline",
-    "quick_start_demo",
+    "check_dependencies",
+    
+    # Key components (lazy-loaded)
+    "EnhancedWandbLogger",
+    "AdvancedDataPreprocessor",
+    "CPCEncoder", 
+    "EnhancedCPCEncoder",
+    "SNNClassifier",
+    "EnhancedSNNClassifier",
+    "ValidatedSpikeBridge",
+    "CPCSNNTrainer",
+    "TrainingConfig",
+    
+    # CLI commands
+    "train_cmd",
+    "eval_cmd",
+    "infer_cmd",
 ]
