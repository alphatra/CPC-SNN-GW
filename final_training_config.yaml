# üöÄ FINAL OPTIMAL TRAINING CONFIGURATION - 50 EPOCHS
# Based on Revolutionary Mathematical Framework + Memory Bank Patterns
# Target: >95% accuracy with neuromorphic efficiency

# üìä Experiment Configuration
experiment:
  name: "cpc_snn_gw_final_50_epochs"
  version: "revolutionary_v1.0"
  description: "Final 50-epoch training with ALL revolutionary enhancements integrated"
  target_accuracy: 0.95
  scientific_validation: true

# üåä Enhanced Data Configuration (Real LIGO Integration)
data:
  # Real LIGO GW150914 parameters
  use_real_ligo_data: true
  sample_rate: 4096  # Hz - LIGO standard
  sequence_length: 256  # Optimized for T4 memory constraints
  window_size: 256     # Temporal window for CPC
  enhanced_overlap: 0.9  # High overlap for more training samples
  num_samples: 2000     # Enhanced dataset size
  data_augmentation: true
  noise_scaling: true
  
  # Quality and preprocessing  
  min_snr: 5.0         # Lower threshold for more diverse data
  preprocessing:
    whitening: true
    bandpass_low: 35.0   # Optimized GW frequency band
    bandpass_high: 350.0
    scaling_factor: 1e20  # Memory bank standard

# üß† Revolutionary Model Architecture (All 5 Enhancements)
model:
  # CPC Encoder with Temporal Transformer
  cpc:
    latent_dim: 128        # Optimized for mathematical framework
    downsample_factor: 4   # Critical fix from memory bank
    context_length: 256    # Temporal context for InfoNCE
    prediction_steps: 8    # Reduced for efficiency
    num_negatives: 8       # Mathematical framework optimum
    temperature: 0.06      # œÑ = 1/‚àöd for d=256
    conv_channels: [64, 128, 256]  # Progressive depth
    
    # üöÄ Temporal Transformer Enhancement
    use_temporal_transformer: true
    transformer_num_heads: 4
    transformer_num_layers: 2
    multi_scale_kernels: [3, 5, 7]
    temporal_attention_dropout: 0.1
    
  # Enhanced Spike Bridge
  spike_bridge:
    encoding_strategy: "phase_preserving"  # Revolutionary enhancement 
    use_learnable_thresholds: true
    num_threshold_scales: 4
    threshold_adaptation_rate: 0.01
    use_phase_preserving_encoding: true
    edge_detection_thresholds: 4
    
  # Enhanced SNN Classifier  
  snn:
    hidden_sizes: [128, 64]  # Memory optimized
    num_classes: 2          # Binary: signal vs noise
    snn_neurons_per_layer: 512  # Mathematical framework requirement
    snn_num_layers: 4           # Depth requirement
    
    # Enhanced LIF parameters
    tau_mem: 50e-6             # Mathematical framework optimum
    tau_syn: 10e-6
    threshold: 1.0
    use_enhanced_lif: true
    use_refractory_period: true
    use_adaptation: true
    refractory_time_constant: 2e-3
    adaptation_time_constant: 20e-3
    
    # Surrogate gradients
    surrogate_gradient_type: "ADAPTIVE_MULTI_SCALE"
    surrogate_gradient_beta: 4.0  # Mathematical framework
    curriculum_learning: true

# üéØ Optimized Training Configuration (50 Epochs)
training:
  # Main training parameters
  num_epochs: 50
  batch_size: 2              # Reduced for CPU memory safety
  learning_rate: 5e-4        # Optimized starting point
  weight_decay: 1e-4         # L2 regularization
  
  # Enhanced learning rate schedule
  learning_rate_schedule: "cosine_with_warmup"
  warmup_epochs: 2           # Quick warmup
  warmup_factor: 0.1         # Gradual warmup
  min_learning_rate: 1e-6    # Final LR
  
  # Gradient optimization
  gradient_clipping: true
  max_gradient_norm: 1.0
  gradient_accumulation_steps: 4  # Effective batch_size = 8*4 = 32
  
  # Enhanced regularization
  dropout_rate: 0.1
  use_mixed_precision: true
  
  # Early stopping
  early_stopping_patience: 10
  early_stopping_metric: "test_accuracy"
  early_stopping_mode: "max"

# üßÆ Mathematical Framework Integration
mathematical_framework:
  # Temporal InfoNCE (Equation 1)
  use_temporal_infonce: true
  temporal_context_length: 256
  temporal_negative_samples: 8
  
  # Adaptive Temperature Control (Section I)
  use_adaptive_temperature: true
  initial_temperature: 0.06
  temperature_learning_rate: 0.001
  temperature_bounds: [0.01, 0.16]
  
  # SNN Capacity (Section 2)
  simulation_time_steps: 256  # Optimized for memory
  simulation_dt: 0.25e-3
  
  # PAC-Bayes Regularization (Section C)
  use_pac_bayes_regularization: true
  pac_bayes_lambda: 0.01
  prior_variance: 1.0
  
  # Gradient Stability (Section H)
  gradient_stability_monitoring: true
  lyapunov_stability_threshold: 1e-6

# üöÄ Revolutionary Enhancements Integration
enhancements:
  # Enhancement 1: Adaptive Surrogate Gradients
  surrogate_adaptivity_factor: 2.0
  
  # Enhancement 2: Temporal Transformer (already in model)
  
  # Enhancement 3: Learnable Multi-Threshold (already in spike_bridge)
  
  # Enhancement 4: Enhanced LIF with Memory (already in snn)
  
  # Enhancement 5: Momentum-based InfoNCE
  use_momentum_negatives: true
  negative_momentum: 0.999
  hard_negative_ratio: 0.3
  curriculum_temperature: true

# üñ•Ô∏è Platform Configuration (CLI Required)
platform:
  device: "cpu"               # Explicitly set to CPU for memory safety
  precision: "float32"
  memory_fraction: 0.15       # Conservative memory usage
  enable_jit: true
  cache_compilation: true

# üíæ GPU Memory Optimization (T4 Compatible)
gpu_optimization:
  memory_fraction: 0.15      # Conservative memory usage
  enable_jit: true
  cache_compilation: true
  
  # 6-Stage GPU Warmup
  comprehensive_gpu_warmup: true
  warmup_stages: 6
  
  # XLA optimization
  xla_python_client_preallocate: false
  xla_python_client_mem_fraction: 0.15

# üìä Comprehensive Monitoring & Logging
monitoring:
  # Weights & Biases
  use_wandb: true
  wandb_project: "cpc_snn_gw_final_50_epochs"
  wandb_run_name: "revolutionary_final_training"
  
  # Logging frequency
  log_every_n_steps: 5      # Frequent logging for 50 epochs
  eval_every_n_epochs: 2    # Regular evaluation
  save_every_n_epochs: 5    # Checkpoint saving
  
  # Metrics tracking
  track_gradient_norms: true
  track_spike_patterns: true
  track_attention_weights: true
  track_temperature_adaptation: true
  
  # Output directory
  output_dir: "outputs/final_50_epochs_training"
  save_best_model: true
  save_final_model: true

# üéØ Performance Targets
targets:
  # Accuracy targets
  target_train_accuracy: 0.98
  target_test_accuracy: 0.95
  target_roc_auc: 0.98
  
  # Performance targets
  target_inference_time: 0.1   # <100ms
  target_energy_efficiency: 0.001  # J per detection
  target_spike_rate: 0.01     # 1% average spike rate
  
  # Quality metrics
  no_model_collapse: true
  gradient_flow_healthy: true
  cpc_loss_working: true      # Must be > 1e-6

# üî¨ Scientific Validation & Baselines
baselines:
  pycbc_template_bank: true    # Enable real PyCBC comparison
  matched_filtering: true
  statistical_significance: true
  confidence_level: 0.95

validation:
  # Test evaluation
  comprehensive_test_evaluation: true
  model_collapse_detection: true
  data_leakage_prevention: true
  
  # Statistical validation
  confidence_intervals: true
  bootstrap_samples: 1000
  statistical_significance: true
  
  # Baseline comparison
  compare_with_pycbc: true
  compare_with_matched_filtering: true

# üìä Evaluation Configuration (CLI Required) 
evaluation:
  metrics: ["roc_auc", "precision", "recall", "f1", "far"]
  target_accuracy: 0.95
  confidence_intervals: true
  bootstrap_samples: 1000
  statistical_tests: ["mcnemar", "wilcoxon"]

# üìù Logging Configuration (CLI Required)
logging:
  level: "INFO"
  use_wandb: true
  wandb_project: "cpc_snn_gw_final_50_epochs"
  checkpoint_dir: "outputs/final_50_epochs_training/checkpoints"
  save_every_n_epochs: 5
  log_every_n_steps: 5

# üö® Safety & Quality Assurance
safety:
  # Training stability
  nan_detection: true
  inf_detection: true
  gradient_explosion_detection: true
  
  # Memory safety
  memory_monitoring: true
  swap_detection: true
  oom_prevention: true
  
  # Model quality
  parameter_validation: true
  architecture_validation: true
  data_quality_validation: true 