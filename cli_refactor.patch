--- a/cli.py
+++ b/cli.py
@@ -1,1885 +1,50 @@
 #!/usr/bin/env python3
 """
-ML4GW-compatible CLI interface for CPC+SNN Neuromorphic GW Detection
+DEPRECATED: Legacy CLI interface - Use modular cli package instead.
 
-Production-ready command line interface following ML4GW standards.
+This file provides backward compatibility wrappers.
+New code should import from cli.commands, cli.parsers, cli.runners modules.
 """
 
-import argparse
-import logging
-import sys
-from pathlib import Path
-from typing import Optional
-
-import yaml
-import numpy as np
+import warnings
 
 try:
     from . import __version__
@@ -23,1862 +23,27 @@ except ImportError:
     except ImportError:
         __version__ = "0.1.0-dev"
 
-def _import_setup_logging():
-    """Lazy import setup_logging to avoid importing JAX too early."""
-    try:
-        from .utils import setup_logging as _sl
-    except ImportError:
-        from utils import setup_logging as _sl
-    return _sl
-
-# Optional imports (will be loaded when needed)
-try:
-    from .training.pretrain_cpc import main as cpc_train_main
-except ImportError:
-    try:
-        from training.pretrain_cpc import main as cpc_train_main
-    except ImportError:
-        cpc_train_main = None
-    
-try:
-    from .models.cpc_encoder import create_enhanced_cpc_encoder
-except ImportError:
-    try:
-        from models.cpc.factory import create_enhanced_cpc_encoder
-    except ImportError:
-        create_enhanced_cpc_encoder = None
-
-logger = logging.getLogger(__name__)
-
-def run_standard_training(config, args):
-    """Run real CPC+SNN training using CPCSNNTrainer."""
-    import time  # Import for training timing
-    import jax
-    import jax.numpy as jnp
-    
-    from .utils.device_auto_detection import setup_auto_device_optimization
-    from .training.base_trainer import CPCSNNTrainer, TrainingConfig
-    
-    logger = logging.getLogger(__name__)
-    
-    # Setup device optimization
-    setup_auto_device_optimization()
-    
-    logger.info("üöÄ Starting STANDARD CPC+SNN Training (Real Implementation)")
-    
-    # ‚úÖ SOLUTION: Real training configuration
-    training_config = TrainingConfig(
-        num_epochs=getattr(args, 'epochs', 100),
-        batch_size=getattr(args, 'batch_size', 32),
-        learning_rate=getattr(args, 'learning_rate', 1e-3),
-        num_classes=2,  # ‚úÖ BINARY: noise vs signal
-        
-        # ‚úÖ CPC Configuration
-        cpc_latent_dim=128,
-        cpc_hidden_dim=256,
-        cpc_num_layers=3,
-        cpc_prediction_steps=12,
-        
-        # ‚úÖ SNN Configuration  
-        snn_hidden_size=getattr(args, 'snn_hidden', 32),
-        snn_num_layers=3,
-        spike_time_steps=getattr(args, 'spike_time_steps', 24),
-        spike_threshold=getattr(args, 'spike_threshold', 0.1),
-        
-        # ‚úÖ Training Configuration
-        weight_decay=1e-4,
-        gradient_clip_value=1.0,
-        early_stopping_patience=15,
-        random_seed=getattr(args, 'random_seed', 42),
-        use_mixed_precision=True,
-        
-        # ‚úÖ Data Configuration
-        sequence_length=512,
-        sample_rate=2048,
-        
-        # ‚úÖ Advanced Configuration
-        use_class_weighting=True,
-        label_smoothing=0.1,
-        focal_loss_gamma=getattr(args, 'focal_gamma', 1.8),
-        class1_weight=getattr(args, 'class1_weight', 1.1)
-    )
-    
-    logger.info(f"üìä Training Configuration:")
-    logger.info(f"   - Epochs: {training_config.num_epochs}")
-    logger.info(f"   - Batch size: {training_config.batch_size}")
-    logger.info(f"   - Learning rate: {training_config.learning_rate}")
-    logger.info(f"   - Classes: {training_config.num_classes} (binary)")
-    logger.info(f"   - CPC latent dim: {training_config.cpc_latent_dim}")
-    logger.info(f"   - SNN hidden: {training_config.snn_hidden_size}")
-    logger.info(f"   - Spike time steps: {training_config.spike_time_steps}")
-    logger.info(f"   - Focal gamma: {training_config.focal_loss_gamma}")
-    logger.info(f"   - Class 1 weight: {training_config.class1_weight}")
-    
-    # ‚úÖ SOLUTION: Create trainer
-    trainer = CPCSNNTrainer(training_config)
-    
-    # ‚úÖ SOLUTION: Load or create dataset
-    logger.info("üìä Loading training dataset...")
-    
-    # Try different dataset sources in order of preference
-    train_signals = None
-    train_labels = None
-    test_signals = None
-    test_labels = None
-    
-    # 1. Try MLGWSC-1 dataset (preferred)
-    try:
-        from .implement_mlgwsc_loader import MLGWSCDataLoader
-        
-        logger.info("   üéØ Attempting MLGWSC-1 dataset loading...")
-        mlgwsc_loader = MLGWSCDataLoader()
-        
-        # Load MLGWSC-1 data
-        mlgwsc_data = mlgwsc_loader.load_mlgwsc_segments(
-            num_segments=2000,  # Realistic size
-            segment_duration=1.0,  # 1 second segments
-            random_seed=42
-        )
-        
-        if mlgwsc_data is not None and len(mlgwsc_data['segments']) > 0:
-            # ‚úÖ SUCCESS: Use MLGWSC-1 data
-            segments = mlgwsc_data['segments']
-            labels = mlgwsc_data['labels']
-            
-            logger.info(f"   ‚úÖ MLGWSC-1 dataset loaded: {len(segments)} segments")
-            logger.info(f"   üìä Label distribution: {jnp.bincount(labels)}")
-            
-            # Split data
-            import os
-            import numpy as np
-            from .utils.data_split import create_stratified_split
-            
-            train_signals, train_labels, test_signals, test_labels = create_stratified_split(
-                segments, labels, train_ratio=0.8, random_seed=42
-            )
-        else:
-            logger.warning("   ‚ö†Ô∏è MLGWSC-1 data not available, falling back...")
-            
-    except Exception as e:
-        logger.warning(f"   ‚ö†Ô∏è MLGWSC-1 loading failed: {e}")
-    
-    # 2. Try enhanced LIGO dataset
-    if train_signals is None:
-        try:
-            from .data.gw_preprocessor import AdvancedDataPreprocessor
-            
-            logger.info("   üåä Attempting enhanced LIGO dataset...")
-            
-            preprocessor = AdvancedDataPreprocessor(
-                sample_rate=2048,
-                bandpass=(20, 512),
-                apply_whitening=True
-            )
-            
-            # Create enhanced dataset
-            import jax.numpy as jnp
-            from .utils.jax_safety import safe_stack_to_device, safe_array_to_device
-            from .utils.data_split import create_stratified_split
-            from .data.real_ligo_integration import create_enhanced_ligo_dataset
-            
-            dataset = create_enhanced_ligo_dataset(
-                num_samples=1600,
-                sequence_length=512,
-                sample_rate=2048,
-                noise_ratio=0.65,  # 65% noise, 35% signal
-                random_seed=42
-            )
-            
-            if dataset is not None:
-                # ‚úÖ SUCCESS: Use enhanced LIGO data
-                all_signals = safe_array_to_device(dataset['data'])
-                all_labels = safe_array_to_device(dataset['labels'])
-                
-                logger.info(f"   ‚úÖ Enhanced LIGO dataset: {len(all_signals)} samples")
-                logger.info(f"   üìä Class distribution: {jnp.bincount(all_labels)}")
-                
-                train_signals, train_labels, test_signals, test_labels = create_stratified_split(
-                    all_signals, all_labels, train_ratio=0.8, random_seed=42
-                )
-            else:
-                logger.warning("   ‚ö†Ô∏è Enhanced LIGO data creation failed")
-                
-        except Exception as e:
-            logger.warning(f"   ‚ö†Ô∏è Enhanced LIGO loading failed: {e}")
-    
-    # 3. Try PyCBC enhanced dataset
-    if train_signals is None:
-        try:
-            from .utils.data_split import create_stratified_split
-            from .data.pycbc_integration import create_pycbc_enhanced_dataset
-            
-            logger.info("   üî¨ Attempting PyCBC enhanced dataset...")
-            
-            pycbc_dataset = create_pycbc_enhanced_dataset(
-                num_samples=1200,
-                sequence_length=512,
-                sample_rate=2048,
-                psd_name="aLIGOZeroDetHighPower",
-                apply_whitening=True,
-                random_seed=42
-            )
-            
-            if pycbc_dataset is not None:
-                # ‚úÖ SUCCESS: Use PyCBC data
-                import jax
-                from .utils.jax_safety import safe_stack_to_device, safe_array_to_device
-                
-                all_signals = safe_array_to_device(pycbc_dataset['data'])
-                all_labels = safe_array_to_device(pycbc_dataset['labels'])
-                
-                logger.info(f"   ‚úÖ PyCBC enhanced dataset: {len(all_signals)} samples")
-                logger.info(f"   üìä Class distribution: {jnp.bincount(all_labels)}")
-                
-                train_signals, train_labels, test_signals, test_labels = create_stratified_split(
-                    all_signals, all_labels, train_ratio=0.8, random_seed=42
-                )
-            else:
-                logger.warning("   ‚ö†Ô∏è PyCBC data creation failed")
-                
-        except Exception as e:
-            logger.warning(f"   ‚ö†Ô∏è PyCBC loading failed: {e}")
-    
-    # 4. Fallback: Standard dataset builder
-    if train_signals is None:
-        try:
-            from .data.gw_dataset_builder import create_evaluation_dataset
-            from .utils.data_split import create_stratified_split
-            
-            logger.info("   üìä Using fallback dataset builder...")
-            
-            dataset = create_evaluation_dataset(
-                num_samples=1000,
-                sequence_length=512,
-                sample_rate=2048,
-                random_key=42
-            )
-            
-            all_signals = dataset['data']
-            all_labels = dataset['labels']
-            
-            logger.info(f"   ‚úÖ Fallback dataset: {len(all_signals)} samples")
-            logger.info(f"   üìä Class distribution: {jnp.bincount(all_labels)}")
-            
-            train_signals, train_labels, test_signals, test_labels = create_stratified_split(
-                all_signals, all_labels, train_ratio=0.8, random_seed=42
-            )
-            
-        except Exception as e:
-            logger.error(f"   ‚ùå Fallback dataset creation failed: {e}")
-            raise RuntimeError("All dataset loading methods failed")
-    
-    # ‚úÖ VALIDATION: Ensure we have valid data
-    if train_signals is None or len(train_signals) == 0:
-        raise RuntimeError("No training data available")
-    
-    logger.info(f"üìä Final dataset split:")
-    logger.info(f"   - Training: {len(train_signals)} samples")
-    logger.info(f"   - Testing: {len(test_signals)} samples")
-    logger.info(f"   - Train class balance: {jnp.mean(train_labels):.1%} positive")
-    logger.info(f"   - Test class balance: {jnp.mean(test_labels):.1%} positive")
-    
-    # ‚úÖ SOLUTION: Run training
-    logger.info("üöÄ Starting CPC+SNN training...")
-    
-    start_time = time.time()
-    
-    # Use trainer's train method
-    training_results = trainer.train(
-        train_signals=train_signals,
-        train_labels=train_labels,
-        test_signals=test_signals,
-        test_labels=test_labels
-    )
-    
-    training_time = time.time() - start_time
-    
-    logger.info(f"‚úÖ Training completed in {training_time:.1f} seconds")
-    logger.info(f"üìä Final results:")
-    logger.info(f"   - Train accuracy: {training_results.get('train_accuracy', 0.0):.3f}")
-    logger.info(f"   - Test accuracy: {training_results.get('test_accuracy', 0.0):.3f}")
-    logger.info(f"   - Best epoch: {training_results.get('best_epoch', 0)}")
-    
-    return training_results
 
+# Backward compatibility wrappers with deprecation warnings
+def train_cmd():
+    """DEPRECATED: Use cli.commands.train.train_cmd instead."""
+    warnings.warn(
+        "Direct import from cli.py is deprecated. Use: from cli.commands import train_cmd",
+        DeprecationWarning,
+        stacklevel=2
+    )
+    from .cli.commands.train import train_cmd as _train_cmd
+    return _train_cmd()
 
-# Placeholder for the _M class used in training
-class _M: pass
-
-# Placeholder implementations for functions referenced in the original
-def run_enhanced_training(config, args):
-    """Run enhanced training with mixed continuous+binary dataset."""
-    try:
-        from .training.enhanced_gw_training import EnhancedGWTrainer
-        
-        logger.info("üöÄ Starting ENHANCED CPC+SNN Training")
-        logger.info("üåü Features: Temporal Transformer + Learnable Thresholds + Enhanced LIF")
-        
-        # Create enhanced config with 3 classes
-        from .training.enhanced_gw_training import EnhancedGWConfig
-        
-        enhanced_config = EnhancedGWConfig(
-            num_epochs=getattr(args, 'epochs', 100),
-            batch_size=getattr(args, 'batch_size', 32),
-            learning_rate=getattr(args, 'learning_rate', 1e-3),
-            num_classes=3,  # ‚úÖ ENHANCED: noise, continuous_gw, binary_merger
-            random_seed=getattr(args, 'random_seed', 42)
-        )
-        
-        trainer = EnhancedGWTrainer(enhanced_config)
-        
-        # Run enhanced training pipeline
-        training_results = trainer.run_full_training_pipeline()
-        
-        logger.info("‚úÖ Enhanced training completed")
-        return training_results
-        
-    except Exception as e:
-        logger.error(f"‚ùå Enhanced training failed: {e}")
-        raise
-
-def run_advanced_training(config, args):
-    """Run advanced training mapped to EnhancedGWTrainer full pipeline (no mocks)."""
-    try:
-        try:
-            from .training.enhanced_gw_training import EnhancedGWTrainer
-            from .training.enhanced_gw_training import EnhancedGWConfig
-        except ImportError:
-            from training.enhanced_gw_training import EnhancedGWTrainer, EnhancedGWConfig
-        
-        logger.info("üöÄ Starting ADVANCED CPC+SNN Training (mapped to Enhanced)")
-        logger.info("üî¨ Advanced = Enhanced pipeline with full features")
-        
-        # Use enhanced config for advanced training
-        enhanced_config = EnhancedGWConfig(
-            num_epochs=getattr(args, 'epochs', 100),
-            batch_size=getattr(args, 'batch_size', 32),
-            learning_rate=getattr(args, 'learning_rate', 1e-3),
-            num_classes=3,  # Advanced uses 3-class system
-            random_seed=getattr(args, 'random_seed', 42)
-        )
-        
-        trainer = EnhancedGWTrainer(enhanced_config)
-        training_results = trainer.run_full_training_pipeline()
-        
-        logger.info("‚úÖ Advanced training completed")
-        return training_results
-        
-    except Exception as e:
-        logger.error(f"‚ùå Advanced training failed: {e}")
-        raise
-
-def run_complete_enhanced_training(config, args):
-    """Run complete enhanced training with ALL 5 revolutionary improvements."""
-    try:
-        from training.complete_enhanced_training import CompleteEnhancedTrainer, CompleteEnhancedConfig
-        
-        logger.info("üöÄ Starting COMPLETE ENHANCED Training")
-        logger.info("üåü ALL 5 Revolutionary Improvements:")
-        logger.info("   1. üß† Adaptive Multi-Scale Surrogate Gradients")
-        logger.info("   2. üîÑ Temporal Transformer with Multi-Scale Convolution")
-        logger.info("   3. üéØ Learnable Multi-Threshold Spike Encoding")
-        logger.info("   4. üíæ Enhanced LIF with Memory and Refractory Period")
-        logger.info("   5. üöÄ Momentum-based InfoNCE with Hard Negative Mining")
-        
-        # Create complete enhanced config
-        complete_config = CompleteEnhancedConfig(
-            num_epochs=getattr(args, 'epochs', 100),
-            batch_size=getattr(args, 'batch_size', 32),
-            learning_rate=getattr(args, 'learning_rate', 1e-3),
-            num_classes=3,  # Complete enhanced uses 3-class system
-            
-            # ‚úÖ 1. ADAPTIVE MULTI-SCALE SURROGATE GRADIENTS
-            surrogate_gradient_type="adaptive_multi_scale",
-            surrogate_gradient_beta=4.0,
-            surrogate_adaptation_rate=0.1,
-            surrogate_beta_schedule="exponential",
-            
-            # ‚úÖ 2. TEMPORAL TRANSFORMER WITH MULTI-SCALE CONVOLUTION
-            use_temporal_transformer=True,
-            transformer_num_heads=8,
-            transformer_num_layers=4,
-            
-            # 3. Learnable Multi-Threshold Spike Encoding
-            use_learnable_thresholds=True,
-            num_threshold_levels=3,
-            threshold_adaptation_rate=0.01,
-            
-            # 4. Enhanced LIF with Memory and Refractory Period
-            use_enhanced_lif=True,
-            lif_membrane_tau=20e-3,
-            lif_refractory_period=2e-3,
-            use_long_term_memory=True,
-            memory_decay=0.95,
-            
-            # 5. Momentum-based InfoNCE with Hard Negative Mining
-            use_momentum_negatives=True,
-            momentum_coefficient=0.999,
-            hard_negative_ratio=0.3,
-            memory_bank_size=65536,
-            
-            random_seed=getattr(args, 'random_seed', 42)
-        )
-        
-        trainer = CompleteEnhancedTrainer(complete_config)
-        training_results = trainer.run_complete_training_pipeline()
-        
-        logger.info("‚úÖ Complete enhanced training finished")
-        logger.info(f"üìä Final accuracy: {training_results.get('test_accuracy', 0.0):.3f}")
-        
-        return training_results
-        
-    except Exception as e:
-        logger.error(f"‚ùå Complete enhanced training failed: {e}")
-        raise
-
-def get_base_parser():
-    """Create base argument parser with common options."""
-    parser = argparse.ArgumentParser(add_help=False)
-    
-    # Global options
-    parser.add_argument(
-        "--config", "-c",
-        type=Path,
-        default=Path("config.yaml"),
-        help="Configuration file path"
-    )
-    
-    parser.add_argument(
-        "--verbose", "-v",
-        action="count",
-        default=0,
-        help="Increase verbosity (use -v, -vv, or -vvv)"
-    )
-    
-    parser.add_argument(
-        "--log-file",
-        type=Path,
-        help="Log file path"
-    )
-    
-    parser.add_argument(
-        "--random-seed",
-        type=int,
-        default=42,
-        help="Random seed for reproducibility"
-    )
-    
-    parser.add_argument(
-        "--device",
-        type=str,
-        choices=["auto", "cpu", "gpu", "tpu"],
-        default="auto",
-        help="Device to use for computation"
-    )
-    
-    return parser
-
-def train_cmd():
-    """Main training command entry point."""
-    parser = get_base_parser()
-    parser.description = "Train CPC+SNN neuromorphic gravitational wave detector"
-    
-    # Training specific arguments
-    parser.add_argument(
-        "--output-dir", "-o",
-        type=Path,
-        default=Path("./outputs"),
-        help="Output directory for training artifacts"
-    )
-    
-    parser.add_argument(
-        "--data-dir",
-        type=Path, 
-        default=Path("./data"),
-        help="Data directory"
-    )
-    
-    parser.add_argument(
-        "--epochs",
-        type=int,
-        default=100,
-        help="Number of training epochs"
-    )
-    
-    parser.add_argument(
-        "--batch-size",
-        type=int,
-        default=32,
-        help="Training batch size"
-    )
-    # SpikeBridge hyperparameters via CLI
-    parser.add_argument("--spike-time-steps", type=int, default=24, help="SpikeBridge time steps T")
-    parser.add_argument("--spike-threshold", type=float, default=0.1, help="Base threshold for encoders")
-    parser.add_argument("--spike-learnable", action="store_true", help="Use learnable multi-threshold encoding")
-    parser.add_argument("--no-spike-learnable", dest="spike_learnable", action="store_false", help="Disable learnable encoding")
-    parser.set_defaults(spike_learnable=True)
-    parser.add_argument("--spike-threshold-levels", type=int, default=4, help="Number of threshold levels")
-    parser.add_argument("--spike-surrogate-type", type=str, default="adaptive_multi_scale", help="Surrogate type for spikes")
-    parser.add_argument("--spike-surrogate-beta", type=float, default=4.0, help="Surrogate beta")
-    parser.add_argument("--spike-pool-seq", action="store_true", help="Enable pooling over seq dimension before SNN")
-    # CPC/Transformer params
-    parser.add_argument("--cpc-heads", type=int, default=8, help="Temporal attention heads")
-    parser.add_argument("--cpc-layers", type=int, default=4, help="Temporal transformer layers")
-    # SNN params
-    parser.add_argument("--snn-hidden", type=int, default=32, help="SNN hidden size")
-    # Early stop and thresholding
-    parser.add_argument("--balanced-early-stop", action="store_true", help="Use balanced accuracy/F1 early stopping")
-    parser.add_argument("--opt-threshold", action="store_true", help="Optimize decision threshold by F1/balanced acc on test per epoch")
-    
-    parser.add_argument(
-        "--learning-rate", "--lr",
-        type=float,
-        default=1e-3,
-        help="Learning rate"
-    )
-    parser.add_argument(
-        "--window-size",
-        type=int,
-        default=512,
-        help="Window size for real LIGO dataset windows"
-    )
-    parser.add_argument(
-        "--overlap",
-        type=float,
-        default=0.5,
-        help="Overlap ratio for windowing (0.0-0.99)"
-    )
-    parser.add_argument(
-        "--use-pycbc",
-        action="store_true",
-        help="Use PyCBC-enhanced synthetic dataset if available"
-    )
-    # PyCBC simulation controls
-    parser.add_argument(
-        "--pycbc-psd",
-        type=str,
-        default="aLIGOZeroDetHighPower",
-        help="PyCBC PSD name (e.g., aLIGOZeroDetHighPower, aLIGOLateHighSensitivity)")
-    parser.add_argument(
-        "--pycbc-whiten",
-        dest="pycbc_whiten",
-        action="store_true",
-        help="Enable PyCBC time-domain whitening"
-    )
-    parser.add_argument(
-        "--no-pycbc-whiten",
-        dest="pycbc_whiten",
-        action="store_false",
-        help="Disable PyCBC time-domain whitening"
-    )
-    parser.set_defaults(pycbc_whiten=True)
-    parser.add_argument(
-        "--pycbc-multi-channel",
-        action="store_true",
-        help="Return H1/L1 as 2-channel inputs (else averaged)"
-    )
-    parser.add_argument(
-        "--pycbc-detectors",
-        type=str,
-        nargs="+",
-        default=["H1", "L1"],
-        help="List of detectors for PyCBC (e.g., H1 L1 V1)"
-    )
-    # MLGWSC-1 controls
-    parser.add_argument(
-        "--mlgwsc-days",
-        type=int,
-        default=4,
-        help="Number of days of MLGWSC-1 data to load (1-30)"
-    )
-    parser.add_argument(
-        "--mlgwsc-slice-seconds",
-        type=float,
-        default=1.25,
-        help="Slice length in seconds for MLGWSC-1 segments"
-    )
-    parser.add_argument(
-        "--mlgwsc-stride-seconds",
-        type=float,
-        default=1.0,
-        help="Stride in seconds for MLGWSC-1 segment sampling"
-    )
-    parser.add_argument(
-        "--k-folds",
-        type=int,
-        default=0,
-        help="Number of folds for stratified K-fold (0 disables K-fold)"
-    )
-    parser.add_argument(
-        "--training-mode",
-        type=str,
-        choices=["standard", "enhanced", "advanced", "complete_enhanced"],
-        default="standard",
-        help="Training mode to use"
-    )
-    
-    # Parse arguments
-    args = parser.parse_args()
-    
-    # Setup logging
-    _sl = _import_setup_logging()
-    _sl(
-        level=logging.INFO if args.verbose == 0 else logging.DEBUG,
-        log_file=args.log_file
-    )
-    
-    logger.info(f"üöÄ Starting CPC+SNN training (v{__version__})")
-    logger.info(f"   Mode: {args.training_mode}")
-    logger.info(f"   Output: {args.output_dir}")
-    logger.info(f"   Epochs: {args.epochs}")
-    logger.info(f"   Batch size: {args.batch_size}")
-    
-    # Create output directory
-    args.output_dir.mkdir(parents=True, exist_ok=True)
-    
-    # Load configuration
-    from .utils.config import load_config
-    config = load_config(args.config)
-    
-    # Route to appropriate training function based on mode
-    try:
-        if args.training_mode == "enhanced":
-            logger.info("üåü Using ENHANCED training mode")
-            return run_enhanced_training(config, args)
-        elif args.training_mode == "advanced":
-            logger.info("üî¨ Using ADVANCED training mode")  
-            return run_advanced_training(config, args)
-        elif args.training_mode == "complete_enhanced":
-            logger.info("üöÄ Using COMPLETE ENHANCED training mode")
-            return run_complete_enhanced_training(config, args)
-        else:  # standard
-            logger.info("üìä Using STANDARD training mode")
-            return run_standard_training(config, args)
-    except KeyboardInterrupt:
-        logger.info("‚ö†Ô∏è Training interrupted by user")
-        return 1
-    except Exception as e:
-        logger.error(f"‚ùå Training failed: {e}")
-        import traceback
-        traceback.print_exc()
-        return 1
+def eval_cmd():
+    """DEPRECATED: Use cli.commands.evaluate.eval_cmd instead."""
+    warnings.warn(
+        "Direct import from cli.py is deprecated. Use: from cli.commands import eval_cmd",
+        DeprecationWarning,
+        stacklevel=2
+    )
+    from .cli.commands.evaluate import eval_cmd as _eval_cmd
+    return _eval_cmd()
 
-# ... [Continuing with the rest of the functions - eval_cmd, infer_cmd, main - following the same pattern]
+def infer_cmd():
+    """DEPRECATED: Use cli.commands.inference.infer_cmd instead."""
+    warnings.warn(
+        "Direct import from cli.py is deprecated. Use: from cli.commands import infer_cmd",
+        DeprecationWarning,
+        stacklevel=2
+    )
+    from .cli.commands.inference import infer_cmd as _infer_cmd
+    return _infer_cmd()
 
+def main():
+    """DEPRECATED: Use cli.main.main instead."""
+    warnings.warn(
+        "Direct import from cli.py is deprecated. Use: from cli.main import main",
+        DeprecationWarning,
+        stacklevel=2
+    )
+    from .cli.main import main as _main
+    return _main()
+
+# Legacy compatibility
+get_base_parser = lambda: __import__('warnings').warn(
+    "get_base_parser from cli.py is deprecated. Use: from cli.parsers import get_base_parser",
+    DeprecationWarning, stacklevel=2
+) or __import__('cli.parsers', fromlist=['get_base_parser']).get_base_parser()
