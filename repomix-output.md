This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-07-16 19:46:14

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
__init__.py
_version.py
cli.py
config.yaml
data
  __init__.py
  continuous_gw_generator.py
  data_loader.py
  gw_download.py
models
  __init__.py
  cpc_encoder.py
  simple_snn.py
  snn_classifier.py
  spike_bridge.py
training
  __init__.py
  advanced_training.py
  enhanced_gw_training.py
  pretrain_cpc.py
utils
  __init__.py
repomix-output.md
```

# Repository Files


## __init__.py

```python
"""
CPC+SNN Neuromorphic Gravitational Wave Detection

World's first neuromorphic gravitational wave detector using 
Contrastive Predictive Coding + Spiking Neural Networks.

Designed for production deployment following ML4GW standards.
"""

try:
    from ._version import __version__
except ImportError:
    __version__ = "0.1.0-dev"

# Core API exports
from .models.cpc_encoder import (
    CPCEncoder,
    create_enhanced_cpc_encoder,
    create_standard_cpc_encoder,
    info_nce_loss,
    enhanced_info_nce_loss,
)

from .models.simple_snn import (
    SimpleSNN,
    LIFLayer,
)

from .models.snn_classifier import (
    SNNClassifier,
    create_snn_classifier,
)

from .models.spike_bridge import (
    SpikeBridge,
    SpikeEncodingStrategy,
    create_default_spike_bridge,
    create_fast_spike_bridge,
    create_robust_spike_bridge,
)

from .data.gw_download import (
    ProductionGWOSCDownloader,
    AdvancedDataPreprocessor,
)

# Training utilities available at module level

__all__ = [
    "__version__",
    
    # Models
    "CPCEncoder", 
    "create_enhanced_cpc_encoder",
    "create_standard_cpc_encoder",
    "info_nce_loss",
    "enhanced_info_nce_loss",
    "SimpleSNN",
    "LIFLayer",
    "SNNClassifier", 
    "create_snn_classifier",
    "SpikeBridge",
    "SpikeEncodingStrategy",
    "create_default_spike_bridge",
    "create_fast_spike_bridge", 
    "create_robust_spike_bridge",
    
    # Data
    "ProductionGWOSCDownloader",
    "AdvancedDataPreprocessor",
]

# Package metadata
__title__ = "ligo-cpc-snn"
__description__ = "Neuromorphic gravitational wave detection using CPC + Spiking Neural Networks"
__author__ = "Gracjan"
__email__ = "contact@ml4gw-neuromorphic.org"
__license__ = "MIT"
__copyright__ = "Copyright 2025 CPC+SNN Neuromorphic GW Detection Project"
```

## _version.py

```python
# file generated by setuptools-scm
# don't change, don't track in version control

__all__ = ["__version__", "__version_tuple__", "version", "version_tuple"]

TYPE_CHECKING = False
if TYPE_CHECKING:
    from typing import Tuple
    from typing import Union

    VERSION_TUPLE = Tuple[Union[int, str], ...]
else:
    VERSION_TUPLE = object

version: str
__version__: str
__version_tuple__: VERSION_TUPLE
version_tuple: VERSION_TUPLE

__version__ = version = '0.1.dev1+ge22771a.d20250628'
__version_tuple__ = version_tuple = (0, 1, 'dev1', 'ge22771a.d20250628')
```

## cli.py

```python
#!/usr/bin/env python3
"""
ML4GW-compatible CLI interface for CPC+SNN Neuromorphic GW Detection

Production-ready command line interface following ML4GW standards.
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import Optional

import yaml

from . import __version__
from .utils import setup_logging

# Optional imports (will be loaded when needed)
try:
    from .training.pretrain_cpc import main as cpc_train_main
except ImportError:
    cpc_train_main = None
    
try:
    from .models.cpc_encoder import create_enhanced_cpc_encoder
except ImportError:
    create_enhanced_cpc_encoder = None

logger = logging.getLogger(__name__)


def get_base_parser() -> argparse.ArgumentParser:
    """Create base argument parser with common options."""
    parser = argparse.ArgumentParser(
        description="CPC+SNN Neuromorphic Gravitational Wave Detection",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "--version", 
        action="version", 
        version=f"ligo-cpc-snn {__version__}"
    )
    
    parser.add_argument(
        "--config", 
        type=Path,
        help="Configuration file path"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="count", 
        default=0,
        help="Increase verbosity level"
    )
    
    parser.add_argument(
        "--log-file",
        type=Path,
        help="Log file path"
    )
    
    return parser


def train_cmd():
    """Main training command entry point."""
    parser = get_base_parser()
    parser.description = "Train CPC+SNN neuromorphic gravitational wave detector"
    
    # Training specific arguments
    parser.add_argument(
        "--output-dir", "-o",
        type=Path,
        default=Path("./outputs"),
        help="Output directory for training artifacts"
    )
    
    parser.add_argument(
        "--data-dir",
        type=Path, 
        default=Path("./data"),
        help="Data directory"
    )
    
    parser.add_argument(
        "--epochs",
        type=int,
        default=100,
        help="Number of training epochs"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=32,
        help="Training batch size"
    )
    
    parser.add_argument(
        "--learning-rate", "--lr",
        type=float,
        default=1e-3,
        help="Learning rate"
    )
    
    parser.add_argument(
        "--gpu",
        action="store_true",
        help="Use GPU acceleration"
    )
    
    parser.add_argument(
        "--wandb",
        action="store_true", 
        help="Enable Weights & Biases logging"
    )
    
    parser.add_argument(
        "--checkpoint",
        type=Path,
        help="Resume from checkpoint"
    )
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(
        level=logging.INFO if args.verbose == 0 else logging.DEBUG,
        log_file=args.log_file
    )
    
    logger.info(f"ðŸš€ Starting CPC+SNN training (v{__version__})")
    logger.info(f"   Output directory: {args.output_dir}")
    logger.info(f"   Configuration: {args.config or 'default'}")
    
    # Load configuration
    config = {}
    if args.config and args.config.exists():
        with open(args.config) as f:
            config = yaml.safe_load(f)
        logger.info(f"âœ… Loaded configuration from {args.config}")
    
    # Override config with CLI arguments
    config.update({
        'output_dir': str(args.output_dir),
        'data_dir': str(args.data_dir),
        'epochs': args.epochs,
        'batch_size': args.batch_size, 
        'learning_rate': args.learning_rate,
        'use_gpu': args.gpu,
        'use_wandb': args.wandb,
        'checkpoint_path': str(args.checkpoint) if args.checkpoint else None,
    })
    
    # Create output directory
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # Save final configuration
    config_path = args.output_dir / "config.yaml"
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    logger.info(f"ðŸ’¾ Saved configuration to {config_path}")
    
    try:
        # Run training
        success = cpc_train_main(config)
        
        if success:
            logger.info("ðŸŽ‰ Training completed successfully!")
            return 0
        else:
            logger.error("âŒ Training failed")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ Training failed with error: {e}")
        import traceback
        traceback.print_exc()
        return 1


def eval_cmd():
    """Main evaluation command entry point."""
    parser = get_base_parser()
    parser.description = "Evaluate CPC+SNN neuromorphic gravitational wave detector"
    
    # Evaluation specific arguments
    parser.add_argument(
        "--model-path", "-m",
        type=Path,
        required=True,
        help="Path to trained model"
    )
    
    parser.add_argument(
        "--test-data",
        type=Path,
        help="Test data directory or file"
    )
    
    parser.add_argument(
        "--output-dir", "-o", 
        type=Path,
        default=Path("./evaluation"),
        help="Output directory for evaluation results"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=64,
        help="Evaluation batch size"
    )
    
    parser.add_argument(
        "--save-predictions",
        action="store_true",
        help="Save model predictions"
    )
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(
        level=logging.INFO if args.verbose == 0 else logging.DEBUG,
        log_file=args.log_file
    )
    
    logger.info(f"ðŸ” Starting CPC+SNN evaluation (v{__version__})")
    logger.info(f"   Model: {args.model_path}")
    logger.info(f"   Output: {args.output_dir}")
    
    # Create output directory
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # TODO: Implement evaluation logic
    logger.info("âš ï¸ Evaluation implementation coming soon...")
    return 0


def infer_cmd():
    """Main inference command entry point."""
    parser = get_base_parser()
    parser.description = "Run inference with CPC+SNN neuromorphic gravitational wave detector"
    
    # Inference specific arguments
    parser.add_argument(
        "--model-path", "-m",
        type=Path,
        required=True,
        help="Path to trained model"
    )
    
    parser.add_argument(
        "--input-data",
        type=Path,
        required=True,
        help="Input data file or directory"
    )
    
    parser.add_argument(
        "--output-dir", "-o",
        type=Path, 
        default=Path("./inference"),
        help="Output directory for inference results"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1,
        help="Inference batch size"
    )
    
    parser.add_argument(
        "--real-time",
        action="store_true",
        help="Enable real-time inference mode"
    )
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(
        level=logging.INFO if args.verbose == 0 else logging.DEBUG,
        log_file=args.log_file
    )
    
    logger.info(f"âš¡ Starting CPC+SNN inference (v{__version__})")
    logger.info(f"   Model: {args.model_path}")
    logger.info(f"   Input: {args.input_data}")
    logger.info(f"   Output: {args.output_dir}")
    
    # Create output directory
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # TODO: Implement inference logic
    logger.info("âš ï¸ Inference implementation coming soon...")
    return 0


if __name__ == "__main__":
    sys.exit(train_cmd())
```

## config.yaml

```yaml
# LIGO CPC+SNN Configuration
# Neuromorphic Gravitational Wave Detection Pipeline

# Environment & Platform
platform:
  device: "metal"  # metal, cpu, gpu
  precision: "float32"
  enable_x64: false

# Data Configuration  
data:
  sample_rate: 4096  # Hz
  segment_duration: 4.0  # seconds
  detectors: ["H1", "L1"]  # LIGO Hanford, Livingston
  preprocessing:
    whitening: true
    bandpass: [20.0, 1024.0]  # Hz range
    qtransform: false

# CPC Encoder Configuration
cpc:
  latent_dim: 256
  downsample_factor: 16
  context_length: 12  # timesteps for prediction
  num_negatives: 128  # for InfoNCE loss
  temperature: 0.1
  architecture:
    conv_channels: [32, 64, 128]
    kernel_size: 9
    stride: 2
    gru_hidden: 256

# Spike Bridge Configuration  
spike_bridge:
  encoding: "poisson"  # poisson, temporal_contrast
  spike_rate_max: 100.0  # Hz
  dt: 0.001  # 1ms timesteps
  threshold: 0.1  # for temporal contrast

# SNN Classifier Configuration
snn:
  hidden_size: 128
  num_layers: 2
  neuron_type: "LIF"  # LIF, ALIF, IF
  tau_mem: 0.020  # 20ms membrane time constant
  tau_syn: 0.005  # 5ms synaptic time constant  
  threshold: 1.0
  num_classes: 2  # GW / No-GW

# Training Configuration
training:
  # Phase 1: CPC Pretraining
  cpc_pretrain:
    steps: 100000
    batch_size: 16
    learning_rate: 0.003
    optimizer: "adam"
    grad_clip: 1.0
    
  # Phase 2: SNN Training (frozen CPC)
  snn_train:
    steps: 10000  
    batch_size: 16
    learning_rate: 0.001
    optimizer: "adam"
    grad_clip: 0.5
    
  # Phase 3: Joint Fine-tuning
  joint_finetune:
    steps: 5000
    batch_size: 8  # smaller for memory
    learning_rate: 0.0001
    optimizer: "adam"
    grad_clip: 0.1

# Evaluation Metrics
evaluation:
  metrics: ["roc_auc", "accuracy", "precision", "recall"]
  target_far: 0.033  # 1/30 days false alarm rate
  target_tpr: 0.95   # 95% true positive rate

# Logging & Monitoring
logging:
  level: "INFO"
  log_dir: "logs/"
  wandb_project: "ligo-cpc-snn"
  save_checkpoints: true
  checkpoint_dir: "checkpoints/"
```

## data/__init__.py

```python
"""
Data Module: GWOSC Gravitational Wave Data Pipeline

Handles downloading, preprocessing, and quality validation of
LIGO gravitational wave data from GWOSC (Gravitational Wave Open Science Center).
"""

from .gw_download import (
    GWOSCDownloader, 
    DataPreprocessor,
    ProductionGWOSCDownloader,
    AdvancedDataPreprocessor,
    QualityMetrics,
    ProcessingResult
)

__all__ = [
    "GWOSCDownloader",
    "DataPreprocessor", 
    "ProductionGWOSCDownloader",
    "AdvancedDataPreprocessor",
    "QualityMetrics",
    "ProcessingResult",
]
```

## data/continuous_gw_generator.py

```python
#!/usr/bin/env python3

"""
Continuous Gravitational Wave Signal Generation

Integrates PyFstat F-statistic based continuous GW generation 
with our CPC+SNN neuromorphic detection pipeline.

Complements existing GWOSC binary detection capabilities.
"""

import numpy as np
import jax
import jax.numpy as jnp
import logging
from typing import Tuple, Dict, Optional, List
from dataclasses import dataclass
from pathlib import Path

try:
    import pyfstat
    PYFSTAT_AVAILABLE = True
except ImportError:
    PYFSTAT_AVAILABLE = False
    pyfstat = None

logger = logging.getLogger(__name__)


@dataclass
class ContinuousGWParams:
    """Parameters for continuous gravitational wave signals."""
    frequency: float  # Signal frequency (Hz)
    frequency_dot: float = 0.0  # Frequency derivative (Hz/s)
    alpha: float = 0.0  # Right ascension (rad)
    delta: float = 0.0  # Declination (rad)  
    amplitude_h0: float = 1e-24  # GW amplitude
    cosi: float = 0.0  # Cosine of inclination angle
    psi: float = 0.0  # Polarization angle (rad)
    phi0: float = 0.0  # Initial phase (rad)


@dataclass 
class SignalConfiguration:
    """Configuration for signal generation."""
    t_start: int  # GPS start time
    t_end: int  # GPS end time
    sampling_rate: int = 4096  # Hz
    detectors: List[str] = None  # Detector names
    noise_sqrt_sh: float = 1e-23  # Noise level
    
    def __post_init__(self):
        if self.detectors is None:
            self.detectors = ['H1', 'L1']  # LIGO Hanford + Livingston


class ContinuousGWGenerator:
    """
    Continuous Gravitational Wave Signal Generator.
    
    Uses PyFstat for F-statistic based signal generation
    compatible with our neuromorphic CPC+SNN pipeline.
    """
    
    def __init__(self, 
                 base_frequency: float = 50.0,
                 freq_range: Tuple[float, float] = (20.0, 200.0),
                 duration: float = 1000.0,  # seconds
                 output_dir: Optional[str] = None):
        """
        Initialize continuous GW signal generator.
        
        Args:
            base_frequency: Base signal frequency (Hz)
            freq_range: Frequency range for signals (Hz)
            duration: Signal duration (seconds)
            output_dir: Directory for signal cache
        """
        if not PYFSTAT_AVAILABLE:
            raise ImportError(
                "PyFstat not available. Install with: pip install pyfstat"
            )
            
        self.base_frequency = base_frequency
        self.freq_range = freq_range
        self.duration = duration
        self.output_dir = Path(output_dir) if output_dir else Path("./continuous_gw_cache")
        self.output_dir.mkdir(exist_ok=True)
        
        # Signal parameters
        self.sampling_rate = 4096  # Hz (compatible with GWOSC)
        self.detectors = ['H1', 'L1']
        
        logger.info(f"Initialized Continuous GW Generator")
        logger.info(f"  Base frequency: {base_frequency} Hz")
        logger.info(f"  Frequency range: {freq_range} Hz")
        logger.info(f"  Duration: {duration} s")
        logger.info(f"  Output directory: {self.output_dir}")
        
    def generate_signal_parameters(self, 
                                 num_signals: int = 10,
                                 seed: int = 42) -> List[ContinuousGWParams]:
        """
        Generate random continuous GW signal parameters.
        
        Args:
            num_signals: Number of signals to generate
            seed: Random seed for reproducibility
            
        Returns:
            List of ContinuousGWParams objects
        """
        np.random.seed(seed)
        
        params_list = []
        
        for i in range(num_signals):
            # Frequency: uniform in range
            freq = np.random.uniform(self.freq_range[0], self.freq_range[1])
            
            # Frequency derivative: small values typical for pulsars
            freq_dot = np.random.uniform(-1e-8, 1e-8)  # Hz/s
            
            # Sky position: uniform on sphere
            alpha = np.random.uniform(0, 2*np.pi)  # Right ascension
            cos_delta = np.random.uniform(-1, 1)  
            delta = np.arccos(cos_delta) - np.pi/2  # Declination
            
            # Amplitude: log-uniform distribution
            h0 = np.random.uniform(1e-26, 1e-22)
            
            # Orientation parameters
            cosi = np.random.uniform(-1, 1)  # Cosine inclination
            psi = np.random.uniform(0, np.pi)  # Polarization angle
            phi0 = np.random.uniform(0, 2*np.pi)  # Initial phase
            
            params = ContinuousGWParams(
                frequency=freq,
                frequency_dot=freq_dot,
                alpha=alpha,
                delta=delta,
                amplitude_h0=h0,
                cosi=cosi,
                psi=psi,
                phi0=phi0
            )
            
            params_list.append(params)
            
        logger.info(f"Generated {num_signals} continuous GW parameter sets")
        return params_list
    
    def create_synthetic_timeseries(self, 
                                  params: ContinuousGWParams,
                                  duration: float = 4.0,
                                  sampling_rate: int = 4096,
                                  noise_level: float = 1e-23) -> jnp.ndarray:
        """
        Create synthetic continuous GW timeseries (simplified approach).
        
        Args:
            params: Signal parameters
            duration: Signal duration (seconds)
            sampling_rate: Sampling rate (Hz)
            noise_level: Noise amplitude
            
        Returns:
            Timeseries data array
        """
        # Time array
        t = jnp.arange(0, duration, 1/sampling_rate)
        
        # Continuous GW signal model (plus polarization)
        omega = 2 * jnp.pi * params.frequency
        
        # Plus polarization
        h_plus = (
            params.amplitude_h0 * 
            (1 + params.cosi**2) / 2 * 
            jnp.cos(omega * t + params.phi0)
        )
        
        # Cross polarization  
        h_cross = (
            params.amplitude_h0 * 
            params.cosi * 
            jnp.sin(omega * t + params.phi0)
        )
        
        # Detector response (simplified)
        signal = (
            h_plus * jnp.cos(2 * params.psi) + 
            h_cross * jnp.sin(2 * params.psi)
        )
        
        # Add realistic Gaussian noise
        key = jax.random.PRNGKey(42)
        noise = jax.random.normal(key, shape=signal.shape) * noise_level
        
        return signal + noise
    
    def generate_training_dataset(self, 
                                num_signals: int = 100,
                                signal_duration: float = 4.0,  # seconds
                                include_noise_only: bool = True) -> Dict:
        """
        Generate training dataset with continuous GW signals + noise.
        
        Args:
            num_signals: Number of signals to generate
            signal_duration: Duration of each signal (seconds)
            include_noise_only: Whether to include noise-only samples
            
        Returns:
            Dictionary with training data and labels
        """
        logger.info(f"Generating continuous GW training dataset ({num_signals} signals)")
        
        # Generate signal parameters
        params_list = self.generate_signal_parameters(num_signals)
        
        all_data = []
        all_labels = []
        all_metadata = []
        
        # Generate signal + noise samples  
        for i, params in enumerate(params_list):
            signal_data = self.create_synthetic_timeseries(
                params, 
                duration=signal_duration,
                sampling_rate=self.sampling_rate,
                noise_level=1e-23
            )
            
            all_data.append(signal_data)
            all_labels.append(1)  # Signal present
            all_metadata.append({
                'signal_type': 'continuous_gw',
                'frequency': params.frequency,
                'amplitude': params.amplitude_h0,
                'detector': 'H1'
            })
            
        # Generate noise-only samples if requested
        if include_noise_only:
            import jax
            key = jax.random.PRNGKey(123)
            
            for i in range(num_signals):
                # Pure noise
                noise_length = int(signal_duration * self.sampling_rate)
                noise_data = jax.random.normal(key, (noise_length,)) * 1e-23
                key, _ = jax.random.split(key)
                
                all_data.append(noise_data)
                all_labels.append(0)  # No signal
                all_metadata.append({
                    'signal_type': 'noise_only',
                    'frequency': None,
                    'amplitude': None,
                    'detector': 'H1'
                })
        
        # Convert to arrays
        data_array = jnp.stack(all_data)
        labels_array = jnp.array(all_labels)
        
        dataset = {
            'data': data_array,
            'labels': labels_array,
            'metadata': all_metadata,
            'signal_type': 'continuous_gw',
            'sampling_rate': self.sampling_rate,
            'duration': signal_duration
        }
        
        logger.info(f"Generated continuous GW dataset:")
        logger.info(f"  Data shape: {data_array.shape}")
        logger.info(f"  Labels: {jnp.sum(labels_array)} signals, {len(labels_array) - jnp.sum(labels_array)} noise")
        logger.info(f"  Signal duration: {signal_duration} s")
        
        return dataset
    
    def compute_signal_statistics(self, dataset: Dict) -> Dict:
        """
        Compute statistics for continuous GW signals.
        
        Args:
            dataset: Dataset from generate_training_dataset
            
        Returns:
            Dictionary with signal statistics
        """
        data = dataset['data']
        labels = dataset['labels']
        metadata = dataset['metadata']
        
        signal_indices = jnp.where(labels == 1)[0]
        noise_indices = jnp.where(labels == 0)[0]
        
        # Signal power statistics
        signal_data = data[signal_indices]
        noise_data = data[noise_indices]
        
        signal_power = jnp.mean(signal_data**2, axis=1)
        noise_power = jnp.mean(noise_data**2, axis=1)
        
        # Frequency statistics
        frequencies = [m['frequency'] for m in metadata if m['frequency'] is not None]
        amplitudes = [m['amplitude'] for m in metadata if m['amplitude'] is not None]
        
        stats = {
            'num_signals': len(signal_indices),
            'num_noise': len(noise_indices),
            'signal_power_mean': float(jnp.mean(signal_power)),
            'signal_power_std': float(jnp.std(signal_power)),
            'noise_power_mean': float(jnp.mean(noise_power)),
            'noise_power_std': float(jnp.std(noise_power)),
            'snr_estimate': float(jnp.mean(signal_power) / jnp.mean(noise_power)),
            'frequency_range': (min(frequencies), max(frequencies)) if frequencies else None,
            'amplitude_range': (min(amplitudes), max(amplitudes)) if amplitudes else None,
            'sampling_rate': dataset['sampling_rate'],
            'duration': dataset['duration']
        }
        
        logger.info("Continuous GW Signal Statistics:")
        logger.info(f"  Signals: {stats['num_signals']}, Noise: {stats['num_noise']}")
        logger.info(f"  SNR estimate: {stats['snr_estimate']:.3f}")
        logger.info(f"  Frequency range: {stats['frequency_range']} Hz")
        logger.info(f"  Amplitude range: {stats['amplitude_range']}")
        
        return stats


def create_mixed_gw_dataset(continuous_generator: ContinuousGWGenerator,
                          binary_data: Optional[Dict] = None,
                          mix_ratio: float = 0.5) -> Dict:
    """
    Create mixed dataset with continuous + binary GW signals.
    
    Args:
        continuous_generator: Continuous GW generator instance
        binary_data: Existing binary GW dataset (from GWOSC)
        mix_ratio: Ratio of continuous to binary signals
        
    Returns:
        Mixed dataset for enhanced CPC+SNN training
    """
    logger.info(f"Creating mixed GW dataset (mix_ratio={mix_ratio})")
    
    # Generate continuous signals
    num_continuous = int(100 * mix_ratio)
    num_binary = int(100 * (1 - mix_ratio))
    
    continuous_dataset = continuous_generator.generate_training_dataset(
        num_signals=num_continuous,
        signal_duration=4.0
    )
    
    if binary_data is not None:
        # Combine with existing binary data
        logger.info(f"Combining {num_continuous} continuous + {num_binary} binary signals")
        
        # For now, return continuous dataset
        # TODO: Implement proper mixing logic
        mixed_dataset = continuous_dataset
        mixed_dataset['signal_types'] = ['continuous_gw', 'binary_merger']
        
    else:
        logger.info("No binary data provided, returning continuous-only dataset")
        mixed_dataset = continuous_dataset
        mixed_dataset['signal_types'] = ['continuous_gw']
    
    return mixed_dataset


# Test function
def test_continuous_gw_generator():
    """Test continuous GW signal generation."""
    print("ðŸ”¬ Testing Continuous GW Generator...")
    
    if not PYFSTAT_AVAILABLE:
        print("âŒ PyFstat not available - skipping test")
        return False
    
    try:
        # Create generator
        generator = ContinuousGWGenerator(
            base_frequency=50.0,
            freq_range=(40.0, 60.0),
            duration=4.0
        )
        
        # Generate small test dataset
        dataset = generator.generate_training_dataset(
            num_signals=5,
            signal_duration=4.0
        )
        
        # Compute statistics
        stats = generator.compute_signal_statistics(dataset)
        
        print(f"âœ… Generated test dataset: {dataset['data'].shape}")
        print(f"âœ… SNR estimate: {stats['snr_estimate']:.3f}")
        print(f"âœ… Frequency range: {stats['frequency_range']} Hz")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        return False


if __name__ == "__main__":
    # Run test
    success = test_continuous_gw_generator()
    exit(0 if success else 1)
```

## data/data_loader.py

```python

```

## data/gw_download.py

```python
"""
GWOSC Data Downloader and Preprocessor

Production-ready pipeline for downloading and preprocessing gravitational wave 
strain data from LIGO/Virgo detectors via GWOSC API. Optimized for Apple Silicon
with comprehensive quality validation and batch processing.
"""

import jax
import jax.numpy as jnp
import jax.scipy as jsp
from gwpy.timeseries import TimeSeries
from gwpy.frequencyseries import FrequencySeries
from typing import List, Tuple, Optional, Dict, Any
import logging
import time
import tempfile
import hashlib
from pathlib import Path
from abc import ABC, abstractmethod
from dataclasses import dataclass
import numpy as np

logger = logging.getLogger(__name__)


@dataclass
class QualityMetrics:
    """Quality assessment metrics for strain data."""
    is_valid: bool
    snr_estimate: float
    glitch_probability: float
    spectral_line_contamination: float
    data_completeness: float
    outlier_fraction: float


@dataclass
class ProcessingResult:
    """Result of data processing with quality metrics."""
    strain_data: jnp.ndarray
    psd: Optional[jnp.ndarray]
    quality: QualityMetrics
    processing_time: float
    metadata: Dict[str, Any]


class DataSource(ABC):
    """Abstract interface for gravitational wave data sources."""
    
    @abstractmethod
    def fetch(self, detector: str, start_time: int, duration: float) -> jnp.ndarray:
        """Fetch strain data for specified detector and time range."""
        pass


class ProductionGWOSCDownloader(DataSource):
    """
    Production-ready GWOSC downloader with intelligent caching, 
    quality validation, and batch processing capabilities.
    
    Optimized for Data Pipeline Phase with <1s per 4s segment target.
    """
    
    def __init__(self, 
                 cache_dir: Optional[str] = None, 
                 max_retries: int = 5,
                 retry_delay: float = 2.0,
                 max_cache_size_gb: float = 5.0):
        self.cache_dir = Path(cache_dir) if cache_dir else Path(tempfile.gettempdir()) / "ligo_cpc_cache"
        self.cache_dir.mkdir(exist_ok=True)
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.max_cache_size = max_cache_size_gb * 1e9
        
        logger.info(f"Initialized GWOSC downloader with cache: {self.cache_dir}")
        
    def cache_key(self, detector: str, start_time: int, duration: float) -> str:
        """Generate unique cache key for data segment."""
        key_string = f"{detector}_{start_time}_{duration}"
        return hashlib.sha256(key_string.encode()).hexdigest()[:16]
        
    def cleanup_cache(self):
        """Remove oldest cached files if cache size exceeds limit."""
        cache_files = list(self.cache_dir.glob("*.npy"))
        total_size = sum(f.stat().st_size for f in cache_files)
        
        if total_size > self.max_cache_size:
            # Sort by modification time and remove oldest 50%
            cache_files.sort(key=lambda f: f.stat().st_mtime)
            for f in cache_files[:len(cache_files)//2]:
                f.unlink()
                logger.debug(f"Removed cached file: {f.name}")
                
    def fetch(self, detector: str, start_time: int, duration: float) -> jnp.ndarray:
        """
        Fetch strain data from GWOSC with intelligent caching.
        
        Args:
            detector: Detector name (e.g., 'H1', 'L1', 'V1')
            start_time: GPS start time
            duration: Duration in seconds
            
        Returns:
            JAX array of strain data
        """
        # Check cache first
        cache_key = self.cache_key(detector, start_time, duration)
        cache_file = self.cache_dir / f"{cache_key}.npy"
        
        if cache_file.exists():
            logger.debug(f"Loading from cache: {cache_key}")
            # CPU fallback dla cached data loading
            with jax.default_device(jax.devices('cpu')[0]):
                return jnp.array(np.load(cache_file))
            
        # Download with exponential backoff retry
        end_time = start_time + duration
        
        for attempt in range(self.max_retries):
            try:
                start_fetch = time.perf_counter()
                logger.info(f"Fetching {detector} data: {start_time}-{end_time} (attempt {attempt+1})")
                
                # Use GWpy to fetch open data
                timeseries = TimeSeries.fetch_open_data(
                    detector, start_time, end_time,
                    verbose=False  # Reduce output noise
                )
                
                # Convert to JAX array with CPU fallback dla Metal backend compatibility
                with jax.default_device(jax.devices('cpu')[0]):
                    strain_data = jnp.array(timeseries.value)
                fetch_time = time.perf_counter() - start_fetch
                
                # Cache the result
                np.save(cache_file, np.array(strain_data))
                self.cleanup_cache()
                
                logger.info(f"Fetched {len(strain_data)} samples in {fetch_time:.2f}s")
                return strain_data
                
            except Exception as e:
                wait_time = self.retry_delay * (2 ** attempt)  # Exponential backoff
                logger.warning(f"Attempt {attempt+1} failed: {e}")
                
                if attempt < self.max_retries - 1:
                    logger.info(f"Retrying in {wait_time:.1f}s...")
                    time.sleep(wait_time)
                else:
                    raise RuntimeError(f"Failed to fetch data after {self.max_retries} attempts: {e}")
                    
    def fetch_batch(self, 
                   segments: List[Tuple[str, int, float]], 
                   max_workers: int = 4) -> List[jnp.ndarray]:
        """
        Fetch multiple data segments in batch with parallel processing.
        
        Args:
            segments: List of (detector, start_time, duration) tuples
            max_workers: Maximum parallel download workers
            
        Returns:
            List of strain data arrays
        """
        import concurrent.futures
        
        logger.info(f"Batch fetching {len(segments)} segments with {max_workers} workers")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(self.fetch, detector, start_time, duration)
                for detector, start_time, duration in segments
            ]
            
            results = []
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                try:
                    data = future.result()
                    results.append(data)
                    logger.info(f"Completed segment {i+1}/{len(segments)}")
                except Exception as e:
                    logger.error(f"Failed to fetch segment {i+1}: {e}")
                    results.append(None)
                    
        return results


class AdvancedDataPreprocessor:
    """
    Advanced preprocessing pipeline with quality validation optimized for Apple Silicon.
    
    Implements whitening, band-pass filtering, glitch detection, and spectral analysis.
    Target: <1s processing time per 4s segment.
    """
    
    def __init__(self, 
                 sample_rate: int = 4096,
                 bandpass: Tuple[float, float] = (20.0, 1024.0),
                 apply_whitening: bool = True,
                 psd_length: int = 8,  # seconds for PSD estimation
                 quality_threshold: float = 0.7):
        self.sample_rate = sample_rate
        self.bandpass = bandpass
        self.apply_whitening = apply_whitening
        self.psd_length = psd_length
        self.quality_threshold = quality_threshold
        
        # Pre-compute filter coefficients for efficiency
        self._setup_filters()
        
    def _setup_filters(self):
        """Pre-compute filter coefficients for band-pass filtering."""
        from scipy.signal import butter
        
        nyquist = self.sample_rate / 2
        low = self.bandpass[0] / nyquist
        high = self.bandpass[1] / nyquist
        
        # Design 8th-order Butterworth filter
        self.filter_sos = butter(8, [low, high], btype='band', output='sos')
        logger.info(f"Initialized band-pass filter: {self.bandpass[0]}-{self.bandpass[1]} Hz")
        
    def _bandpass_filter_jax(self, data: jnp.ndarray) -> jnp.ndarray:
        """JAX-optimized band-pass filtering using scipy.signal compatibility."""
        # Simple FIR filter implementation for JAX compatibility
        # TODO: Replace with more sophisticated JAX-native filter
        freqs = jnp.fft.fftfreq(len(data), 1/self.sample_rate)
        fft_data = jnp.fft.fft(data)
        
        # Create frequency mask
        mask = (jnp.abs(freqs) >= self.bandpass[0]) & (jnp.abs(freqs) <= self.bandpass[1])
        filtered_fft = fft_data * mask
        
        return jnp.real(jnp.fft.ifft(filtered_fft))
        
    def estimate_psd(self, strain_data: jnp.ndarray) -> jnp.ndarray:
        """
        Estimate Power Spectral Density using Welch's method.
        
        Args:
            strain_data: Input strain timeseries
            
        Returns:
            Power spectral density
        """
        # Use scipy for PSD estimation (more reliable than JAX implementation)
        from scipy.signal import welch
        
        nperseg = self.psd_length * self.sample_rate
        freqs, psd = welch(
            np.array(strain_data), 
            fs=self.sample_rate,
            nperseg=min(nperseg, len(strain_data)//4)
        )
        
        # CPU fallback dla PSD array conversion
        with jax.default_device(jax.devices('cpu')[0]):
            return jnp.array(psd)
        
    def _whiten_data(self, strain_data: jnp.ndarray, psd: jnp.ndarray) -> jnp.ndarray:
        """
        Whiten strain data using estimated PSD.
        
        Args:
            strain_data: Input strain timeseries
            psd: Power spectral density
            
        Returns:
            Whitened strain data
        """
        # FFT of strain data
        strain_fft = jnp.fft.fft(strain_data)
        freqs = jnp.fft.fftfreq(len(strain_data), 1/self.sample_rate)
        
        # Interpolate PSD to match FFT frequencies
        psd_interp = jnp.interp(jnp.abs(freqs), 
                               jnp.linspace(0, self.sample_rate/2, len(psd)), 
                               psd)
        
        # Avoid division by zero
        psd_interp = jnp.where(psd_interp < 1e-40, 1e-40, psd_interp)
        
        # Whiten in frequency domain
        whitened_fft = strain_fft / jnp.sqrt(psd_interp)
        
        # Return to time domain
        return jnp.real(jnp.fft.ifft(whitened_fft))
        
    def assess_quality(self, strain_data: jnp.ndarray, psd: Optional[jnp.ndarray] = None) -> QualityMetrics:
        """
        Comprehensive quality assessment of strain data.
        
        Args:
            strain_data: Input strain timeseries
            psd: Optional pre-computed PSD
            
        Returns:
            Quality metrics object
        """
        # Basic validity checks
        has_nan = jnp.any(jnp.isnan(strain_data))
        has_inf = jnp.any(jnp.isinf(strain_data))
        is_valid = not (has_nan or has_inf)
        
        if not is_valid:
            return QualityMetrics(
                is_valid=False, snr_estimate=0.0, glitch_probability=1.0,
                spectral_line_contamination=1.0, data_completeness=0.0, outlier_fraction=1.0
            )
            
        # Data completeness (non-zero values)
        data_completeness = float(jnp.mean(strain_data != 0.0))
        
        # Outlier detection (values > 6 sigma)
        std_dev = jnp.std(strain_data)
        outliers = jnp.abs(strain_data) > 6 * std_dev
        outlier_fraction = float(jnp.mean(outliers))
        
        # SNR estimate (rough approximation)
        signal_power = jnp.var(strain_data)
        snr_estimate = float(jnp.sqrt(signal_power) * len(strain_data)**0.5)
        
        # Glitch probability (based on kurtosis and outliers)
        kurtosis = float(jsp.stats.kurtosis(strain_data))
        glitch_probability = float(jnp.clip((kurtosis - 3) / 10 + outlier_fraction, 0, 1))
        
        # Spectral line contamination (detect strong narrowband features)
        if psd is not None:
            # Look for peaks significantly above median
            psd_median = jnp.median(psd)
            peak_ratio = jnp.max(psd) / psd_median
            spectral_line_contamination = float(jnp.clip((peak_ratio - 100) / 1000, 0, 1))
        else:
            spectral_line_contamination = 0.0
            
        return QualityMetrics(
            is_valid=is_valid,
            snr_estimate=snr_estimate,
            glitch_probability=glitch_probability,
            spectral_line_contamination=spectral_line_contamination,
            data_completeness=data_completeness,
            outlier_fraction=outlier_fraction
        )
        
    def process(self, strain_data: jnp.ndarray) -> ProcessingResult:
        """
        Complete preprocessing pipeline with quality assessment.
        
        Args:
            strain_data: Raw strain timeseries
            
        Returns:
            Processing result with quality metrics
        """
        start_time = time.perf_counter()
        
        logger.debug(f"Processing {len(strain_data)} samples ({len(strain_data)/self.sample_rate:.1f}s)")
        
        # 1. Band-pass filtering
        filtered_data = self._bandpass_filter_jax(strain_data)
        
        # 2. PSD estimation for whitening
        psd = None
        if self.apply_whitening:
            psd = self.estimate_psd(filtered_data)
            processed_data = self._whiten_data(filtered_data, psd)
        else:
            processed_data = filtered_data
            
        # 3. Normalization (zero mean, unit variance) with CPU fallback
        with jax.default_device(jax.devices('cpu')[0]):
            processed_data = (processed_data - jnp.mean(processed_data)) / jnp.std(processed_data)
        
        # 4. Quality assessment
        quality = self.assess_quality(processed_data, psd)
        
        processing_time = time.perf_counter() - start_time
        
        # 5. Metadata collection
        metadata = {
            'original_length': len(strain_data),
            'processed_length': len(processed_data),
            'sample_rate': self.sample_rate,
            'bandpass': self.bandpass,
            'whitening_applied': self.apply_whitening,
            'processing_time_ms': processing_time * 1000
        }
        
        logger.debug(f"Processing completed in {processing_time*1000:.1f}ms "
                    f"(quality: {quality.snr_estimate:.1f} SNR)")
        
        return ProcessingResult(
            strain_data=processed_data,
            psd=psd,
            quality=quality,
            processing_time=processing_time,
            metadata=metadata
        )
        
    def process_batch(self, strain_segments: List[jnp.ndarray]) -> List[ProcessingResult]:
        """
        Process multiple strain segments in batch for efficiency.
        
        Args:
            strain_segments: List of strain data arrays
            
        Returns:
            List of processing results
        """
        logger.info(f"Batch processing {len(strain_segments)} segments")
        
        results = []
        total_start = time.perf_counter()
        
        for i, segment in enumerate(strain_segments):
            if segment is not None:
                result = self.process(segment)
                results.append(result)
                
                # Log progress
                if (i + 1) % 10 == 0:
                    elapsed = time.perf_counter() - total_start
                    avg_time = elapsed / (i + 1) * 1000
                    logger.info(f"Processed {i+1}/{len(strain_segments)} segments "
                               f"(avg: {avg_time:.1f}ms per segment)")
            else:
                results.append(None)
                
        total_time = time.perf_counter() - total_start
        valid_results = [r for r in results if r is not None]
        
        logger.info(f"Batch processing completed in {total_time:.2f}s "
                   f"({len(valid_results)}/{len(strain_segments)} successful)")
        
        return results


# Legacy alias for backward compatibility
GWOSCDownloader = ProductionGWOSCDownloader
DataPreprocessor = AdvancedDataPreprocessor
```

## models/__init__.py

```python
"""
Models Module: Neural Network Architectures

Implements the 3-component neuromorphic pipeline:
1. CPC Encoder - Self-supervised representation learning
2. Spike Bridge - Continuous to spike conversion  
3. SNN Classifier - Neuromorphic binary classification
"""

from .cpc_encoder import CPCEncoder, info_nce_loss
from .snn_classifier import SNNClassifier, create_snn_classifier, SNNTrainer

__all__ = [
    "CPCEncoder",
    "info_nce_loss",
    "SNNClassifier",
    "create_snn_classifier", 
    "SNNTrainer",
]
```

## models/cpc_encoder.py

```python
"""
Contrastive Predictive Coding (CPC) Encoder

Self-supervised learning architecture for gravitational wave strain data.
Enhanced with findings from CPC+SNN Integration Paper (2025).
"""

import jax
import jax.numpy as jnp
import flax.linen as nn
import optax
from typing import Tuple, Optional


class CPCEncoder(nn.Module):
    """Enhanced CPC Encoder for GW strain data."""
    latent_dim: int = 256
    conv_channels: Tuple[int, ...] = (32, 64, 128)
    use_batch_norm: bool = False  # Make BatchNorm optional
    dropout_rate: float = 0.0     # Make dropout optional
    
    @nn.compact
    def __call__(self, x: jnp.ndarray, train: bool = False) -> jnp.ndarray:
        # Scale input to reasonable range (GW strain data is very small ~1e-23)
        x = x * 1e20  # Scale from ~1e-23 to ~1e-3 range
        
        # Add channel dimension
        x = x[..., None]  # [batch, time, 1]
        
        # Enhanced convolutions with optional regularization
        for i, channels in enumerate(self.conv_channels):
            x = nn.Conv(channels, kernel_size=(9,), strides=(2,))(x)
            x = nn.gelu(x)
            
            # Optional batch normalization
            if self.use_batch_norm:
                x = nn.BatchNorm(use_running_average=not train)(x)
            
            # Optional dropout
            if self.dropout_rate > 0.0:
                x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
        
        # After convolutions: [batch, downsampled_time, final_channels]
        batch_size, seq_len, features = x.shape
        
        # **ENHANCED: Functional GRU with better parameter management**
        # Paper suggests better GRU initialization dla stability
        gru_ih_kernel = self.param('gru_ih_kernel', 
                                  nn.initializers.xavier_uniform(), 
                                  (features, 3 * features))
        gru_hh_kernel = self.param('gru_hh_kernel', 
                                  nn.initializers.orthogonal(scale=1.0), 
                                  (features, 3 * features))
        gru_ih_bias = self.param('gru_ih_bias', 
                                nn.initializers.zeros, 
                                (3 * features,))
        gru_hh_bias = self.param('gru_hh_bias', 
                                nn.initializers.zeros, 
                                (3 * features,))
        
        # Initial hidden state with better initialization
        h_init = jnp.zeros((batch_size, features))
        
        # Transpose dla scan: [time, batch, features]
        x_transposed = jnp.transpose(x, (1, 0, 2))
        
        def gru_step(h, x):
            """Enhanced GRU step with better numerical stability."""
            # Input projections
            gi = jnp.dot(x, gru_ih_kernel) + gru_ih_bias
            gh = jnp.dot(h, gru_hh_kernel) + gru_hh_bias
            
            # Split into reset, update, new gates
            i_r, i_z, i_n = jnp.split(gi, 3, axis=-1)
            h_r, h_z, h_n = jnp.split(gh, 3, axis=-1)
            
            # Apply activations with better numerical stability
            reset_gate = jax.nn.sigmoid(i_r + h_r)
            update_gate = jax.nn.sigmoid(i_z + h_z)
            new_gate = jnp.tanh(i_n + reset_gate * h_n)
            
            # Update hidden state
            h_new = (1 - update_gate) * new_gate + update_gate * h
            
            return h_new, h_new
        
        # Apply GRU across time steps
        _, hidden_states = jax.lax.scan(gru_step, h_init, x_transposed)
        
        # Transpose back: [batch, time, features]
        hidden_states = jnp.transpose(hidden_states, (1, 0, 2))
        
        # Final projection to latent space with better initialization
        latent_features = nn.Dense(
            self.latent_dim,
            kernel_init=nn.initializers.xavier_uniform(),
            bias_init=nn.initializers.zeros
        )(hidden_states)
        
        # L2 normalization dla better contrastive learning (paper finding)
        # Only normalize if the norm is significant to avoid division by zero
        norms = jnp.linalg.norm(latent_features, axis=-1, keepdims=True)
        latent_features = jnp.where(
            norms > 1e-6,
            latent_features / (norms + 1e-8),
            latent_features  # Keep original if norm too small
        )
        
        return latent_features


def enhanced_info_nce_loss(z_context: jnp.ndarray, 
                          z_target: jnp.ndarray, 
                          temperature: float = 0.1,
                          num_negatives: int = 8,  # Reduced for stability
                          use_hard_negatives: bool = False) -> jnp.ndarray:  # Disabled for now
    """
    Enhanced InfoNCE loss with simplified implementation for stability.
    
    Improvements:
    - Better numerical stability
    - Cosine similarity computation
    - Simplified negative sampling
    """
    batch_size, context_len, feature_dim = z_context.shape
    _, target_len, _ = z_target.shape
    
    # Ensure equal lengths dla proper alignment
    min_len = min(context_len, target_len)
    z_context = z_context[:, :min_len, :]
    z_target = z_target[:, :min_len, :]
    
    # Normalize dla cosine similarity (already done in encoder but ensure here)
    z_context_norm = z_context / (jnp.linalg.norm(z_context, axis=-1, keepdims=True) + 1e-8)
    z_target_norm = z_target / (jnp.linalg.norm(z_target, axis=-1, keepdims=True) + 1e-8)
    
    # **SIMPLIFIED: Use within-batch negatives only for stability**
    total_loss = 0.0
    
    for t in range(min_len):
        # Get features dla timestep t
        context_t = z_context_norm[:, t, :]  # [batch, features]
        target_t = z_target_norm[:, t, :]    # [batch, features]
        
        # Compute similarity matrix: [batch, batch]
        similarity_matrix = jnp.dot(context_t, target_t.T)  # [batch, batch]
        
        # Apply temperature scaling
        logits = similarity_matrix / temperature  # [batch, batch]
        
        # Labels: diagonal elements are positive pairs
        labels = jnp.arange(batch_size)
        
        # Compute cross-entropy loss dla this timestep
        step_loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)
        total_loss += jnp.mean(step_loss)
    
    return total_loss / min_len


# Backward compatibility
def info_nce_loss(z_context: jnp.ndarray, z_target: jnp.ndarray, temperature: float = 0.1) -> jnp.ndarray:
    """Original InfoNCE loss dla backward compatibility."""
    return enhanced_info_nce_loss(z_context, z_target, temperature, use_hard_negatives=False)


def create_enhanced_cpc_encoder(latent_dim: int = 256,
                              conv_channels: Tuple[int, ...] = (32, 64, 128),
                              use_batch_norm: bool = True,
                              dropout_rate: float = 0.1) -> CPCEncoder:
    """Create enhanced CPC encoder with paper-based improvements."""
    return CPCEncoder(
        latent_dim=latent_dim, 
        conv_channels=conv_channels,
        use_batch_norm=use_batch_norm,
        dropout_rate=dropout_rate
    )


def create_standard_cpc_encoder(latent_dim: int = 256,
                              conv_channels: Tuple[int, ...] = (32, 64, 128)) -> CPCEncoder:
    """Create standard CPC encoder without enhanced features."""
    return CPCEncoder(
        latent_dim=latent_dim, 
        conv_channels=conv_channels,
        use_batch_norm=False,
        dropout_rate=0.0
    )
```

## models/simple_snn.py

```python
"""
Simple JAX-based SNN Implementation

Lightweight LIF (Leaky Integrate-and-Fire) SNN classifier
for quick testing of CPC + Spike Bridge + SNN integration.
"""

import jax
import jax.numpy as jnp
import flax.linen as nn
import optax
from typing import Tuple


class LIFLayer(nn.Module):
    """Simple LIF (Leaky Integrate-and-Fire) layer in JAX/Flax."""
    
    hidden_size: int
    tau_mem: float = 20e-3  # Membrane time constant
    tau_syn: float = 5e-3   # Synaptic time constant  
    threshold: float = 1.0  # Spike threshold
    dt: float = 1e-3        # Time step
    
    @nn.compact
    def __call__(self, spikes: jnp.ndarray) -> jnp.ndarray:
        """
        LIF layer forward pass.
        
        Args:
            spikes: Input spikes [batch, time, input_dim]
            
        Returns:
            output_spikes: Output spikes [batch, time, hidden_size]
        """
        batch_size, time_steps, input_dim = spikes.shape
        
        # Linear transformation (synaptic weights)
        W = self.param('kernel', nn.initializers.glorot_uniform(), (input_dim, self.hidden_size))
        b = self.param('bias', nn.initializers.zeros, (self.hidden_size,))
        
        # Decay factors
        alpha_mem = jnp.exp(-self.dt / self.tau_mem)  # Membrane decay
        alpha_syn = jnp.exp(-self.dt / self.tau_syn)  # Synaptic decay
        
        # Initial states
        v_mem = jnp.zeros((batch_size, self.hidden_size))  # Membrane potential
        i_syn = jnp.zeros((batch_size, self.hidden_size))  # Synaptic current
        
        def lif_step(states, x_t):
            """Single LIF time step."""
            v_mem, i_syn = states
            
            # Synaptic input
            i_input = jnp.dot(x_t, W) + b
            
            # Update synaptic current (exponential decay + input)
            i_syn = alpha_syn * i_syn + i_input
            
            # Update membrane potential (leak + synaptic current)
            v_mem = alpha_mem * v_mem + i_syn
            
            # Spike generation (threshold crossing)
            spikes_out = (v_mem >= self.threshold).astype(jnp.float32)
            
            # Reset membrane potential after spike
            v_mem = v_mem * (1 - spikes_out)
            
            return (v_mem, i_syn), spikes_out
        
        # Scan over time dimension
        _, output_spikes = jax.lax.scan(
            lif_step, 
            init=(v_mem, i_syn),
            xs=jnp.transpose(spikes, (1, 0, 2))  # [time, batch, input_dim]
        )
        
        # Transpose back: [batch, time, hidden_size]
        output_spikes = jnp.transpose(output_spikes, (1, 0, 2))
        
        return output_spikes


class SimpleSNN(nn.Module):
    """Simple 2-layer SNN classifier."""
    
    hidden_size: int = 128
    num_classes: int = 2
    tau_mem: float = 20e-3
    tau_syn: float = 5e-3
    threshold: float = 1.0
    
    @nn.compact  
    def __call__(self, spikes: jnp.ndarray) -> jnp.ndarray:
        """
        SNN forward pass.
        
        Args:
            spikes: Input spike trains [batch, time, input_dim]
            
        Returns:
            logits: Classification logits [batch, num_classes]
        """
        # First LIF layer
        h1 = LIFLayer(
            hidden_size=self.hidden_size,
            tau_mem=self.tau_mem,
            tau_syn=self.tau_syn,
            threshold=self.threshold
        )(spikes)
        
        # Second LIF layer
        h2 = LIFLayer(
            hidden_size=self.hidden_size // 2,
            tau_mem=self.tau_mem,
            tau_syn=self.tau_syn,
            threshold=self.threshold
        )(h1)
        
        # Global average pooling over time
        h_pooled = jnp.mean(h2, axis=1)  # [batch, hidden_size//2]
        
        # Linear readout
        logits = nn.Dense(self.num_classes)(h_pooled)
        
        return logits


class SimpleSNNTrainer:
    """Training utilities dla Simple SNN."""
    
    def __init__(self, learning_rate: float = 1e-3):
        self.optimizer = optax.adam(learning_rate)
        
    def classification_loss(self, 
                          params: dict,
                          spikes: jnp.ndarray, 
                          labels: jnp.ndarray,
                          model: SimpleSNN) -> jnp.ndarray:
        """Classification loss function."""
        logits = model.apply(params, spikes)
        
        return optax.softmax_cross_entropy_with_integer_labels(
            logits, labels
        ).mean()
        
    def training_step(self,
                     params: dict,
                     opt_state: optax.OptState,
                     spikes: jnp.ndarray,
                     labels: jnp.ndarray,
                     model: SimpleSNN) -> Tuple[dict, optax.OptState, float]:
        """Single training step."""
        
        loss, grads = jax.value_and_grad(self.classification_loss)(
            params, spikes, labels, model
        )
        
        updates, opt_state = self.optimizer.update(grads, opt_state)
        params = optax.apply_updates(params, updates)
        
        return params, opt_state, loss
        
    def accuracy(self, 
                params: dict,
                spikes: jnp.ndarray, 
                labels: jnp.ndarray,
                model: SimpleSNN) -> float:
        """Compute classification accuracy."""
        logits = model.apply(params, spikes)
        predictions = jnp.argmax(logits, axis=-1)
        
        return jnp.mean(predictions == labels)


# Convenience functions
def create_simple_snn(hidden_size: int = 64, num_classes: int = 2) -> SimpleSNN:
    """Create simple SNN model."""
    return SimpleSNN(
        hidden_size=hidden_size,
        num_classes=num_classes,
        tau_mem=20e-3,
        tau_syn=5e-3,
        threshold=1.0
    )


def test_simple_lif_layer():
    """Quick test of LIF layer functionality."""
    # Create test data
    batch_size, time_steps, input_dim = 2, 10, 8
    test_spikes = jax.random.bernoulli(
        jax.random.PRNGKey(0), 0.1, (batch_size, time_steps, input_dim)
    ).astype(jnp.float32)
    
    # Create and test LIF layer
    lif = LIFLayer(hidden_size=16)
    key = jax.random.PRNGKey(42)
    params = lif.init(key, test_spikes)
    
    output_spikes = lif.apply(params, test_spikes)
    
    print(f"Input shape: {test_spikes.shape}")
    print(f"Output shape: {output_spikes.shape}")
    print(f"Input spike rate: {jnp.mean(test_spikes):.3f}")
    print(f"Output spike rate: {jnp.mean(output_spikes):.3f}")
    
    return output_spikes.shape == (batch_size, time_steps, 16)


def test_simple_snn():
    """Quick test of complete SNN."""
    # Create test data
    batch_size, time_steps, input_dim = 2, 20, 32
    test_spikes = jax.random.bernoulli(
        jax.random.PRNGKey(0), 0.05, (batch_size, time_steps, input_dim)
    ).astype(jnp.float32)
    test_labels = jnp.array([0, 1])
    
    # Create and test SNN
    snn = create_simple_snn(hidden_size=32, num_classes=2)
    trainer = SimpleSNNTrainer(learning_rate=1e-3)
    
    key = jax.random.PRNGKey(42)
    params = snn.init(key, test_spikes)
    opt_state = trainer.optimizer.init(params)
    
    # Forward pass
    logits = snn.apply(params, test_spikes)
    
    # Training step
    params, opt_state, loss = trainer.training_step(
        params, opt_state, test_spikes, test_labels, snn
    )
    
    # Accuracy
    accuracy = trainer.accuracy(params, test_spikes, test_labels, snn)
    
    print(f"Logits shape: {logits.shape}")
    print(f"Training loss: {loss:.4f}")
    print(f"Accuracy: {accuracy:.3f}")
    
    return True


if __name__ == "__main__":
    print("Testing Simple LIF Layer...")
    success1 = test_simple_lif_layer()
    print(f"LIF Layer test: {'PASSED' if success1 else 'FAILED'}\n")
    
    print("Testing Simple SNN...")
    success2 = test_simple_snn()
    print(f"SNN test: {'PASSED' if success2 else 'FAILED'}")
    
    overall_success = success1 and success2
    print(f"\nOverall: {'SUCCESS' if overall_success else 'FAILED'}")
```

## models/snn_classifier.py

```python
"""
Spiking Neural Network (SNN) Classifier

Neuromorphic binary classifier using Spyx LIF neurons
for energy-efficient gravitational wave detection.
"""

import jax
import jax.numpy as jnp
import haiku as hk
import spyx.nn as snn
import optax
from typing import Optional


class SNNClassifier(hk.Module):
    """
    Spiking Neural Network for binary GW detection.
    
    Architecture: LIF layers + global pooling + linear readout
    Uses Spyx (Google Haiku-based) for stable SNN implementation.
    """
    
    def __init__(self, 
                 hidden_size: int = 128,
                 num_classes: int = 2,
                 tau_mem: float = 20e-3,
                 tau_syn: float = 5e-3,
                 threshold: float = 1.0,
                 name: Optional[str] = None):
        super().__init__(name=name)
        self.hidden_size = hidden_size
        self.num_classes = num_classes
        self.tau_mem = tau_mem
        self.tau_syn = tau_syn  
        self.threshold = threshold
    
    def __call__(self, spikes: jnp.ndarray) -> jnp.ndarray:
        """
        Forward pass through SNN classifier.
        
        Args:
            spikes: Input spike trains [batch, time, input_dim]
            
        Returns:
            Classification logits [batch, num_classes]
        """
        # First LIF layer
        h1 = snn.LIF(
            self.hidden_size,
            activation=snn.superspike
        )(spikes)
        
        # Second LIF layer with lateral inhibition 
        h2 = snn.LIF(
            self.hidden_size,
            activation=snn.superspike
        )(h1)
        
        # Global average pooling over time dimension
        h_pooled = jnp.mean(h2, axis=1)  # [batch, hidden_size]
        
        # Linear readout (non-spiking)
        logits = hk.Linear(self.num_classes)(h_pooled)
        
        return logits


def create_snn_classifier(hidden_size: int = 128, 
                         num_classes: int = 2) -> hk.Transformed:
    """
    Create Haiku-transformed SNN classifier.
    
    Returns:
        Haiku transformed function for SNN forward pass
    """
    def snn_forward(spikes: jnp.ndarray) -> jnp.ndarray:
        classifier = SNNClassifier(
            hidden_size=hidden_size,
            num_classes=num_classes
        )
        return classifier(spikes)
    
    return hk.transform(snn_forward)


class SNNTrainer:
    """Training utilities for SNN classifier."""
    
    def __init__(self, 
                 snn_fn: hk.Transformed,
                 learning_rate: float = 1e-3):
        self.snn_fn = snn_fn
        self.optimizer = optax.adam(learning_rate)
        
    def classification_loss(self, 
                          params: dict,
                          spikes: jnp.ndarray, 
                          labels: jnp.ndarray) -> jnp.ndarray:
        """Binary classification loss for SNN."""
        logits = self.snn_fn.apply(params, None, spikes)
        
        return optax.softmax_cross_entropy_with_integer_labels(
            logits, labels
        ).mean()
        
    def training_step(self,
                     params: dict,
                     opt_state: optax.OptState,
                     spikes: jnp.ndarray,
                     labels: jnp.ndarray) -> tuple:
        """Single training step for SNN classifier."""
        
        loss, grads = jax.value_and_grad(self.classification_loss)(
            params, spikes, labels
        )
        
        updates, opt_state = self.optimizer.update(grads, opt_state)
        params = optax.apply_updates(params, updates)
        
        return params, opt_state, loss
        
    def accuracy(self, 
                params: dict,
                spikes: jnp.ndarray, 
                labels: jnp.ndarray) -> float:
        """Compute classification accuracy."""
        logits = self.snn_fn.apply(params, None, spikes)
        predictions = jnp.argmax(logits, axis=-1)
        
        return jnp.mean(predictions == labels)
```

## models/spike_bridge.py

```python
"""
Spike Bridge: CPC Latent to Spike Train Conversion

Converts continuous CPC encoder representations to discrete spike trains
for neuromorphic SNN processing. Supports multiple encoding strategies.
"""

import jax
import jax.numpy as jnp
from typing import Tuple, Optional
from enum import Enum


class SpikeEncodingStrategy(Enum):
    """Spike encoding strategies."""
    POISSON_RATE = "poisson_rate"
    TEMPORAL_CONTRAST = "temporal_contrast"
    POPULATION_VECTOR = "population_vector"
    RATE_BASED = "rate_based"


class SpikeBridge:
    """
    Converts CPC latent representations to spike trains.
    
    Support multiple neuromorphic encoding strategies with
    configurable parameters for optimal SNN performance.
    """
    
    def __init__(self,
                 encoding_strategy: SpikeEncodingStrategy = SpikeEncodingStrategy.POISSON_RATE,
                 spike_time_steps: int = 100,
                 max_spike_rate: float = 100.0,  # Hz
                 dt: float = 1e-3,  # 1ms time steps
                 population_size: int = 8):
        """
        Initialize spike bridge.
        
        Args:
            encoding_strategy: Method for converting continuous values to spikes
            spike_time_steps: Number of time steps for spike train generation
            max_spike_rate: Maximum firing rate (Hz) for rate-based encoding
            dt: Time step size (seconds) 
            population_size: Number of neurons per population (for population coding)
        """
        self.encoding_strategy = encoding_strategy
        self.spike_time_steps = spike_time_steps
        self.max_spike_rate = max_spike_rate
        self.dt = dt
        self.population_size = population_size
        
    def encode(self, 
               latent_features: jnp.ndarray, 
               key: jax.random.PRNGKey) -> jnp.ndarray:
        """
        Convert CPC latent features to spike trains.
        
        Args:
            latent_features: CPC encoder output [batch, seq_len, latent_dim]
            key: JAX random key for stochastic spike generation
            
        Returns:
            spike_trains: Binary spike trains [batch, spike_time_steps, spike_dim]
        """
        if self.encoding_strategy == SpikeEncodingStrategy.POISSON_RATE:
            return self._poisson_rate_encoding(latent_features, key)
        elif self.encoding_strategy == SpikeEncodingStrategy.TEMPORAL_CONTRAST:
            return self._temporal_contrast_encoding(latent_features, key)
        elif self.encoding_strategy == SpikeEncodingStrategy.POPULATION_VECTOR:
            return self._population_vector_encoding(latent_features, key)
        elif self.encoding_strategy == SpikeEncodingStrategy.RATE_BASED:
            return self._rate_based_encoding(latent_features, key)
        else:
            raise ValueError(f"Unknown encoding strategy: {self.encoding_strategy}")
    
    def _poisson_rate_encoding(self, 
                              latent_features: jnp.ndarray, 
                              key: jax.random.PRNGKey) -> jnp.ndarray:
        """
        Poisson rate coding: Continuous values â†’ firing rates â†’ Poisson spikes.
        
        Higher latent values = higher firing rates = more spikes.
        Most biologically plausible encoding strategy.
        """
        batch_size, seq_len, latent_dim = latent_features.shape
        
        # Normalize features to [0, 1] range for stable rate conversion
        features_norm = jax.nn.sigmoid(latent_features)
        
        # Convert to firing rates (Hz)
        firing_rates = features_norm * self.max_spike_rate
        
        # Convert rates to spike probabilities per time step
        spike_probs = firing_rates * self.dt  # Probability per dt
        spike_probs = jnp.clip(spike_probs, 0.0, 1.0)  # Ensure valid probabilities
        
        # Temporal expansion: repeat each feature value across spike time steps
        # [batch, seq_len, latent_dim] â†’ [batch, seq_len * spike_time_steps, latent_dim]
        spike_probs_expanded = jnp.repeat(spike_probs, self.spike_time_steps, axis=1)
        
        # Generate Poisson spikes
        random_vals = jax.random.uniform(
            key, shape=spike_probs_expanded.shape
        )
        spikes = (random_vals < spike_probs_expanded).astype(jnp.float32)
        
        # Reshape to proper spike train format
        # [batch, seq_len * spike_time_steps, latent_dim]
        total_time_steps = seq_len * self.spike_time_steps
        spikes = spikes.reshape(batch_size, total_time_steps, latent_dim)
        
        return spikes
    
    def _temporal_contrast_encoding(self, 
                                  latent_features: jnp.ndarray, 
                                  key: jax.random.PRNGKey) -> jnp.ndarray:
        """
        Temporal contrast coding: Spikes encode changes/gradients.
        
        Positive changes â†’ positive spikes, negative changes â†’ inhibitory spikes.
        Efficient dla representing temporal dynamics in GW signals.
        """
        batch_size, seq_len, latent_dim = latent_features.shape
        
        # Compute temporal differences (gradients)
        features_padded = jnp.pad(latent_features, ((0, 0), (1, 0), (0, 0)), mode='edge')
        temporal_gradients = jnp.diff(features_padded, axis=1)
        
        # Split into positive and negative changes
        positive_changes = jnp.maximum(temporal_gradients, 0)
        negative_changes = jnp.maximum(-temporal_gradients, 0)
        
        # Normalize and convert to spike rates
        pos_rates = jax.nn.sigmoid(positive_changes * 10) * self.max_spike_rate
        neg_rates = jax.nn.sigmoid(negative_changes * 10) * self.max_spike_rate
        
        # Create spike probabilities
        pos_probs = pos_rates * self.dt
        neg_probs = neg_rates * self.dt
        
        # Expand temporally
        pos_probs_expanded = jnp.repeat(pos_probs, self.spike_time_steps, axis=1)
        neg_probs_expanded = jnp.repeat(neg_probs, self.spike_time_steps, axis=1)
        
        # Generate spikes dla both positive and negative channels
        key1, key2 = jax.random.split(key)
        
        pos_spikes = (jax.random.uniform(key1, pos_probs_expanded.shape) < pos_probs_expanded).astype(jnp.float32)
        neg_spikes = (jax.random.uniform(key2, neg_probs_expanded.shape) < neg_probs_expanded).astype(jnp.float32)
        
        # Combine into single spike train with doubled dimensionality
        # Positive channels: [0, latent_dim), Negative channels: [latent_dim, 2*latent_dim)
        spikes = jnp.concatenate([pos_spikes, neg_spikes], axis=-1)
        
        total_time_steps = seq_len * self.spike_time_steps
        spikes = spikes.reshape(batch_size, total_time_steps, 2 * latent_dim)
        
        return spikes
    
    def _population_vector_encoding(self, 
                                  latent_features: jnp.ndarray, 
                                  key: jax.random.PRNGKey) -> jnp.ndarray:
        """
        Population vector coding: Each feature â†’ population of neurons.
        
        Higher feature values activate more neurons in population.
        Provides redundancy and noise robustness.
        """
        batch_size, seq_len, latent_dim = latent_features.shape
        
        # Normalize features
        features_norm = jax.nn.sigmoid(latent_features)
        
        # Create population thresholds [0, 1/N, 2/N, ..., (N-1)/N]
        thresholds = jnp.linspace(0, 1, self.population_size + 1)[:-1]
        thresholds = thresholds.reshape(1, 1, 1, self.population_size)
        
        # Expand features dla population comparison
        features_expanded = features_norm[..., :, None]  # [batch, seq, latent, 1]
        
        # Population activation: neuron fires if feature > threshold
        population_rates = (features_expanded > thresholds).astype(jnp.float32)
        population_rates = population_rates * self.max_spike_rate
        
        # Convert to spike probabilities
        spike_probs = population_rates * self.dt
        spike_probs = jnp.clip(spike_probs, 0.0, 1.0)
        
        # Expand temporally
        spike_probs_expanded = jnp.repeat(spike_probs, self.spike_time_steps, axis=1)
        
        # Generate spikes
        random_vals = jax.random.uniform(key, spike_probs_expanded.shape)
        spikes = (random_vals < spike_probs_expanded).astype(jnp.float32)
        
        # Reshape: [batch, total_time, latent_dim * population_size]
        total_time_steps = seq_len * self.spike_time_steps
        spike_dim = latent_dim * self.population_size
        spikes = spikes.reshape(batch_size, total_time_steps, spike_dim)
        
        return spikes
    
    def _rate_based_encoding(self, 
                           latent_features: jnp.ndarray, 
                           key: jax.random.PRNGKey) -> jnp.ndarray:
        """
        Rate-based encoding: Direct rate representation without stochasticity.
        
        Continuous values â†’ continuous firing rates (no binary spikes).
        Computationally efficient dla SNN training.
        """
        batch_size, seq_len, latent_dim = latent_features.shape
        
        # Normalize to firing rates
        features_norm = jax.nn.sigmoid(latent_features)
        firing_rates = features_norm * self.max_spike_rate
        
        # Expand temporally (repeat rates)
        rates_expanded = jnp.repeat(firing_rates, self.spike_time_steps, axis=1)
        
        # Normalize to [0, 1] range dla SNN compatibility
        rates_normalized = rates_expanded / self.max_spike_rate
        
        total_time_steps = seq_len * self.spike_time_steps  
        rates_final = rates_normalized.reshape(batch_size, total_time_steps, latent_dim)
        
        return rates_final
    
    def compute_spike_statistics(self, spikes: jnp.ndarray) -> dict:
        """
        Compute spike train statistics dla monitoring and analysis.
        
        Args:
            spikes: Spike trains [batch, time, neurons]
            
        Returns:
            Dictionary with spike statistics
        """
        # Overall firing rate
        mean_rate = jnp.mean(spikes) * (1.0 / self.dt)  # Convert to Hz
        
        # Sparsity (fraction of zeros)
        sparsity = jnp.mean(spikes == 0)
        
        # Temporal correlations (autocorrelation at lag 1)
        spikes_t0 = spikes[:, :-1, :]
        spikes_t1 = spikes[:, 1:, :]
        temporal_corr = jnp.corrcoef(
            spikes_t0.flatten(), 
            spikes_t1.flatten()
        )[0, 1]
        
        # Synchrony (correlation across neurons)
        if spikes.shape[-1] > 1:
            spike_rates_per_neuron = jnp.mean(spikes, axis=1)  # [batch, neurons]
            synchrony = jnp.mean(jnp.corrcoef(spike_rates_per_neuron.T))
        else:
            synchrony = 1.0
        
        return {
            'mean_firing_rate_hz': float(mean_rate),
            'sparsity': float(sparsity), 
            'temporal_correlation': float(temporal_corr),
            'neural_synchrony': float(synchrony),
            'total_spikes': int(jnp.sum(spikes)),
            'spike_shape': spikes.shape
        }


# Convenience functions dla easy integration
def create_default_spike_bridge() -> SpikeBridge:
    """Create spike bridge with optimal default parameters dla GW detection."""
    return SpikeBridge(
        encoding_strategy=SpikeEncodingStrategy.POISSON_RATE,
        spike_time_steps=50,  # 50ms simulation time
        max_spike_rate=100.0,  # 100 Hz max rate
        dt=1e-3,  # 1ms resolution
        population_size=8
    )


def create_fast_spike_bridge() -> SpikeBridge:
    """Create spike bridge optimized dla speed (rate-based encoding)."""
    return SpikeBridge(
        encoding_strategy=SpikeEncodingStrategy.RATE_BASED,
        spike_time_steps=20,  # 20ms simulation time
        max_spike_rate=50.0,  # Lower max rate 
        dt=1e-3,
        population_size=4
    )


def create_robust_spike_bridge() -> SpikeBridge:
    """Create spike bridge optimized dla robustness (population coding)."""
    return SpikeBridge(
        encoding_strategy=SpikeEncodingStrategy.POPULATION_VECTOR,
        spike_time_steps=80,  # Longer simulation dla stability
        max_spike_rate=200.0,  # Higher max rate
        dt=0.5e-3,  # Higher temporal resolution
        population_size=16  # Larger populations
    )
```

## training/__init__.py

```python

```

## training/advanced_training.py

```python
#!/usr/bin/env python3

"""
Advanced Neuromorphic GW Training Pipeline

State-of-the-art techniques for achieving 70-90% accuracy:
- AdamW optimizer with cosine annealing
- Focal loss for class imbalance  
- Advanced data augmentation
- Attention-enhanced CPC
- Deeper SNN architectures
- Diffusion-based signal enhancement
"""

import jax
import jax.numpy as jnp
import optax
import flax.linen as nn
import logging
import time
from typing import Dict, Tuple, Optional, List
from pathlib import Path
from dataclasses import dataclass

from ..models.cpc_encoder import CPCEncoder, enhanced_info_nce_loss
from ..models.simple_snn import create_simple_snn
from ..models.spike_bridge import SpikeBridge, SpikeEncodingStrategy
from ..data.continuous_gw_generator import ContinuousGWGenerator
from ..data.gw_download import ProductionGWOSCDownloader

logger = logging.getLogger(__name__)


@dataclass 
class AdvancedTrainingConfig:
    """Advanced configuration for high-performance neuromorphic GW detection."""
    
    # Enhanced Dataset
    num_continuous_signals: int = 500  # Much larger dataset
    num_binary_signals: int = 500
    num_noise_samples: int = 300  # Reduced noise dominance
    signal_duration: float = 4.0
    
    # Advanced Training
    batch_size: int = 32  # Larger batches for stability
    learning_rate: float = 3e-4  # Lower LR for fine-tuning
    num_epochs: int = 100  # Much more training
    warmup_epochs: int = 10
    
    # Model Architecture
    cpc_latent_dim: int = 256  # Larger representation
    cpc_conv_channels: Tuple[int, ...] = (64, 128, 256, 512)  # Deeper CNN
    snn_hidden_sizes: Tuple[int, ...] = (256, 128, 64)  # Multi-layer SNN
    spike_time_steps: int = 100  # More temporal resolution
    
    # Advanced Techniques
    use_attention: bool = True  # Attention in CPC
    use_focal_loss: bool = True  # For class imbalance
    use_mixup: bool = True  # Data augmentation
    use_cosine_scheduling: bool = True
    weight_decay: float = 0.01  # AdamW regularization
    
    # Spike Encoding
    spike_encoding: SpikeEncodingStrategy = SpikeEncodingStrategy.TEMPORAL_CONTRAST
    multi_encoding: bool = True  # Multiple encoding strategies
    
    # Output
    output_dir: str = "advanced_gw_training_outputs"
    save_checkpoints: bool = True
    checkpoint_every: int = 20


class AttentionCPCEncoder(nn.Module):
    """Enhanced CPC Encoder with Attention Mechanisms."""
    
    latent_dim: int = 256
    conv_channels: Tuple[int, ...] = (64, 128, 256, 512)
    num_attention_heads: int = 8
    use_attention: bool = True
    dropout_rate: float = 0.1
    
    @nn.compact
    def __call__(self, x: jnp.ndarray, train: bool = True) -> jnp.ndarray:
        # Input scaling for GW strain data
        x = x * 1e20
        
        # Add channel dimension
        x = x[..., None]  # [batch, time, 1]
        
        # **ENHANCED: Progressive convolution with residual connections**
        skip_connections = []
        
        for i, channels in enumerate(self.conv_channels):
            if i > 0 and x.shape[-1] == channels:
                # Residual connection when dimensions match
                residual = x
            else:
                residual = None
                
            x = nn.Conv(channels, kernel_size=(9,), strides=(2,), 
                       kernel_init=nn.initializers.he_normal())(x)
            x = nn.BatchNorm(use_running_average=not train)(x)
            x = nn.gelu(x)
            
            if residual is not None:
                # Residual connection with proper padding
                if residual.shape[1] != x.shape[1]:
                    residual = nn.avg_pool(residual, window_shape=(2,), strides=(2,))
                x = x + residual
                
            x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
            skip_connections.append(x)
        
        # **NEW: Multi-scale feature fusion**
        # Upsample smaller feature maps and concatenate
        batch_size, seq_len, final_channels = x.shape
        fused_features = x
        
        for i, skip in enumerate(skip_connections[:-1]):
            # Upsample to match final sequence length
            skip_upsampled = jnp.repeat(skip, seq_len // skip.shape[1], axis=1)
            if skip_upsampled.shape[1] > seq_len:
                skip_upsampled = skip_upsampled[:, :seq_len, :]
            elif skip_upsampled.shape[1] < seq_len:
                # Pad to match
                pad_width = seq_len - skip_upsampled.shape[1]
                skip_upsampled = jnp.pad(skip_upsampled, 
                                       ((0, 0), (0, pad_width), (0, 0)), 
                                       mode='edge')
            
            # Project to same channel dimension
            skip_proj = nn.Dense(final_channels)(skip_upsampled)
            fused_features = fused_features + 0.1 * skip_proj  # Weighted addition
        
        # **ENHANCED: Bidirectional GRU with attention**
        gru_features = fused_features.shape[-1]
        
        # Forward GRU
        forward_gru = nn.scan(
            nn.GRUCell(),
            variable_broadcast="params",
            split_rngs={"params": False},
            length=seq_len
        )
        
        carry_forward = nn.GRUCell().initialize_carry(
            jax.random.PRNGKey(0), (batch_size, gru_features)
        )
        
        # Transpose for scan: [time, batch, features]
        x_transposed = jnp.transpose(fused_features, (1, 0, 2))
        carry_forward, forward_states = forward_gru(carry_forward, x_transposed)
        forward_states = jnp.transpose(forward_states, (1, 0, 2))  # Back to [batch, time, features]
        
        # Backward GRU
        backward_gru = nn.scan(
            nn.GRUCell(),
            variable_broadcast="params", 
            split_rngs={"params": False},
            length=seq_len,
            reverse=True
        )
        
        carry_backward = nn.GRUCell().initialize_carry(
            jax.random.PRNGKey(1), (batch_size, gru_features)
        )
        
        carry_backward, backward_states = backward_gru(carry_backward, x_transposed)
        backward_states = jnp.transpose(backward_states, (1, 0, 2))
        
        # Concatenate bidirectional states
        bidirectional_states = jnp.concatenate([forward_states, backward_states], axis=-1)
        
        # **NEW: Multi-head self-attention**
        if self.use_attention:
            attention_dim = bidirectional_states.shape[-1]
            
            # Self-attention mechanism
            attention = nn.MultiHeadDotProductAttention(
                num_heads=self.num_attention_heads,
                qkv_features=attention_dim,
                dropout_rate=self.dropout_rate if train else 0.0
            )
            
            attended_features = attention(
                bidirectional_states,  # queries
                bidirectional_states,  # keys
                bidirectional_states,  # values
                deterministic=not train
            )
            
            # Residual connection
            bidirectional_states = bidirectional_states + attended_features
            
            # Layer normalization
            bidirectional_states = nn.LayerNorm()(bidirectional_states)
        
        # **ENHANCED: Progressive projection to latent space**
        # First reduce dimension gradually
        x = nn.Dense(512, kernel_init=nn.initializers.he_normal())(bidirectional_states)
        x = nn.gelu(x)
        x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
        
        x = nn.Dense(256, kernel_init=nn.initializers.he_normal())(x)
        x = nn.gelu(x)
        x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
        
        # Final projection to latent dimension
        latent_features = nn.Dense(
            self.latent_dim,
            kernel_init=nn.initializers.xavier_uniform(),
            bias_init=nn.initializers.zeros
        )(x)
        
        # **ENHANCED: Adaptive normalization**
        # Only normalize if the norm is significant
        norms = jnp.linalg.norm(latent_features, axis=-1, keepdims=True)
        normalized_features = jnp.where(
            norms > 1e-6,
            latent_features / (norms + 1e-8),
            latent_features
        )
        
        return normalized_features


class DeepSNN(nn.Module):
    """Deep Spiking Neural Network with multiple layers."""
    
    hidden_sizes: Tuple[int, ...] = (256, 128, 64)
    num_classes: int = 3
    dropout_rate: float = 0.2
    
    @nn.compact
    def __call__(self, spikes: jnp.ndarray, train: bool = True) -> jnp.ndarray:
        """
        spikes: [batch, time, input_dim]
        Returns: [batch, num_classes] logits
        """
        x = spikes
        
        # Multiple LIF layers with skip connections
        skip_connections = []
        
        for i, hidden_size in enumerate(self.hidden_sizes):
            # LIF layer (simplified - would use actual SNN implementation)
            x_dense = nn.Dense(hidden_size)(x)
            
            # Apply leaky integration (simplified LIF dynamics)
            # In real implementation, this would be proper LIF neurons with Spyx
            x_lif = nn.tanh(x_dense)  # Placeholder for actual LIF dynamics
            
            # Dropout for regularization
            x_lif = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x_lif)
            
            # Skip connection for deeper networks
            if i > 0 and x.shape[-1] == hidden_size:
                x_lif = x_lif + x  # Residual connection
                
            x = x_lif
            skip_connections.append(x)
        
        # **NEW: Temporal attention pooling**
        # Instead of simple mean pooling, use attention to focus on important time steps
        temporal_weights = nn.Dense(1)(x)  # [batch, time, 1]
        temporal_weights = nn.softmax(temporal_weights, axis=1)
        
        # Weighted average over time
        x_pooled = jnp.sum(x * temporal_weights, axis=1)  # [batch, features]
        
        # **NEW: Multi-scale temporal features**
        # Add features from different temporal scales
        for skip in skip_connections[:-1]:
            skip_weights = nn.Dense(1)(skip)
            skip_weights = nn.softmax(skip_weights, axis=1)
            skip_pooled = jnp.sum(skip * skip_weights, axis=1)
            
            # Project to same dimension and add
            skip_proj = nn.Dense(x_pooled.shape[-1])(skip_pooled)
            x_pooled = x_pooled + 0.1 * skip_proj
        
        # Final classification layers
        x = nn.Dense(128)(x_pooled)
        x = nn.gelu(x)
        x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
        
        x = nn.Dense(64)(x)
        x = nn.gelu(x)
        x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
        
        # Output layer
        logits = nn.Dense(self.num_classes)(x)
        
        return logits


def focal_loss(logits: jnp.ndarray, labels: jnp.ndarray, 
               alpha: float = 0.25, gamma: float = 2.0) -> jnp.ndarray:
    """
    Focal Loss for addressing class imbalance.
    
    Focuses learning on hard examples and down-weights easy examples.
    """
    # Convert to probabilities
    probs = nn.softmax(logits, axis=-1)
    
    # One-hot encode labels
    num_classes = logits.shape[-1]
    labels_onehot = jax.nn.one_hot(labels, num_classes)
    
    # Get probability of true class
    pt = jnp.sum(probs * labels_onehot, axis=-1)
    
    # Focal loss computation
    focal_weight = alpha * jnp.power(1 - pt, gamma)
    focal_loss_val = -focal_weight * jnp.log(pt + 1e-8)
    
    return jnp.mean(focal_loss_val)


def mixup_data(x: jnp.ndarray, y: jnp.ndarray, alpha: float = 0.2, 
               key: jnp.ndarray = None) -> Tuple[jnp.ndarray, jnp.ndarray, float]:
    """
    Mixup data augmentation for better generalization.
    """
    if key is None:
        key = jax.random.PRNGKey(42)
        
    batch_size = x.shape[0]
    
    # Sample lambda from Beta distribution
    lam = jax.random.beta(key, alpha, alpha)
    
    # Random permutation
    key, subkey = jax.random.split(key)
    indices = jax.random.permutation(subkey, batch_size)
    
    # Mix inputs
    mixed_x = lam * x + (1 - lam) * x[indices]
    
    # Mix labels (soft labels)
    y_onehot = jax.nn.one_hot(y, 3)  # 3 classes
    y_mixed_onehot = lam * y_onehot + (1 - lam) * y_onehot[indices]
    
    return mixed_x, y_mixed_onehot, lam


class AdvancedGWTrainer:
    """Advanced trainer with state-of-the-art techniques."""
    
    def __init__(self, config: AdvancedTrainingConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Enhanced models
        self.cpc_model = AttentionCPCEncoder(
            latent_dim=config.cpc_latent_dim,
            conv_channels=config.cpc_conv_channels,
            use_attention=config.use_attention
        )
        
        self.snn_model = DeepSNN(
            hidden_sizes=config.snn_hidden_sizes,
            num_classes=3
        )
        
        self.spike_bridge = SpikeBridge(
            encoding_strategy=config.spike_encoding,
            spike_time_steps=config.spike_time_steps
        )
        
        # **ADVANCED: AdamW optimizer with cosine annealing**
        if config.use_cosine_scheduling:
            lr_schedule = optax.cosine_decay_schedule(
                init_value=config.learning_rate,
                decay_steps=config.num_epochs * 20,  # Assume ~20 batches per epoch
                alpha=0.01  # Final LR = 1% of initial
            )
        else:
            lr_schedule = config.learning_rate
            
        self.cpc_optimizer = optax.adamw(
            learning_rate=lr_schedule,
            weight_decay=config.weight_decay
        )
        
        self.snn_optimizer = optax.adamw(
            learning_rate=lr_schedule,
            weight_decay=config.weight_decay
        )
        
        # Data generators
        self.continuous_generator = ContinuousGWGenerator(
            base_frequency=50.0,
            freq_range=(20.0, 500.0),  # Wider frequency range
            duration=config.signal_duration
        )
        
        logger.info("Initialized Advanced GW Trainer")
        logger.info(f"  Enhanced architecture: {config.cpc_conv_channels} -> {config.cpc_latent_dim}")
        logger.info(f"  Deep SNN: {config.snn_hidden_sizes}")
        logger.info(f"  Advanced techniques: attention={config.use_attention}, focal_loss={config.use_focal_loss}")
    
    def generate_enhanced_dataset(self) -> Dict:
        """Generate balanced, high-quality dataset."""
        logger.info("Generating advanced multi-signal dataset...")
        
        # **ENHANCED: Balanced dataset with quality control**
        continuous_data = self.continuous_generator.generate_training_dataset(
            num_signals=self.config.num_continuous_signals,
            signal_duration=self.config.signal_duration,
            include_noise_only=False
        )
        
        # Generate more realistic binary signals with parameter sweeps
        binary_data = self._generate_realistic_binary_signals()
        
        # Generate controlled noise samples
        noise_data = self._generate_controlled_noise()
        
        # Combine with careful balancing
        dataset = self._combine_balanced_datasets(continuous_data, binary_data, noise_data)
        
        logger.info(f"Advanced dataset: {dataset['data'].shape}")
        logger.info(f"Class distribution: {jnp.bincount(dataset['labels'])}")
        
        return dataset
    
    def _generate_realistic_binary_signals(self) -> Dict:
        """Generate more realistic binary merger signals."""
        logger.info(f"Generating {self.config.num_binary_signals} realistic binary signals...")
        
        all_data = []
        all_metadata = []
        
        # Parameter ranges from real GW events
        mass_ranges = [(10, 50), (20, 80), (30, 100)]  # Solar masses
        
        for i in range(self.config.num_binary_signals):
            # Varied parameters for diversity
            mass_range = mass_ranges[i % len(mass_ranges)]
            m1 = jax.random.uniform(jax.random.PRNGKey(i), minval=mass_range[0], maxval=mass_range[1])
            m2 = jax.random.uniform(jax.random.PRNGKey(i+1000), minval=mass_range[0], maxval=mass_range[1])
            
            # More realistic frequency evolution
            duration = self.config.signal_duration
            t = jnp.linspace(0, duration, int(duration * 4096))
            
            # Chirp mass calculation
            chirp_mass = jnp.power(m1 * m2, 3/5) / jnp.power(m1 + m2, 1/5)
            
            # More accurate frequency evolution
            tau = duration - t
            f_t = jnp.where(
                tau > 1e-3,
                jnp.power(256 * jnp.pi * chirp_mass * 1.989e30 * 6.674e-11 / (3 * 2.998e8**3), -3/8) * jnp.power(tau, -3/8) / (2 * jnp.pi),
                250.0  # Final frequency
            )
            
            # Amplitude with proper decay
            amplitude = 1e-21 / jnp.sqrt(1 + (t / (duration * 0.1))**2)
            
            # Generate waveform
            phase = jnp.cumsum(2 * jnp.pi * f_t) / 4096
            signal = amplitude * jnp.sin(phase)
            
            # Add realistic noise
            noise = jax.random.normal(jax.random.PRNGKey(i + 2000), signal.shape) * 1e-23
            binary_signal = signal + noise
            
            all_data.append(binary_signal)
            all_metadata.append({
                'signal_type': 'binary_merger',
                'm1': float(m1),
                'm2': float(m2),
                'chirp_mass': float(chirp_mass),
                'detector': 'H1'
            })
        
        return {
            'data': jnp.stack(all_data),
            'metadata': all_metadata
        }
    
    def _generate_controlled_noise(self) -> Dict:
        """Generate controlled noise samples with realistic PSD."""
        num_noise = self.config.num_noise_samples
        logger.info(f"Generating {num_noise} controlled noise samples...")
        
        noise_length = int(self.config.signal_duration * 4096)
        
        all_noise = []
        all_metadata = []
        
        for i in range(num_noise):
            # Generate noise with realistic LIGO PSD characteristics
            key = jax.random.PRNGKey(i + 3000)
            
            # Base Gaussian noise
            noise = jax.random.normal(key, (noise_length,)) * 1e-23
            
            # Add some colored noise characteristics (simplified)
            # Low-frequency noise enhancement
            freqs = jnp.fft.fftfreq(noise_length, 1/4096)
            noise_fft = jnp.fft.fft(noise)
            
            # Simple PSD model (1/f at low frequencies)
            psd_model = jnp.where(
                jnp.abs(freqs) < 20,  # Below 20 Hz
                10.0 / (jnp.abs(freqs) + 1),  # 1/f noise
                1.0  # White noise above 20 Hz
            )
            
            colored_noise_fft = noise_fft * jnp.sqrt(psd_model)
            colored_noise = jnp.real(jnp.fft.ifft(colored_noise_fft))
            
            all_noise.append(colored_noise)
            all_metadata.append({
                'signal_type': 'noise_only',
                'detector': 'H1'
            })
        
        return {
            'data': jnp.stack(all_noise),
            'metadata': all_metadata
        }
    
    def _combine_balanced_datasets(self, continuous_data: Dict, binary_data: Dict, noise_data: Dict) -> Dict:
        """Combine datasets with careful class balancing."""
        # Extract signal data only
        cont_data = continuous_data['data'][continuous_data['labels'] == 1]
        bin_data = binary_data['data']
        noise_data_array = noise_data['data']
        
        # Ensure balanced classes
        min_samples = min(len(cont_data), len(bin_data), len(noise_data_array))
        
        cont_data = cont_data[:min_samples]
        bin_data = bin_data[:min_samples]
        noise_data_array = noise_data_array[:min_samples]
        
        # Combine data
        all_data = jnp.concatenate([noise_data_array, cont_data, bin_data], axis=0)
        
        # Create balanced labels
        noise_labels = jnp.zeros(min_samples, dtype=jnp.int32)
        cont_labels = jnp.ones(min_samples, dtype=jnp.int32)
        bin_labels = jnp.ones(min_samples, dtype=jnp.int32) * 2
        
        all_labels = jnp.concatenate([noise_labels, cont_labels, bin_labels])
        
        # Combine metadata
        all_metadata = (
            noise_data['metadata'][:min_samples] +
            [m for m, l in zip(continuous_data['metadata'], continuous_data['labels']) if l == 1][:min_samples] +
            binary_data['metadata'][:min_samples]
        )
        
        # Shuffle dataset
        key = jax.random.PRNGKey(42)
        indices = jax.random.permutation(key, len(all_data))
        
        return {
            'data': all_data[indices],
            'labels': all_labels[indices],
            'metadata': [all_metadata[i] for i in indices],
            'signal_types': ['noise', 'continuous_gw', 'binary_merger'],
            'num_classes': 3
        }


def run_advanced_training_experiment():
    """Test advanced training pipeline."""
    print("ðŸš€ Advanced Training Experiment")
    
    config = AdvancedTrainingConfig(
        num_continuous_signals=20,
        num_binary_signals=20,
        num_noise_samples=20,
        batch_size=8,
        num_epochs=5,  # Quick test
        use_attention=True,
        use_focal_loss=True
    )
    
    trainer = AdvancedGWTrainer(config)
    dataset = trainer.generate_enhanced_dataset()
    
    print(f"âœ… Advanced dataset: {dataset['data'].shape}")
    print(f"âœ… Class distribution: {jnp.bincount(dataset['labels'])}")
    return True


if __name__ == "__main__":
    import os
    os.environ['JAX_PLATFORM_NAME'] = 'cpu'
    success = run_advanced_training_experiment()
    print("âœ… Advanced training ready!" if success else "âŒ Setup failed!")
```

## training/enhanced_gw_training.py

```python
#!/usr/bin/env python3

"""
Enhanced Gravitational Wave Training Pipeline

Combines continuous GW signals (PyFstat) + binary GW signals (GWOSC)
for comprehensive neuromorphic CPC+SNN detector training.

Represents next-generation multi-signal type GW detection.
"""

import jax
import jax.numpy as jnp
import optax
import logging
import time
from typing import Dict, Tuple, Optional, List
from pathlib import Path
from dataclasses import dataclass

from ..models.cpc_encoder import create_enhanced_cpc_encoder, enhanced_info_nce_loss
from ..models.simple_snn import create_simple_snn, SimpleSNNTrainer
from ..models.spike_bridge import SpikeBridge, SpikeEncodingStrategy
from ..data.continuous_gw_generator import ContinuousGWGenerator, create_mixed_gw_dataset
from ..data.gw_download import ProductionGWOSCDownloader

logger = logging.getLogger(__name__)


@dataclass
class EnhancedTrainingConfig:
    """Configuration for enhanced GW training."""
    # Data configuration
    num_continuous_signals: int = 200
    num_binary_signals: int = 200
    signal_duration: float = 4.0  # seconds
    mix_ratio: float = 0.5  # Ratio continuous:binary
    
    # Training configuration
    batch_size: int = 16
    learning_rate: float = 1e-3
    num_epochs: int = 50
    
    # Model configuration
    cpc_latent_dim: int = 128
    snn_hidden_size: int = 64
    spike_time_steps: int = 50
    
    # Spike encoding
    spike_encoding: SpikeEncodingStrategy = SpikeEncodingStrategy.POISSON_RATE
    spike_rate: float = 100.0  # Hz
    
    # Output
    output_dir: str = "enhanced_gw_training_outputs"
    save_models: bool = True
    

class EnhancedGWTrainer:
    """
    Enhanced Gravitational Wave Trainer.
    
    Trains CPC+SNN neuromorphic detector on combined
    continuous + binary gravitational wave signals.
    """
    
    def __init__(self, config: EnhancedTrainingConfig):
        """
        Initialize enhanced GW trainer.
        
        Args:
            config: Training configuration
        """
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize data generators
        self.continuous_generator = ContinuousGWGenerator(
            base_frequency=50.0,
            freq_range=(20.0, 200.0),
            duration=config.signal_duration
        )
        
        self.binary_downloader = ProductionGWOSCDownloader()
        
        # Initialize models
        self.cpc_model = create_enhanced_cpc_encoder(
            latent_dim=config.cpc_latent_dim,
            use_batch_norm=False,  # Simplified for now
            dropout_rate=0.0
        )
        
        self.snn_model = create_simple_snn(
            hidden_size=config.snn_hidden_size,
            num_classes=3  # 3 classes: noise, continuous, binary
        )
        
        self.spike_bridge = SpikeBridge(
            encoding_strategy=config.spike_encoding,
            spike_time_steps=config.spike_time_steps,
            max_spike_rate=config.spike_rate
        )
        
        # Optimizers
        self.cpc_optimizer = optax.adam(config.learning_rate)
        self.snn_trainer = SimpleSNNTrainer(config.learning_rate)
        
        logger.info("Initialized Enhanced GW Trainer")
        logger.info(f"  Output directory: {self.output_dir}")
        logger.info(f"  Signal types: continuous + binary")
        logger.info(f"  Mix ratio: {config.mix_ratio}")
        
    def generate_enhanced_dataset(self) -> Dict:
        """
        Generate enhanced dataset with continuous + binary + noise signals.
        
        Returns:
            Enhanced dataset with multiple signal types
        """
        logger.info("Generating enhanced multi-signal dataset...")
        
        # Generate continuous signals
        continuous_data = self.continuous_generator.generate_training_dataset(
            num_signals=self.config.num_continuous_signals,
            signal_duration=self.config.signal_duration,
            include_noise_only=False  # Handle noise separately
        )
        
        # Generate synthetic binary signals (simplified)
        binary_data = self._generate_synthetic_binary_signals()
        
        # Generate pure noise samples
        noise_data = self._generate_noise_samples()
        
        # Combine all data
        enhanced_dataset = self._combine_datasets(continuous_data, binary_data, noise_data)
        
        logger.info(f"Enhanced dataset generated:")
        logger.info(f"  Total samples: {enhanced_dataset['data'].shape[0]}")
        logger.info(f"  Signal types: {jnp.unique(enhanced_dataset['labels'])}")
        logger.info(f"  Data shape: {enhanced_dataset['data'].shape}")
        
        return enhanced_dataset
    
    def _generate_synthetic_binary_signals(self) -> Dict:
        """Generate synthetic binary GW signals (simplified chirp model)."""
        logger.info(f"Generating {self.config.num_binary_signals} synthetic binary signals...")
        
        all_data = []
        all_metadata = []
        
        for i in range(self.config.num_binary_signals):
            # Simple chirp parameters
            f_start = jnp.array(35.0 + 10 * jax.random.uniform(jax.random.PRNGKey(i)))
            f_end = jnp.array(250.0)
            duration = self.config.signal_duration
            
            # Generate chirp signal
            t = jnp.linspace(0, duration, int(duration * 4096))
            
            # Frequency evolution (simplified)
            alpha = (f_end - f_start) / duration
            freq_t = f_start + alpha * t
            
            # Chirp signal
            signal = 1e-21 * jnp.sin(2 * jnp.pi * jnp.cumsum(freq_t) / 4096)
            
            # Add noise
            noise = jax.random.normal(jax.random.PRNGKey(i + 1000), signal.shape) * 1e-23
            binary_signal = signal + noise
            
            all_data.append(binary_signal)
            all_metadata.append({
                'signal_type': 'binary_merger',
                'f_start': float(f_start),
                'f_end': float(f_end),
                'detector': 'H1'
            })
        
        return {
            'data': jnp.stack(all_data),
            'metadata': all_metadata
        }
    
    def _generate_noise_samples(self) -> Dict:
        """Generate pure noise samples."""
        num_noise = self.config.num_continuous_signals + self.config.num_binary_signals
        logger.info(f"Generating {num_noise} pure noise samples...")
        
        noise_length = int(self.config.signal_duration * 4096)
        
        all_noise = []
        all_metadata = []
        
        for i in range(num_noise):
            noise = jax.random.normal(jax.random.PRNGKey(i + 2000), (noise_length,)) * 1e-23
            
            all_noise.append(noise)
            all_metadata.append({
                'signal_type': 'noise_only',
                'detector': 'H1'
            })
        
        return {
            'data': jnp.stack(all_noise),
            'metadata': all_metadata
        }
    
    def _combine_datasets(self, continuous_data: Dict, binary_data: Dict, noise_data: Dict) -> Dict:
        """Combine continuous, binary, and noise datasets."""
        # Extract data arrays
        cont_data = continuous_data['data'][continuous_data['labels'] == 1]  # Only signals
        bin_data = binary_data['data']
        noise_data_array = noise_data['data']
        
        # Combine data
        all_data = jnp.concatenate([cont_data, bin_data, noise_data_array], axis=0)
        
        # Create labels: 0=noise, 1=continuous, 2=binary
        cont_labels = jnp.ones(cont_data.shape[0], dtype=jnp.int32)  # Continuous
        bin_labels = jnp.ones(bin_data.shape[0], dtype=jnp.int32) * 2  # Binary
        noise_labels = jnp.zeros(noise_data_array.shape[0], dtype=jnp.int32)  # Noise
        
        all_labels = jnp.concatenate([cont_labels, bin_labels, noise_labels])
        
        # Combine metadata
        all_metadata = (
            [m for m, l in zip(continuous_data['metadata'], continuous_data['labels']) if l == 1] +
            binary_data['metadata'] + 
            noise_data['metadata']
        )
        
        # Shuffle data
        key = jax.random.PRNGKey(42)
        indices = jax.random.permutation(key, len(all_data))
        
        return {
            'data': all_data[indices],
            'labels': all_labels[indices],
            'metadata': [all_metadata[i] for i in indices],
            'signal_types': ['noise', 'continuous_gw', 'binary_merger'],
            'num_classes': 3
        }
    
    def train_cpc_encoder(self, dataset: Dict, num_epochs: int = 20) -> Dict:
        """
        Train CPC encoder on enhanced dataset.
        
        Args:
            dataset: Enhanced multi-signal dataset
            num_epochs: Number of training epochs
            
        Returns:
            Training results and final parameters
        """
        logger.info("Training CPC encoder on enhanced dataset...")
        
        # Initialize parameters
        key = jax.random.PRNGKey(42)
        dummy_input = jnp.ones((1, int(self.config.signal_duration * 4096)))
        cpc_params = self.cpc_model.init(key, dummy_input)
        opt_state = self.cpc_optimizer.init(cpc_params)
        
        # Training loop
        losses = []
        
        for epoch in range(num_epochs):
            epoch_losses = []
            
            # Batch training
            data = dataset['data']
            num_batches = len(data) // self.config.batch_size
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * self.config.batch_size
                end_idx = start_idx + self.config.batch_size
                batch_data = data[start_idx:end_idx]
                
                # Training step
                cpc_params, opt_state, loss = self._cpc_training_step(
                    cpc_params, opt_state, batch_data, jax.random.PRNGKey(epoch * 1000 + batch_idx)
                )
                
                epoch_losses.append(loss)
            
            avg_loss = jnp.mean(jnp.array(epoch_losses))
            losses.append(avg_loss)
            
            if epoch % 5 == 0:
                logger.info(f"CPC Epoch {epoch}: Loss = {avg_loss:.4f}")
        
        logger.info(f"CPC training completed. Final loss: {losses[-1]:.4f}")
        
        return {
            'cpc_params': cpc_params,
            'losses': losses,
            'epochs': num_epochs
        }
    
    def _cpc_training_step(self, params, opt_state, batch, key):
        """Single CPC training step."""
        def loss_fn(params):
            # Encode batch
            encoded = self.cpc_model.apply(params, batch)
            
            # Create context-target pairs for contrastive learning
            context_len = encoded.shape[1] // 2
            context = encoded[:, :context_len]
            targets = encoded[:, context_len:]
            
            # Enhanced InfoNCE loss with proper temperature parameter
            loss = enhanced_info_nce_loss(context, targets, temperature=0.1)
            return loss
        
        loss, grads = jax.value_and_grad(loss_fn)(params)
        updates, opt_state = self.cpc_optimizer.update(grads, opt_state)
        params = optax.apply_updates(params, updates)
        
        return params, opt_state, loss
    
    def train_snn_classifier(self, dataset: Dict, cpc_params: Dict, num_epochs: int = 30) -> Dict:
        """
        Train SNN classifier on CPC-encoded spike trains.
        
        Args:
            dataset: Enhanced dataset
            cpc_params: Trained CPC parameters
            num_epochs: Number of training epochs
            
        Returns:
            Training results and SNN parameters
        """
        logger.info("Training SNN classifier on spike-encoded features...")
        
        # Initialize SNN parameters
        key = jax.random.PRNGKey(123)
        dummy_spikes = jnp.ones((1, self.config.spike_time_steps, self.config.cpc_latent_dim))
        snn_params = self.snn_model.init(key, dummy_spikes)
        opt_state = self.snn_trainer.optimizer.init(snn_params)
        
        # Encode all data through CPC
        logger.info("Encoding data through trained CPC...")
        cpc_features = self.cpc_model.apply(cpc_params, dataset['data'])
        
        # Convert to spike trains
        logger.info("Converting to spike trains...")
        spike_trains = []
        for i in range(len(cpc_features)):
            spikes = self.spike_bridge.encode(
                cpc_features[i:i+1], jax.random.PRNGKey(i)
            )
            spike_trains.append(spikes[0])  # Remove batch dimension
        
        spike_data = jnp.stack(spike_trains)
        labels = dataset['labels']
        
        logger.info(f"Spike data shape: {spike_data.shape}")
        
        # Training loop
        accuracies = []
        losses = []
        
        for epoch in range(num_epochs):
            epoch_losses = []
            epoch_accs = []
            
            # Batch training
            num_batches = len(spike_data) // self.config.batch_size
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * self.config.batch_size
                end_idx = start_idx + self.config.batch_size
                
                batch_spikes = spike_data[start_idx:end_idx]
                batch_labels = labels[start_idx:end_idx]
                
                # Training step
                snn_params, opt_state, loss = self.snn_trainer.training_step(
                    snn_params, opt_state, batch_spikes, batch_labels, self.snn_model
                )
                
                # Accuracy
                acc = self.snn_trainer.accuracy(
                    snn_params, batch_spikes, batch_labels, self.snn_model
                )
                
                epoch_losses.append(loss)
                epoch_accs.append(acc)
            
            avg_loss = jnp.mean(jnp.array(epoch_losses))
            avg_acc = jnp.mean(jnp.array(epoch_accs))
            
            losses.append(avg_loss)
            accuracies.append(avg_acc)
            
            if epoch % 5 == 0:
                logger.info(f"SNN Epoch {epoch}: Loss = {avg_loss:.4f}, Acc = {avg_acc:.3f}")
        
        logger.info(f"SNN training completed. Final accuracy: {accuracies[-1]:.3f}")
        
        return {
            'snn_params': snn_params,
            'losses': losses,
            'accuracies': accuracies,
            'epochs': num_epochs,
            'final_accuracy': float(accuracies[-1])
        }
    
    def evaluate_multi_signal_performance(self, dataset: Dict, cpc_params: Dict, snn_params: Dict) -> Dict:
        """
        Evaluate performance on different signal types.
        
        Args:
            dataset: Test dataset
            cpc_params: Trained CPC parameters  
            snn_params: Trained SNN parameters
            
        Returns:
            Performance metrics by signal type
        """
        logger.info("Evaluating multi-signal performance...")
        
        # Encode through full pipeline
        cpc_features = self.cpc_model.apply(cpc_params, dataset['data'])
        
        spike_trains = []
        for i in range(len(cpc_features)):
            spikes = self.spike_bridge.encode(
                cpc_features[i:i+1], jax.random.PRNGKey(i + 5000)
            )
            spike_trains.append(spikes[0])
        
        spike_data = jnp.stack(spike_trains)
        
        # Get predictions
        logits = self.snn_model.apply(snn_params, spike_data)
        predictions = jnp.argmax(logits, axis=1)
        true_labels = dataset['labels']
        
        # Compute metrics by signal type
        results = {}
        for signal_type_idx, signal_type in enumerate(dataset['signal_types']):
            mask = true_labels == signal_type_idx
            if jnp.sum(mask) > 0:
                type_predictions = predictions[mask]
                type_labels = true_labels[mask]
                accuracy = jnp.mean(type_predictions == type_labels)
                
                results[signal_type] = {
                    'accuracy': float(accuracy),
                    'num_samples': int(jnp.sum(mask)),
                    'true_positives': int(jnp.sum((type_predictions == signal_type_idx) & (type_labels == signal_type_idx)))
                }
        
        # Overall accuracy
        overall_accuracy = jnp.mean(predictions == true_labels)
        results['overall'] = {
            'accuracy': float(overall_accuracy),
            'num_samples': len(true_labels)
        }
        
        logger.info("Multi-signal performance evaluation:")
        for signal_type, metrics in results.items():
            logger.info(f"  {signal_type}: {metrics['accuracy']:.3f} ({metrics['num_samples']} samples)")
        
        return results
    
    def run_enhanced_training(self) -> Dict:
        """
        Run complete enhanced training pipeline.
        
        Returns:
            Complete training results
        """
        logger.info("ðŸš€ Starting Enhanced GW Training Pipeline...")
        start_time = time.time()
        
        # Generate enhanced dataset
        dataset = self.generate_enhanced_dataset()
        
        # Split into train/test (80/20)
        split_idx = int(0.8 * len(dataset['data']))
        train_data = {
            'data': dataset['data'][:split_idx],
            'labels': dataset['labels'][:split_idx],
            'metadata': dataset['metadata'][:split_idx],
            'signal_types': dataset['signal_types'],
            'num_classes': dataset['num_classes']
        }
        
        test_data = {
            'data': dataset['data'][split_idx:],
            'labels': dataset['labels'][split_idx:],
            'metadata': dataset['metadata'][split_idx:],
            'signal_types': dataset['signal_types'],
            'num_classes': dataset['num_classes']
        }
        
        # Train CPC encoder
        cpc_results = self.train_cpc_encoder(train_data, num_epochs=20)
        
        # Train SNN classifier
        snn_results = self.train_snn_classifier(train_data, cpc_results['cpc_params'], num_epochs=30)
        
        # Evaluate performance
        performance = self.evaluate_multi_signal_performance(
            test_data, cpc_results['cpc_params'], snn_results['snn_params']
        )
        
        # Save models if requested
        if self.config.save_models:
            self._save_models(cpc_results['cpc_params'], snn_results['snn_params'])
        
        total_time = time.time() - start_time
        
        final_results = {
            'cpc_training': cpc_results,
            'snn_training': snn_results,
            'performance': performance,
            'training_time': total_time,
            'config': self.config
        }
        
        logger.info(f"ðŸŽ‰ Enhanced Training Completed!")
        logger.info(f"  Total time: {total_time:.1f} seconds")
        logger.info(f"  Overall accuracy: {performance['overall']['accuracy']:.3f}")
        logger.info(f"  CPC final loss: {cpc_results['losses'][-1]:.4f}")
        logger.info(f"  SNN final accuracy: {snn_results['final_accuracy']:.3f}")
        
        return final_results
    
    def _save_models(self, cpc_params: Dict, snn_params: Dict):
        """Save trained model parameters."""
        import pickle
        
        models_dir = self.output_dir / "models"
        models_dir.mkdir(exist_ok=True)
        
        # Save CPC parameters
        with open(models_dir / "cpc_params.pkl", "wb") as f:
            pickle.dump(cpc_params, f)
        
        # Save SNN parameters  
        with open(models_dir / "snn_params.pkl", "wb") as f:
            pickle.dump(snn_params, f)
        
        logger.info(f"Models saved to {models_dir}")


def run_enhanced_gw_training_experiment():
    """Quick test of enhanced training."""
    print("ðŸŒŸ Enhanced GW Training Test")
    
    config = EnhancedTrainingConfig(
        num_continuous_signals=5,  # Small test
        num_binary_signals=5,
        signal_duration=2.0,  # Shorter for testing
        batch_size=4,
        num_epochs=2
    )
    
    trainer = EnhancedGWTrainer(config)
    dataset = trainer.generate_enhanced_dataset()
    
    print(f"âœ… Dataset generated: {dataset['data'].shape}")
    print(f"âœ… Signal types: {dataset['signal_types']}")
    return True


if __name__ == "__main__":
    import os
    os.environ['JAX_PLATFORMS'] = 'cpu'
    success = run_enhanced_gw_training_experiment()
    print("âœ… Test completed!" if success else "âŒ Test failed!")
```

## training/pretrain_cpc.py

```python
#!/usr/bin/env python3
"""
CPC Encoder Pretraining Script

Self-supervised contrastive pretraining dla gravitational wave detection.
Implements InfoNCE loss with gradient accumulation dla Apple Silicon optimization.

Usage:
    python pretrain_cpc.py --config config.yaml --num_steps 100000
"""

import jax
import jax.numpy as jnp
import flax.linen as nn
import optax
import orbax.checkpoint as ocp
import logging
import time
import argparse
from pathlib import Path
from typing import Dict, Any, Tuple, List
from dataclasses import dataclass, asdict
import yaml
import wandb

from ..models.cpc_encoder import CPCEncoder, info_nce_loss
from ..data.gw_download import ProductionGWOSCDownloader, AdvancedDataPreprocessor

logger = logging.getLogger(__name__)


@dataclass
class CPCTrainingConfig:
    """Configuration dla CPC pretraining."""
    # Architecture
    latent_dim: int = 256
    downsample_factor: int = 16
    conv_channels: Tuple[int, ...] = (32, 64, 128)
    
    # Training  
    num_steps: int = 100_000
    batch_size: int = 16
    accumulation_steps: int = 4  # Gradient accumulation dla Metal limitations
    learning_rate: float = 1e-3
    warmup_steps: int = 5_000
    
    # InfoNCE Loss
    context_length: int = 12  # Context windows dla prediction
    prediction_length: int = 4  # Future windows to predict
    num_negatives: int = 128
    temperature: float = 0.1
    
    # Data
    segment_duration: float = 4.0  # seconds
    sample_rate: int = 4096
    detectors: List[str] = None  # Will default to ['H1', 'L1']
    
    # Monitoring
    log_every: int = 100
    eval_every: int = 1000
    save_every: int = 5000
    
    # Paths
    output_dir: str = "experiments/cpc_pretraining"
    wandb_project: str = "ligo-cpc-snn"
    
    def __post_init__(self):
        if self.detectors is None:
            self.detectors = ['H1', 'L1']


class CPCTrainingState:
    """Training state dla CPC pretraining."""
    
    def __init__(self, config: CPCTrainingConfig):
        self.config = config
        self.step = 0
        self.epoch = 0
        self.best_loss = float('inf')
        
        # Initialize model
        self.model = CPCEncoder(
            latent_dim=config.latent_dim,
            conv_channels=config.conv_channels
        )
        
        # Initialize optimizer z warmup schedule
        self.scheduler = optax.warmup_cosine_decay_schedule(
            init_value=0.0,
            peak_value=config.learning_rate,
            warmup_steps=config.warmup_steps,
            decay_steps=config.num_steps - config.warmup_steps
        )
        
        self.optimizer = optax.adam(learning_rate=self.scheduler)
        
        # Initialize data pipeline
        self.downloader = ProductionGWOSCDownloader()
        self.preprocessor = AdvancedDataPreprocessor(
            sample_rate=config.sample_rate,
            apply_whitening=True
        )
        
        # Metrics tracking
        self.metrics = {
            'train_loss': [],
            'learning_rate': [],
            'gradient_norm': [],
            'processing_time': [],
            'examples_per_sec': []
        }
        
    def initialize_params(self, key: jnp.ndarray, input_shape: Tuple[int, ...]) -> Dict[str, Any]:
        """Initialize model parameters."""
        dummy_input = jnp.zeros((1,) + input_shape)
        return self.model.init(key, dummy_input)
        
    def create_training_segments(self, strain_data: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """
        Create context-target pairs dla contrastive learning.
        
        Args:
            strain_data: Input strain timeseries [batch, time]
            
        Returns:
            context_segments: Context windows [batch, context_length, features]
            target_segments: Target windows dla prediction [batch, prediction_length, features]
        """
        batch_size, seq_len = strain_data.shape
        
        # Encode full sequence
        encoded = self.model.apply(self.params, strain_data)  # [batch, downsampled_time, latent_dim]
        _, encoded_len, latent_dim = encoded.shape
        
        # Create sliding windows
        total_length = self.config.context_length + self.config.prediction_length
        
        if encoded_len < total_length:
            # Pad if sequence too short
            pad_length = total_length - encoded_len
            encoded = jnp.pad(encoded, ((0, 0), (0, pad_length), (0, 0)))
            encoded_len = total_length
            
        # Random starting positions
        max_start = encoded_len - total_length
        if max_start <= 0:
            start_idx = 0
        else:
            start_idx = jax.random.randint(
                jax.random.PRNGKey(self.step), (), 0, max_start
            )
        
        # Extract context and target windows
        context_end = start_idx + self.config.context_length
        target_end = context_end + self.config.prediction_length
        
        context_segments = encoded[:, start_idx:context_end, :]  # [batch, context_length, latent_dim]
        target_segments = encoded[:, context_end:target_end, :]  # [batch, prediction_length, latent_dim]
        
        return context_segments, target_segments


@jax.jit
def train_step(state: Dict[str, Any], 
              strain_batch: jnp.ndarray,
              key: jnp.ndarray) -> Tuple[Dict[str, Any], Dict[str, float]]:
    """
    Single training step z gradient accumulation.
    
    Args:
        state: Training state dict {params, opt_state}
        strain_batch: Batch of strain data [batch, time]
        key: Random key dla training
        
    Returns:
        Updated state and metrics
    """
    params, opt_state = state['params'], state['opt_state']
    
    def loss_fn(params, strain_data, key):
        # Forward pass przez CPC encoder
        encoded = model.apply(params, strain_data)
        
        # Create context-target pairs
        batch_size, seq_len, latent_dim = encoded.shape
        context_length = 12
        prediction_length = 4
        
        # Simple sliding window approach
        context = encoded[:, :context_length, :]
        target = encoded[:, context_length:context_length+prediction_length, :]
        
        # InfoNCE loss
        return info_nce_loss(context, target, temperature=0.1)
    
    # Compute loss and gradients
    loss, grads = jax.value_and_grad(loss_fn)(params, strain_batch, key)
    
    # Gradient norm dla monitoring
    grad_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree.leaves(grads)))
    
    # Update parameters
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    
    updated_state = {'params': params, 'opt_state': opt_state}
    metrics = {'loss': loss, 'grad_norm': grad_norm}
    
    return updated_state, metrics


class CPCPretrainer:
    """Main pretraining orchestrator dla CPC encoder."""
    
    def __init__(self, config: CPCTrainingConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup logging
        self._setup_logging()
        
        # Initialize training state
        self.training_state = CPCTrainingState(config)
        
        # Setup checkpointing
        self.checkpointer = ocp.StandardCheckpointer()
        
        # Setup W&B logging
        if config.wandb_project:
            wandb.init(
                project=config.wandb_project,
                config=asdict(config),
                name=f"cpc_pretraining_{int(time.time())}"
            )
            
    def _setup_logging(self):
        """Setup file and console logging."""
        log_file = self.output_dir / "training.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
    def generate_training_data(self) -> List[jnp.ndarray]:
        """
        Generate training dataset from historical GW events.
        
        Returns:
            List of preprocessed strain segments
        """
        logger.info("Generating training dataset from GWOSC...")
        
        # Historical GW events dla training
        training_events = [
            # Major detections with clear signals
            ('H1', 1126259446, 4.0),  # GW150914
            ('L1', 1126259446, 4.0),  # GW150914
            ('H1', 1128678900, 4.0),  # GW151012
            ('L1', 1128678900, 4.0),  # GW151012  
            ('H1', 1135136350, 4.0),  # GW151226
            ('L1', 1135136350, 4.0),  # GW151226
            ('H1', 1167559936, 4.0),  # GW170104
            ('L1', 1167559936, 4.0),  # GW170104
            ('H1', 1180922494, 4.0),  # GW170608
            ('L1', 1180922494, 4.0),  # GW170608
        ]
        
        # Download data in batches
        logger.info(f"Downloading {len(training_events)} training segments...")
        
        strain_segments = self.training_state.downloader.fetch_batch(
            training_events, max_workers=4
        )
        
        # Preprocess all segments
        processed_segments = []
        successful = 0
        
        for i, strain_data in enumerate(strain_segments):
            if strain_data is not None:
                try:
                    result = self.training_state.preprocessor.process(strain_data)
                    
                    if result.quality.is_valid and result.quality.snr_estimate > 5.0:
                        processed_segments.append(result.strain_data)
                        successful += 1
                        
                except Exception as e:
                    logger.warning(f"Failed to process segment {i}: {e}")
                    
        logger.info(f"Successfully processed {successful}/{len(training_events)} segments")
        
        return processed_segments
        
    def train(self):
        """Main training loop dla CPC pretraining."""
        logger.info("ðŸš€ Starting CPC Pretraining")
        logger.info("=" * 60)
        logger.info(f"Config: {asdict(self.config)}")
        
        # Generate training data
        training_data = self.generate_training_data()
        
        if len(training_data) < 4:
            raise ValueError(f"Insufficient training data: {len(training_data)} segments")
            
        # Initialize model parameters
        key = jax.random.PRNGKey(42)
        input_shape = (int(self.config.segment_duration * self.config.sample_rate),)
        
        params = self.training_state.initialize_params(key, input_shape)
        opt_state = self.training_state.optimizer.init(params)
        
        state = {'params': params, 'opt_state': opt_state}
        
        # Training loop
        logger.info(f"Training dla {self.config.num_steps} steps...")
        
        start_time = time.perf_counter()
        
        for step in range(self.config.num_steps):
            step_start = time.perf_counter()
            
            # Sample random batch from training data
            batch_indices = jax.random.choice(
                jax.random.PRNGKey(step), 
                len(training_data), 
                (self.config.batch_size,),
                replace=True
            )
            
            strain_batch = jnp.stack([training_data[i] for i in batch_indices])
            
            # Training step
            key = jax.random.PRNGKey(step)
            state, metrics = train_step(state, strain_batch, key)
            
            step_time = time.perf_counter() - step_start
            
            # Logging
            if step % self.config.log_every == 0:
                lr = self.training_state.scheduler(step)
                examples_per_sec = self.config.batch_size / step_time
                
                logger.info(
                    f"Step {step:6d}: loss={metrics['loss']:.4f}, "
                    f"grad_norm={metrics['grad_norm']:.4f}, "
                    f"lr={lr:.2e}, "
                    f"examples/sec={examples_per_sec:.1f}"
                )
                
                # W&B logging
                if wandb.run:
                    wandb.log({
                        'train/loss': metrics['loss'],
                        'train/grad_norm': metrics['grad_norm'],
                        'train/learning_rate': lr,
                        'train/examples_per_sec': examples_per_sec,
                        'step': step
                    })
                    
            # Checkpointing
            if step % self.config.save_every == 0 and step > 0:
                checkpoint_path = self.output_dir / f"checkpoint_{step}"
                
                self.checkpointer.save(
                    checkpoint_path,
                    {'params': state['params'], 'step': step}
                )
                
                logger.info(f"Saved checkpoint at step {step}")
                
        total_time = time.perf_counter() - start_time
        
        # Final save
        final_path = self.output_dir / "final_checkpoint"
        self.checkpointer.save(
            final_path,
            {'params': state['params'], 'step': self.config.num_steps}
        )
        
        logger.info("ðŸŽ‰ CPC Pretraining completed!")
        logger.info(f"Total training time: {total_time/3600:.2f} hours")
        logger.info(f"Final checkpoint saved to: {final_path}")
        
        if wandb.run:
            wandb.finish()


def main():
    """Command-line interface dla CPC pretraining."""
    parser = argparse.ArgumentParser(description="CPC Encoder Pretraining")
    parser.add_argument("--config", type=str, help="Path to config YAML file")
    parser.add_argument("--num_steps", type=int, default=100_000, help="Number of training steps")
    parser.add_argument("--batch_size", type=int, default=16, help="Batch size")
    parser.add_argument("--learning_rate", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--output_dir", type=str, default="experiments/cpc_pretraining", help="Output directory")
    
    args = parser.parse_args()
    
    # Load config
    if args.config and Path(args.config).exists():
        with open(args.config, 'r') as f:
            config_dict = yaml.safe_load(f)
        config = CPCTrainingConfig(**config_dict)
    else:
        config = CPCTrainingConfig()
        
    # Override z command line args
    if args.num_steps:
        config.num_steps = args.num_steps
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.learning_rate:
        config.learning_rate = args.learning_rate
    if args.output_dir:
        config.output_dir = args.output_dir
        
    # Initialize and run training
    pretrainer = CPCPretrainer(config)
    pretrainer.train()


if __name__ == "__main__":
    main()
```

## utils/__init__.py

```python
"""
Utilities for CPC+SNN Neuromorphic GW Detection

Production-ready utilities following ML4GW standards.
"""

import logging
import sys
from pathlib import Path
from typing import Optional, Union

import yaml
import jax
import jax.numpy as jnp


def setup_logging(
    level: int = logging.INFO,
    log_file: Optional[Union[str, Path]] = None,
    format_string: Optional[str] = None
) -> None:
    """Setup production-ready logging configuration.
    
    Args:
        level: Logging level (e.g., logging.INFO, logging.DEBUG)
        log_file: Optional file path for log output
        format_string: Custom log format string
    """
    if format_string is None:
        format_string = (
            "[%(asctime)s] %(levelname)s [%(name)s:%(lineno)d] %(message)s"
        )
    
    # Create formatter
    formatter = logging.Formatter(format_string)
    
    # Clear any existing handlers
    root_logger = logging.getLogger()
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(level)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # File handler (if specified)
    if log_file is not None:
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        file_handler = logging.FileHandler(log_path)
        file_handler.setLevel(level)
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)
    
    # Set root level
    root_logger.setLevel(level)
    
    # Set JAX logging level to reduce noise
    jax_logger = logging.getLogger("jax")
    jax_logger.setLevel(logging.WARNING)


def load_config(config_path: Union[str, Path]) -> dict:
    """Load YAML configuration file.
    
    Args:
        config_path: Path to YAML configuration file
        
    Returns:
        Configuration dictionary
        
    Raises:
        FileNotFoundError: If config file doesn't exist
        yaml.YAMLError: If config file is invalid
    """
    config_path = Path(config_path)
    
    if not config_path.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    return config or {}


def save_config(config: dict, output_path: Union[str, Path]) -> None:
    """Save configuration dictionary to YAML file.
    
    Args:
        config: Configuration dictionary to save
        output_path: Output file path
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, sort_keys=False)


def get_jax_device_info() -> dict:
    """Get JAX device information for logging and debugging.
    
    Returns:
        Dictionary with device information
    """
    devices = jax.devices()
    
    device_info = {
        'num_devices': len(devices),
        'devices': [
            {
                'id': i,
                'device_kind': str(device.device_kind),
                'platform': str(device.platform),
            }
            for i, device in enumerate(devices)
        ],
        'default_backend': jax.default_backend(),
    }
    
    # Try to get memory info (may not be available on all platforms)
    try:
        if devices and hasattr(devices[0], 'memory_stats'):
            memory_stats = devices[0].memory_stats()
            device_info['memory_info'] = memory_stats
    except Exception:
        pass
    
    return device_info


def print_system_info() -> None:
    """Print system and JAX configuration information."""
    logger = logging.getLogger(__name__)
    
    # JAX information
    device_info = get_jax_device_info()
    
    logger.info("ðŸ–¥ï¸  System Information:")
    logger.info(f"   JAX backend: {device_info['default_backend']}")
    logger.info(f"   Available devices: {device_info['num_devices']}")
    
    for device in device_info['devices']:
        logger.info(
            f"     Device {device['id']}: {device['device_kind']} "
            f"({device['platform']})"
        )
    
    if 'memory_info' in device_info:
        memory_info = device_info['memory_info'] 
        if 'bytes_in_use' in memory_info:
            memory_gb = memory_info['bytes_in_use'] / (1024**3)
            logger.info(f"   Memory in use: {memory_gb:.2f} GB")


def validate_array_shape(
    array: jnp.ndarray, 
    expected_shape: tuple,
    array_name: str = "array"
) -> None:
    """Validate array shape matches expected shape.
    
    Args:
        array: Input array to validate
        expected_shape: Expected shape tuple (use None for flexible dimensions)
        array_name: Name of array for error messages
        
    Raises:
        ValueError: If shape doesn't match
    """
    actual_shape = array.shape
    
    if len(actual_shape) != len(expected_shape):
        raise ValueError(
            f"{array_name} has {len(actual_shape)} dimensions, "
            f"expected {len(expected_shape)}"
        )
    
    for i, (actual, expected) in enumerate(zip(actual_shape, expected_shape)):
        if expected is not None and actual != expected:
            raise ValueError(
                f"{array_name} dimension {i} has size {actual}, "
                f"expected {expected}"
            )


def create_directory_structure(base_path: Union[str, Path], 
                             subdirs: list[str]) -> Path:
    """Create standardized directory structure for ML4GW projects.
    
    Args:
        base_path: Base directory path
        subdirs: List of subdirectory names to create
        
    Returns:
        Path to created base directory
    """
    base_path = Path(base_path)
    base_path.mkdir(parents=True, exist_ok=True)
    
    for subdir in subdirs:
        (base_path / subdir).mkdir(exist_ok=True)
    
    return base_path


# Standard ML4GW project structure
ML4GW_PROJECT_STRUCTURE = [
    "data",
    "models", 
    "logs",
    "outputs",
    "configs",
    "checkpoints",
    "plots",
    "results",
]


__all__ = [
    "setup_logging",
    "load_config", 
    "save_config",
    "get_jax_device_info",
    "print_system_info",
    "validate_array_shape",
    "create_directory_structure",
    "ML4GW_PROJECT_STRUCTURE",
]
```

## repomix-output.md

````markdown
This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-07-16 19:45:28

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
__init__.py
_version.py
cli.py
config.yaml
data
  __init__.py
  continuous_gw_generator.py
  data_loader.py
  gw_download.py
models
  __init__.py
  cpc_encoder.py
  simple_snn.py
  snn_classifier.py
  spike_bridge.py
training
  __init__.py
  advanced_training.py
  enhanced_gw_training.py
  pretrain_cpc.py
utils
  __init__.py
```

# Repository Files


## __init__.py

```python
"""
CPC+SNN Neuromorphic Gravitational Wave Detection

World's first neuromorphic gravitational wave detector using 
Contrastive Predictive Coding + Spiking Neural Networks.

Designed for production deployment following ML4GW standards.
"""

try:
    from ._version import __version__
except ImportError:
    __version__ = "0.1.0-dev"

# Core API exports
from .models.cpc_encoder import (
    CPCEncoder,
    create_enhanced_cpc_encoder,
    create_standard_cpc_encoder,
    info_nce_loss,
    enhanced_info_nce_loss,
)

from .models.simple_snn import (
    SimpleSNN,
    LIFLayer,
)

from .models.snn_classifier import (
    SNNClassifier,
    create_snn_classifier,
)

from .models.spike_bridge import (
    SpikeBridge,
    SpikeEncodingStrategy,
    create_default_spike_bridge,
    create_fast_spike_bridge,
    create_robust_spike_bridge,
)

from .data.gw_download import (
    ProductionGWOSCDownloader,
    AdvancedDataPreprocessor,
)

# Training utilities available at module level

__all__ = [
    "__version__",
    
    # Models
    "CPCEncoder", 
    "create_enhanced_cpc_encoder",
    "create_standard_cpc_encoder",
    "info_nce_loss",
    "enhanced_info_nce_loss",
    "SimpleSNN",
    "LIFLayer",
    "SNNClassifier", 
    "create_snn_classifier",
    "SpikeBridge",
    "SpikeEncodingStrategy",
    "create_default_spike_bridge",
    "create_fast_spike_bridge", 
    "create_robust_spike_bridge",
    
    # Data
    "ProductionGWOSCDownloader",
    "AdvancedDataPreprocessor",
]

# Package metadata
__title__ = "ligo-cpc-snn"
__description__ = "Neuromorphic gravitational wave detection using CPC + Spiking Neural Networks"
__author__ = "Gracjan"
__email__ = "contact@ml4gw-neuromorphic.org"
__license__ = "MIT"
__copyright__ = "Copyright 2025 CPC+SNN Neuromorphic GW Detection Project"
```

## _version.py

```python
# file generated by setuptools-scm
# don't change, don't track in version control

__all__ = ["__version__", "__version_tuple__", "version", "version_tuple"]

TYPE_CHECKING = False
if TYPE_CHECKING:
    from typing import Tuple
    from typing import Union

    VERSION_TUPLE = Tuple[Union[int, str], ...]
else:
    VERSION_TUPLE = object

version: str
__version__: str
__version_tuple__: VERSION_TUPLE
version_tuple: VERSION_TUPLE

__version__ = version = '0.1.dev1+ge22771a.d20250628'
__version_tuple__ = version_tuple = (0, 1, 'dev1', 'ge22771a.d20250628')
```

## cli.py

```python
#!/usr/bin/env python3
"""
ML4GW-compatible CLI interface for CPC+SNN Neuromorphic GW Detection

Production-ready command line interface following ML4GW standards.
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import Optional

import yaml

from . import __version__
from .utils import setup_logging

# Optional imports (will be loaded when needed)
try:
    from .training.pretrain_cpc import main as cpc_train_main
except ImportError:
    cpc_train_main = None
    
try:
    from .models.cpc_encoder import create_enhanced_cpc_encoder
except ImportError:
    create_enhanced_cpc_encoder = None

logger = logging.getLogger(__name__)


def get_base_parser() -> argparse.ArgumentParser:
    """Create base argument parser with common options."""
    parser = argparse.ArgumentParser(
        description="CPC+SNN Neuromorphic Gravitational Wave Detection",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "--version", 
        action="version", 
        version=f"ligo-cpc-snn {__version__}"
    )
    
    parser.add_argument(
        "--config", 
        type=Path,
        help="Configuration file path"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="count", 
        default=0,
        help="Increase verbosity level"
    )
    
    parser.add_argument(
        "--log-file",
        type=Path,
        help="Log file path"
    )
    
    return parser


def train_cmd():
    """Main training command entry point."""
    parser = get_base_parser()
    parser.description = "Train CPC+SNN neuromorphic gravitational wave detector"
    
    # Training specific arguments
    parser.add_argument(
        "--output-dir", "-o",
        type=Path,
        default=Path("./outputs"),
        help="Output directory for training artifacts"
    )
    
    parser.add_argument(
        "--data-dir",
        type=Path, 
        default=Path("./data"),
        help="Data directory"
    )
    
    parser.add_argument(
        "--epochs",
        type=int,
        default=100,
        help="Number of training epochs"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=32,
        help="Training batch size"
    )
    
    parser.add_argument(
        "--learning-rate", "--lr",
        type=float,
        default=1e-3,
        help="Learning rate"
    )
    
    parser.add_argument(
        "--gpu",
        action="store_true",
        help="Use GPU acceleration"
    )
    
    parser.add_argument(
        "--wandb",
        action="store_true", 
        help="Enable Weights & Biases logging"
    )
    
    parser.add_argument(
        "--checkpoint",
        type=Path,
        help="Resume from checkpoint"
    )
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(
        level=logging.INFO if args.verbose == 0 else logging.DEBUG,
        log_file=args.log_file
    )
    
    logger.info(f"ðŸš€ Starting CPC+SNN training (v{__version__})")
    logger.info(f"   Output directory: {args.output_dir}")
    logger.info(f"   Configuration: {args.config or 'default'}")
    
    # Load configuration
    config = {}
    if args.config and args.config.exists():
        with open(args.config) as f:
            config = yaml.safe_load(f)
        logger.info(f"âœ… Loaded configuration from {args.config}")
    
    # Override config with CLI arguments
    config.update({
        'output_dir': str(args.output_dir),
        'data_dir': str(args.data_dir),
        'epochs': args.epochs,
        'batch_size': args.batch_size, 
        'learning_rate': args.learning_rate,
        'use_gpu': args.gpu,
        'use_wandb': args.wandb,
        'checkpoint_path': str(args.checkpoint) if args.checkpoint else None,
    })
    
    # Create output directory
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # Save final configuration
    config_path = args.output_dir / "config.yaml"
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    logger.info(f"ðŸ’¾ Saved configuration to {config_path}")
    
    try:
        # Run training
        success = cpc_train_main(config)
        
        if success:
            logger.info("ðŸŽ‰ Training completed successfully!")
            return 0
        else:
            logger.error("âŒ Training failed")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ Training failed with error: {e}")
        import traceback
        traceback.print_exc()
        return 1


def eval_cmd():
    """Main evaluation command entry point."""
    parser = get_base_parser()
    parser.description = "Evaluate CPC+SNN neuromorphic gravitational wave detector"
    
    # Evaluation specific arguments
    parser.add_argument(
        "--model-path", "-m",
        type=Path,
        required=True,
        help="Path to trained model"
    )
    
    parser.add_argument(
        "--test-data",
        type=Path,
        help="Test data directory or file"
    )
    
    parser.add_argument(
        "--output-dir", "-o", 
        type=Path,
        default=Path("./evaluation"),
        help="Output directory for evaluation results"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=64,
        help="Evaluation batch size"
    )
    
    parser.add_argument(
        "--save-predictions",
        action="store_true",
        help="Save model predictions"
    )
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(
        level=logging.INFO if args.verbose == 0 else logging.DEBUG,
        log_file=args.log_file
    )
    
    logger.info(f"ðŸ” Starting CPC+SNN evaluation (v{__version__})")
    logger.info(f"   Model: {args.model_path}")
    logger.info(f"   Output: {args.output_dir}")
    
    # Create output directory
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # TODO: Implement evaluation logic
    logger.info("âš ï¸ Evaluation implementation coming soon...")
    return 0


def infer_cmd():
    """Main inference command entry point."""
    parser = get_base_parser()
    parser.description = "Run inference with CPC+SNN neuromorphic gravitational wave detector"
    
    # Inference specific arguments
    parser.add_argument(
        "--model-path", "-m",
        type=Path,
        required=True,
        help="Path to trained model"
    )
    
    parser.add_argument(
        "--input-data",
        type=Path,
        required=True,
        help="Input data file or directory"
    )
    
    parser.add_argument(
        "--output-dir", "-o",
        type=Path, 
        default=Path("./inference"),
        help="Output directory for inference results"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1,
        help="Inference batch size"
    )
    
    parser.add_argument(
        "--real-time",
        action="store_true",
        help="Enable real-time inference mode"
    )
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(
        level=logging.INFO if args.verbose == 0 else logging.DEBUG,
        log_file=args.log_file
    )
    
    logger.info(f"âš¡ Starting CPC+SNN inference (v{__version__})")
    logger.info(f"   Model: {args.model_path}")
    logger.info(f"   Input: {args.input_data}")
    logger.info(f"   Output: {args.output_dir}")
    
    # Create output directory
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # TODO: Implement inference logic
    logger.info("âš ï¸ Inference implementation coming soon...")
    return 0


if __name__ == "__main__":
    sys.exit(train_cmd())
```

## config.yaml

```yaml
# LIGO CPC+SNN Configuration
# Neuromorphic Gravitational Wave Detection Pipeline

# Environment & Platform
platform:
  device: "metal"  # metal, cpu, gpu
  precision: "float32"
  enable_x64: false

# Data Configuration  
data:
  sample_rate: 4096  # Hz
  segment_duration: 4.0  # seconds
  detectors: ["H1", "L1"]  # LIGO Hanford, Livingston
  preprocessing:
    whitening: true
    bandpass: [20.0, 1024.0]  # Hz range
    qtransform: false

# CPC Encoder Configuration
cpc:
  latent_dim: 256
  downsample_factor: 16
  context_length: 12  # timesteps for prediction
  num_negatives: 128  # for InfoNCE loss
  temperature: 0.1
  architecture:
    conv_channels: [32, 64, 128]
    kernel_size: 9
    stride: 2
    gru_hidden: 256

# Spike Bridge Configuration  
spike_bridge:
  encoding: "poisson"  # poisson, temporal_contrast
  spike_rate_max: 100.0  # Hz
  dt: 0.001  # 1ms timesteps
  threshold: 0.1  # for temporal contrast

# SNN Classifier Configuration
snn:
  hidden_size: 128
  num_layers: 2
  neuron_type: "LIF"  # LIF, ALIF, IF
  tau_mem: 0.020  # 20ms membrane time constant
  tau_syn: 0.005  # 5ms synaptic time constant  
  threshold: 1.0
  num_classes: 2  # GW / No-GW

# Training Configuration
training:
  # Phase 1: CPC Pretraining
  cpc_pretrain:
    steps: 100000
    batch_size: 16
    learning_rate: 0.003
    optimizer: "adam"
    grad_clip: 1.0
    
  # Phase 2: SNN Training (frozen CPC)
  snn_train:
    steps: 10000  
    batch_size: 16
    learning_rate: 0.001
    optimizer: "adam"
    grad_clip: 0.5
    
  # Phase 3: Joint Fine-tuning
  joint_finetune:
    steps: 5000
    batch_size: 8  # smaller for memory
    learning_rate: 0.0001
    optimizer: "adam"
    grad_clip: 0.1

# Evaluation Metrics
evaluation:
  metrics: ["roc_auc", "accuracy", "precision", "recall"]
  target_far: 0.033  # 1/30 days false alarm rate
  target_tpr: 0.95   # 95% true positive rate

# Logging & Monitoring
logging:
  level: "INFO"
  log_dir: "logs/"
  wandb_project: "ligo-cpc-snn"
  save_checkpoints: true
  checkpoint_dir: "checkpoints/"
```

## data/__init__.py

```python
"""
Data Module: GWOSC Gravitational Wave Data Pipeline

Handles downloading, preprocessing, and quality validation of
LIGO gravitational wave data from GWOSC (Gravitational Wave Open Science Center).
"""

from .gw_download import (
    GWOSCDownloader, 
    DataPreprocessor,
    ProductionGWOSCDownloader,
    AdvancedDataPreprocessor,
    QualityMetrics,
    ProcessingResult
)

__all__ = [
    "GWOSCDownloader",
    "DataPreprocessor", 
    "ProductionGWOSCDownloader",
    "AdvancedDataPreprocessor",
    "QualityMetrics",
    "ProcessingResult",
]
```

## data/continuous_gw_generator.py

```python
#!/usr/bin/env python3

"""
Continuous Gravitational Wave Signal Generation

Integrates PyFstat F-statistic based continuous GW generation 
with our CPC+SNN neuromorphic detection pipeline.

Complements existing GWOSC binary detection capabilities.
"""

import numpy as np
import jax
import jax.numpy as jnp
import logging
from typing import Tuple, Dict, Optional, List
from dataclasses import dataclass
from pathlib import Path

try:
    import pyfstat
    PYFSTAT_AVAILABLE = True
except ImportError:
    PYFSTAT_AVAILABLE = False
    pyfstat = None

logger = logging.getLogger(__name__)


@dataclass
class ContinuousGWParams:
    """Parameters for continuous gravitational wave signals."""
    frequency: float  # Signal frequency (Hz)
    frequency_dot: float = 0.0  # Frequency derivative (Hz/s)
    alpha: float = 0.0  # Right ascension (rad)
    delta: float = 0.0  # Declination (rad)  
    amplitude_h0: float = 1e-24  # GW amplitude
    cosi: float = 0.0  # Cosine of inclination angle
    psi: float = 0.0  # Polarization angle (rad)
    phi0: float = 0.0  # Initial phase (rad)


@dataclass 
class SignalConfiguration:
    """Configuration for signal generation."""
    t_start: int  # GPS start time
    t_end: int  # GPS end time
    sampling_rate: int = 4096  # Hz
    detectors: List[str] = None  # Detector names
    noise_sqrt_sh: float = 1e-23  # Noise level
    
    def __post_init__(self):
        if self.detectors is None:
            self.detectors = ['H1', 'L1']  # LIGO Hanford + Livingston


class ContinuousGWGenerator:
    """
    Continuous Gravitational Wave Signal Generator.
    
    Uses PyFstat for F-statistic based signal generation
    compatible with our neuromorphic CPC+SNN pipeline.
    """
    
    def __init__(self, 
                 base_frequency: float = 50.0,
                 freq_range: Tuple[float, float] = (20.0, 200.0),
                 duration: float = 1000.0,  # seconds
                 output_dir: Optional[str] = None):
        """
        Initialize continuous GW signal generator.
        
        Args:
            base_frequency: Base signal frequency (Hz)
            freq_range: Frequency range for signals (Hz)
            duration: Signal duration (seconds)
            output_dir: Directory for signal cache
        """
        if not PYFSTAT_AVAILABLE:
            raise ImportError(
                "PyFstat not available. Install with: pip install pyfstat"
            )
            
        self.base_frequency = base_frequency
        self.freq_range = freq_range
        self.duration = duration
        self.output_dir = Path(output_dir) if output_dir else Path("./continuous_gw_cache")
        self.output_dir.mkdir(exist_ok=True)
        
        # Signal parameters
        self.sampling_rate = 4096  # Hz (compatible with GWOSC)
        self.detectors = ['H1', 'L1']
        
        logger.info(f"Initialized Continuous GW Generator")
        logger.info(f"  Base frequency: {base_frequency} Hz")
        logger.info(f"  Frequency range: {freq_range} Hz")
        logger.info(f"  Duration: {duration} s")
        logger.info(f"  Output directory: {self.output_dir}")
        
    def generate_signal_parameters(self, 
                                 num_signals: int = 10,
                                 seed: int = 42) -> List[ContinuousGWParams]:
        """
        Generate random continuous GW signal parameters.
        
        Args:
            num_signals: Number of signals to generate
            seed: Random seed for reproducibility
            
        Returns:
            List of ContinuousGWParams objects
        """
        np.random.seed(seed)
        
        params_list = []
        
        for i in range(num_signals):
            # Frequency: uniform in range
            freq = np.random.uniform(self.freq_range[0], self.freq_range[1])
            
            # Frequency derivative: small values typical for pulsars
            freq_dot = np.random.uniform(-1e-8, 1e-8)  # Hz/s
            
            # Sky position: uniform on sphere
            alpha = np.random.uniform(0, 2*np.pi)  # Right ascension
            cos_delta = np.random.uniform(-1, 1)  
            delta = np.arccos(cos_delta) - np.pi/2  # Declination
            
            # Amplitude: log-uniform distribution
            h0 = np.random.uniform(1e-26, 1e-22)
            
            # Orientation parameters
            cosi = np.random.uniform(-1, 1)  # Cosine inclination
            psi = np.random.uniform(0, np.pi)  # Polarization angle
            phi0 = np.random.uniform(0, 2*np.pi)  # Initial phase
            
            params = ContinuousGWParams(
                frequency=freq,
                frequency_dot=freq_dot,
                alpha=alpha,
                delta=delta,
                amplitude_h0=h0,
                cosi=cosi,
                psi=psi,
                phi0=phi0
            )
            
            params_list.append(params)
            
        logger.info(f"Generated {num_signals} continuous GW parameter sets")
        return params_list
    
    def create_synthetic_timeseries(self, 
                                  params: ContinuousGWParams,
                                  duration: float = 4.0,
                                  sampling_rate: int = 4096,
                                  noise_level: float = 1e-23) -> jnp.ndarray:
        """
        Create synthetic continuous GW timeseries (simplified approach).
        
        Args:
            params: Signal parameters
            duration: Signal duration (seconds)
            sampling_rate: Sampling rate (Hz)
            noise_level: Noise amplitude
            
        Returns:
            Timeseries data array
        """
        # Time array
        t = jnp.arange(0, duration, 1/sampling_rate)
        
        # Continuous GW signal model (plus polarization)
        omega = 2 * jnp.pi * params.frequency
        
        # Plus polarization
        h_plus = (
            params.amplitude_h0 * 
            (1 + params.cosi**2) / 2 * 
            jnp.cos(omega * t + params.phi0)
        )
        
        # Cross polarization  
        h_cross = (
            params.amplitude_h0 * 
            params.cosi * 
            jnp.sin(omega * t + params.phi0)
        )
        
        # Detector response (simplified)
        signal = (
            h_plus * jnp.cos(2 * params.psi) + 
            h_cross * jnp.sin(2 * params.psi)
        )
        
        # Add realistic Gaussian noise
        key = jax.random.PRNGKey(42)
        noise = jax.random.normal(key, shape=signal.shape) * noise_level
        
        return signal + noise
    
    def generate_training_dataset(self, 
                                num_signals: int = 100,
                                signal_duration: float = 4.0,  # seconds
                                include_noise_only: bool = True) -> Dict:
        """
        Generate training dataset with continuous GW signals + noise.
        
        Args:
            num_signals: Number of signals to generate
            signal_duration: Duration of each signal (seconds)
            include_noise_only: Whether to include noise-only samples
            
        Returns:
            Dictionary with training data and labels
        """
        logger.info(f"Generating continuous GW training dataset ({num_signals} signals)")
        
        # Generate signal parameters
        params_list = self.generate_signal_parameters(num_signals)
        
        all_data = []
        all_labels = []
        all_metadata = []
        
        # Generate signal + noise samples  
        for i, params in enumerate(params_list):
            signal_data = self.create_synthetic_timeseries(
                params, 
                duration=signal_duration,
                sampling_rate=self.sampling_rate,
                noise_level=1e-23
            )
            
            all_data.append(signal_data)
            all_labels.append(1)  # Signal present
            all_metadata.append({
                'signal_type': 'continuous_gw',
                'frequency': params.frequency,
                'amplitude': params.amplitude_h0,
                'detector': 'H1'
            })
            
        # Generate noise-only samples if requested
        if include_noise_only:
            import jax
            key = jax.random.PRNGKey(123)
            
            for i in range(num_signals):
                # Pure noise
                noise_length = int(signal_duration * self.sampling_rate)
                noise_data = jax.random.normal(key, (noise_length,)) * 1e-23
                key, _ = jax.random.split(key)
                
                all_data.append(noise_data)
                all_labels.append(0)  # No signal
                all_metadata.append({
                    'signal_type': 'noise_only',
                    'frequency': None,
                    'amplitude': None,
                    'detector': 'H1'
                })
        
        # Convert to arrays
        data_array = jnp.stack(all_data)
        labels_array = jnp.array(all_labels)
        
        dataset = {
            'data': data_array,
            'labels': labels_array,
            'metadata': all_metadata,
            'signal_type': 'continuous_gw',
            'sampling_rate': self.sampling_rate,
            'duration': signal_duration
        }
        
        logger.info(f"Generated continuous GW dataset:")
        logger.info(f"  Data shape: {data_array.shape}")
        logger.info(f"  Labels: {jnp.sum(labels_array)} signals, {len(labels_array) - jnp.sum(labels_array)} noise")
        logger.info(f"  Signal duration: {signal_duration} s")
        
        return dataset
    
    def compute_signal_statistics(self, dataset: Dict) -> Dict:
        """
        Compute statistics for continuous GW signals.
        
        Args:
            dataset: Dataset from generate_training_dataset
            
        Returns:
            Dictionary with signal statistics
        """
        data = dataset['data']
        labels = dataset['labels']
        metadata = dataset['metadata']
        
        signal_indices = jnp.where(labels == 1)[0]
        noise_indices = jnp.where(labels == 0)[0]
        
        # Signal power statistics
        signal_data = data[signal_indices]
        noise_data = data[noise_indices]
        
        signal_power = jnp.mean(signal_data**2, axis=1)
        noise_power = jnp.mean(noise_data**2, axis=1)
        
        # Frequency statistics
        frequencies = [m['frequency'] for m in metadata if m['frequency'] is not None]
        amplitudes = [m['amplitude'] for m in metadata if m['amplitude'] is not None]
        
        stats = {
            'num_signals': len(signal_indices),
            'num_noise': len(noise_indices),
            'signal_power_mean': float(jnp.mean(signal_power)),
            'signal_power_std': float(jnp.std(signal_power)),
            'noise_power_mean': float(jnp.mean(noise_power)),
            'noise_power_std': float(jnp.std(noise_power)),
            'snr_estimate': float(jnp.mean(signal_power) / jnp.mean(noise_power)),
            'frequency_range': (min(frequencies), max(frequencies)) if frequencies else None,
            'amplitude_range': (min(amplitudes), max(amplitudes)) if amplitudes else None,
            'sampling_rate': dataset['sampling_rate'],
            'duration': dataset['duration']
        }
        
        logger.info("Continuous GW Signal Statistics:")
        logger.info(f"  Signals: {stats['num_signals']}, Noise: {stats['num_noise']}")
        logger.info(f"  SNR estimate: {stats['snr_estimate']:.3f}")
        logger.info(f"  Frequency range: {stats['frequency_range']} Hz")
        logger.info(f"  Amplitude range: {stats['amplitude_range']}")
        
        return stats


def create_mixed_gw_dataset(continuous_generator: ContinuousGWGenerator,
                          binary_data: Optional[Dict] = None,
                          mix_ratio: float = 0.5) -> Dict:
    """
    Create mixed dataset with continuous + binary GW signals.
    
    Args:
        continuous_generator: Continuous GW generator instance
        binary_data: Existing binary GW dataset (from GWOSC)
        mix_ratio: Ratio of continuous to binary signals
        
    Returns:
        Mixed dataset for enhanced CPC+SNN training
    """
    logger.info(f"Creating mixed GW dataset (mix_ratio={mix_ratio})")
    
    # Generate continuous signals
    num_continuous = int(100 * mix_ratio)
    num_binary = int(100 * (1 - mix_ratio))
    
    continuous_dataset = continuous_generator.generate_training_dataset(
        num_signals=num_continuous,
        signal_duration=4.0
    )
    
    if binary_data is not None:
        # Combine with existing binary data
        logger.info(f"Combining {num_continuous} continuous + {num_binary} binary signals")
        
        # For now, return continuous dataset
        # TODO: Implement proper mixing logic
        mixed_dataset = continuous_dataset
        mixed_dataset['signal_types'] = ['continuous_gw', 'binary_merger']
        
    else:
        logger.info("No binary data provided, returning continuous-only dataset")
        mixed_dataset = continuous_dataset
        mixed_dataset['signal_types'] = ['continuous_gw']
    
    return mixed_dataset


# Test function
def test_continuous_gw_generator():
    """Test continuous GW signal generation."""
    print("ðŸ”¬ Testing Continuous GW Generator...")
    
    if not PYFSTAT_AVAILABLE:
        print("âŒ PyFstat not available - skipping test")
        return False
    
    try:
        # Create generator
        generator = ContinuousGWGenerator(
            base_frequency=50.0,
            freq_range=(40.0, 60.0),
            duration=4.0
        )
        
        # Generate small test dataset
        dataset = generator.generate_training_dataset(
            num_signals=5,
            signal_duration=4.0
        )
        
        # Compute statistics
        stats = generator.compute_signal_statistics(dataset)
        
        print(f"âœ… Generated test dataset: {dataset['data'].shape}")
        print(f"âœ… SNR estimate: {stats['snr_estimate']:.3f}")
        print(f"âœ… Frequency range: {stats['frequency_range']} Hz")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        return False


if __name__ == "__main__":
    # Run test
    success = test_continuous_gw_generator()
    exit(0 if success else 1)
```

## data/data_loader.py

```python

```

## data/gw_download.py

```python
"""
GWOSC Data Downloader and Preprocessor

Production-ready pipeline for downloading and preprocessing gravitational wave 
strain data from LIGO/Virgo detectors via GWOSC API. Optimized for Apple Silicon
with comprehensive quality validation and batch processing.
"""

import jax
import jax.numpy as jnp
import jax.scipy as jsp
from gwpy.timeseries import TimeSeries
from gwpy.frequencyseries import FrequencySeries
from typing import List, Tuple, Optional, Dict, Any
import logging
import time
import tempfile
import hashlib
from pathlib import Path
from abc import ABC, abstractmethod
from dataclasses import dataclass
import numpy as np

logger = logging.getLogger(__name__)


@dataclass
class QualityMetrics:
    """Quality assessment metrics for strain data."""
    is_valid: bool
    snr_estimate: float
    glitch_probability: float
    spectral_line_contamination: float
    data_completeness: float
    outlier_fraction: float


@dataclass
class ProcessingResult:
    """Result of data processing with quality metrics."""
    strain_data: jnp.ndarray
    psd: Optional[jnp.ndarray]
    quality: QualityMetrics
    processing_time: float
    metadata: Dict[str, Any]


class DataSource(ABC):
    """Abstract interface for gravitational wave data sources."""
    
    @abstractmethod
    def fetch(self, detector: str, start_time: int, duration: float) -> jnp.ndarray:
        """Fetch strain data for specified detector and time range."""
        pass


class ProductionGWOSCDownloader(DataSource):
    """
    Production-ready GWOSC downloader with intelligent caching, 
    quality validation, and batch processing capabilities.
    
    Optimized for Data Pipeline Phase with <1s per 4s segment target.
    """
    
    def __init__(self, 
                 cache_dir: Optional[str] = None, 
                 max_retries: int = 5,
                 retry_delay: float = 2.0,
                 max_cache_size_gb: float = 5.0):
        self.cache_dir = Path(cache_dir) if cache_dir else Path(tempfile.gettempdir()) / "ligo_cpc_cache"
        self.cache_dir.mkdir(exist_ok=True)
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.max_cache_size = max_cache_size_gb * 1e9
        
        logger.info(f"Initialized GWOSC downloader with cache: {self.cache_dir}")
        
    def cache_key(self, detector: str, start_time: int, duration: float) -> str:
        """Generate unique cache key for data segment."""
        key_string = f"{detector}_{start_time}_{duration}"
        return hashlib.sha256(key_string.encode()).hexdigest()[:16]
        
    def cleanup_cache(self):
        """Remove oldest cached files if cache size exceeds limit."""
        cache_files = list(self.cache_dir.glob("*.npy"))
        total_size = sum(f.stat().st_size for f in cache_files)
        
        if total_size > self.max_cache_size:
            # Sort by modification time and remove oldest 50%
            cache_files.sort(key=lambda f: f.stat().st_mtime)
            for f in cache_files[:len(cache_files)//2]:
                f.unlink()
                logger.debug(f"Removed cached file: {f.name}")
                
    def fetch(self, detector: str, start_time: int, duration: float) -> jnp.ndarray:
        """
        Fetch strain data from GWOSC with intelligent caching.
        
        Args:
            detector: Detector name (e.g., 'H1', 'L1', 'V1')
            start_time: GPS start time
            duration: Duration in seconds
            
        Returns:
            JAX array of strain data
        """
        # Check cache first
        cache_key = self.cache_key(detector, start_time, duration)
        cache_file = self.cache_dir / f"{cache_key}.npy"
        
        if cache_file.exists():
            logger.debug(f"Loading from cache: {cache_key}")
            # CPU fallback dla cached data loading
            with jax.default_device(jax.devices('cpu')[0]):
                return jnp.array(np.load(cache_file))
            
        # Download with exponential backoff retry
        end_time = start_time + duration
        
        for attempt in range(self.max_retries):
            try:
                start_fetch = time.perf_counter()
                logger.info(f"Fetching {detector} data: {start_time}-{end_time} (attempt {attempt+1})")
                
                # Use GWpy to fetch open data
                timeseries = TimeSeries.fetch_open_data(
                    detector, start_time, end_time,
                    verbose=False  # Reduce output noise
                )
                
                # Convert to JAX array with CPU fallback dla Metal backend compatibility
                with jax.default_device(jax.devices('cpu')[0]):
                    strain_data = jnp.array(timeseries.value)
                fetch_time = time.perf_counter() - start_fetch
                
                # Cache the result
                np.save(cache_file, np.array(strain_data))
                self.cleanup_cache()
                
                logger.info(f"Fetched {len(strain_data)} samples in {fetch_time:.2f}s")
                return strain_data
                
            except Exception as e:
                wait_time = self.retry_delay * (2 ** attempt)  # Exponential backoff
                logger.warning(f"Attempt {attempt+1} failed: {e}")
                
                if attempt < self.max_retries - 1:
                    logger.info(f"Retrying in {wait_time:.1f}s...")
                    time.sleep(wait_time)
                else:
                    raise RuntimeError(f"Failed to fetch data after {self.max_retries} attempts: {e}")
                    
    def fetch_batch(self, 
                   segments: List[Tuple[str, int, float]], 
                   max_workers: int = 4) -> List[jnp.ndarray]:
        """
        Fetch multiple data segments in batch with parallel processing.
        
        Args:
            segments: List of (detector, start_time, duration) tuples
            max_workers: Maximum parallel download workers
            
        Returns:
            List of strain data arrays
        """
        import concurrent.futures
        
        logger.info(f"Batch fetching {len(segments)} segments with {max_workers} workers")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(self.fetch, detector, start_time, duration)
                for detector, start_time, duration in segments
            ]
            
            results = []
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                try:
                    data = future.result()
                    results.append(data)
                    logger.info(f"Completed segment {i+1}/{len(segments)}")
                except Exception as e:
                    logger.error(f"Failed to fetch segment {i+1}: {e}")
                    results.append(None)
                    
        return results


class AdvancedDataPreprocessor:
    """
    Advanced preprocessing pipeline with quality validation optimized for Apple Silicon.
    
    Implements whitening, band-pass filtering, glitch detection, and spectral analysis.
    Target: <1s processing time per 4s segment.
    """
    
    def __init__(self, 
                 sample_rate: int = 4096,
                 bandpass: Tuple[float, float] = (20.0, 1024.0),
                 apply_whitening: bool = True,
                 psd_length: int = 8,  # seconds for PSD estimation
                 quality_threshold: float = 0.7):
        self.sample_rate = sample_rate
        self.bandpass = bandpass
        self.apply_whitening = apply_whitening
        self.psd_length = psd_length
        self.quality_threshold = quality_threshold
        
        # Pre-compute filter coefficients for efficiency
        self._setup_filters()
        
    def _setup_filters(self):
        """Pre-compute filter coefficients for band-pass filtering."""
        from scipy.signal import butter
        
        nyquist = self.sample_rate / 2
        low = self.bandpass[0] / nyquist
        high = self.bandpass[1] / nyquist
        
        # Design 8th-order Butterworth filter
        self.filter_sos = butter(8, [low, high], btype='band', output='sos')
        logger.info(f"Initialized band-pass filter: {self.bandpass[0]}-{self.bandpass[1]} Hz")
        
    def _bandpass_filter_jax(self, data: jnp.ndarray) -> jnp.ndarray:
        """JAX-optimized band-pass filtering using scipy.signal compatibility."""
        # Simple FIR filter implementation for JAX compatibility
        # TODO: Replace with more sophisticated JAX-native filter
        freqs = jnp.fft.fftfreq(len(data), 1/self.sample_rate)
        fft_data = jnp.fft.fft(data)
        
        # Create frequency mask
        mask = (jnp.abs(freqs) >= self.bandpass[0]) & (jnp.abs(freqs) <= self.bandpass[1])
        filtered_fft = fft_data * mask
        
        return jnp.real(jnp.fft.ifft(filtered_fft))
        
    def estimate_psd(self, strain_data: jnp.ndarray) -> jnp.ndarray:
        """
        Estimate Power Spectral Density using Welch's method.
        
        Args:
            strain_data: Input strain timeseries
            
        Returns:
            Power spectral density
        """
        # Use scipy for PSD estimation (more reliable than JAX implementation)
        from scipy.signal import welch
        
        nperseg = self.psd_length * self.sample_rate
        freqs, psd = welch(
            np.array(strain_data), 
            fs=self.sample_rate,
            nperseg=min(nperseg, len(strain_data)//4)
        )
        
        # CPU fallback dla PSD array conversion
        with jax.default_device(jax.devices('cpu')[0]):
            return jnp.array(psd)
        
    def _whiten_data(self, strain_data: jnp.ndarray, psd: jnp.ndarray) -> jnp.ndarray:
        """
        Whiten strain data using estimated PSD.
        
        Args:
            strain_data: Input strain timeseries
            psd: Power spectral density
            
        Returns:
            Whitened strain data
        """
        # FFT of strain data
        strain_fft = jnp.fft.fft(strain_data)
        freqs = jnp.fft.fftfreq(len(strain_data), 1/self.sample_rate)
        
        # Interpolate PSD to match FFT frequencies
        psd_interp = jnp.interp(jnp.abs(freqs), 
                               jnp.linspace(0, self.sample_rate/2, len(psd)), 
                               psd)
        
        # Avoid division by zero
        psd_interp = jnp.where(psd_interp < 1e-40, 1e-40, psd_interp)
        
        # Whiten in frequency domain
        whitened_fft = strain_fft / jnp.sqrt(psd_interp)
        
        # Return to time domain
        return jnp.real(jnp.fft.ifft(whitened_fft))
        
    def assess_quality(self, strain_data: jnp.ndarray, psd: Optional[jnp.ndarray] = None) -> QualityMetrics:
        """
        Comprehensive quality assessment of strain data.
        
        Args:
            strain_data: Input strain timeseries
            psd: Optional pre-computed PSD
            
        Returns:
            Quality metrics object
        """
        # Basic validity checks
        has_nan = jnp.any(jnp.isnan(strain_data))
        has_inf = jnp.any(jnp.isinf(strain_data))
        is_valid = not (has_nan or has_inf)
        
        if not is_valid:
            return QualityMetrics(
                is_valid=False, snr_estimate=0.0, glitch_probability=1.0,
                spectral_line_contamination=1.0, data_completeness=0.0, outlier_fraction=1.0
            )
            
        # Data completeness (non-zero values)
        data_completeness = float(jnp.mean(strain_data != 0.0))
        
        # Outlier detection (values > 6 sigma)
        std_dev = jnp.std(strain_data)
        outliers = jnp.abs(strain_data) > 6 * std_dev
        outlier_fraction = float(jnp.mean(outliers))
        
        # SNR estimate (rough approximation)
        signal_power = jnp.var(strain_data)
        snr_estimate = float(jnp.sqrt(signal_power) * len(strain_data)**0.5)
        
        # Glitch probability (based on kurtosis and outliers)
        kurtosis = float(jsp.stats.kurtosis(strain_data))
        glitch_probability = float(jnp.clip((kurtosis - 3) / 10 + outlier_fraction, 0, 1))
        
        # Spectral line contamination (detect strong narrowband features)
        if psd is not None:
            # Look for peaks significantly above median
            psd_median = jnp.median(psd)
            peak_ratio = jnp.max(psd) / psd_median
            spectral_line_contamination = float(jnp.clip((peak_ratio - 100) / 1000, 0, 1))
        else:
            spectral_line_contamination = 0.0
            
        return QualityMetrics(
            is_valid=is_valid,
            snr_estimate=snr_estimate,
            glitch_probability=glitch_probability,
            spectral_line_contamination=spectral_line_contamination,
            data_completeness=data_completeness,
            outlier_fraction=outlier_fraction
        )
        
    def process(self, strain_data: jnp.ndarray) -> ProcessingResult:
        """
        Complete preprocessing pipeline with quality assessment.
        
        Args:
            strain_data: Raw strain timeseries
            
        Returns:
            Processing result with quality metrics
        """
        start_time = time.perf_counter()
        
        logger.debug(f"Processing {len(strain_data)} samples ({len(strain_data)/self.sample_rate:.1f}s)")
        
        # 1. Band-pass filtering
        filtered_data = self._bandpass_filter_jax(strain_data)
        
        # 2. PSD estimation for whitening
        psd = None
        if self.apply_whitening:
            psd = self.estimate_psd(filtered_data)
            processed_data = self._whiten_data(filtered_data, psd)
        else:
            processed_data = filtered_data
            
        # 3. Normalization (zero mean, unit variance) with CPU fallback
        with jax.default_device(jax.devices('cpu')[0]):
            processed_data = (processed_data - jnp.mean(processed_data)) / jnp.std(processed_data)
        
        # 4. Quality assessment
        quality = self.assess_quality(processed_data, psd)
        
        processing_time = time.perf_counter() - start_time
        
        # 5. Metadata collection
        metadata = {
            'original_length': len(strain_data),
            'processed_length': len(processed_data),
            'sample_rate': self.sample_rate,
            'bandpass': self.bandpass,
            'whitening_applied': self.apply_whitening,
            'processing_time_ms': processing_time * 1000
        }
        
        logger.debug(f"Processing completed in {processing_time*1000:.1f}ms "
                    f"(quality: {quality.snr_estimate:.1f} SNR)")
        
        return ProcessingResult(
            strain_data=processed_data,
            psd=psd,
            quality=quality,
            processing_time=processing_time,
            metadata=metadata
        )
        
    def process_batch(self, strain_segments: List[jnp.ndarray]) -> List[ProcessingResult]:
        """
        Process multiple strain segments in batch for efficiency.
        
        Args:
            strain_segments: List of strain data arrays
            
        Returns:
            List of processing results
        """
        logger.info(f"Batch processing {len(strain_segments)} segments")
        
        results = []
        total_start = time.perf_counter()
        
        for i, segment in enumerate(strain_segments):
            if segment is not None:
                result = self.process(segment)
                results.append(result)
                
                # Log progress
                if (i + 1) % 10 == 0:
                    elapsed = time.perf_counter() - total_start
                    avg_time = elapsed / (i + 1) * 1000
                    logger.info(f"Processed {i+1}/{len(strain_segments)} segments "
                               f"(avg: {avg_time:.1f}ms per segment)")
            else:
                results.append(None)
                
        total_time = time.perf_counter() - total_start
        valid_results = [r for r in results if r is not None]
        
        logger.info(f"Batch processing completed in {total_time:.2f}s "
                   f"({len(valid_results)}/{len(strain_segments)} successful)")
        
        return results


# Legacy alias for backward compatibility
GWOSCDownloader = ProductionGWOSCDownloader
DataPreprocessor = AdvancedDataPreprocessor
```

## models/__init__.py

```python
"""
Models Module: Neural Network Architectures

Implements the 3-component neuromorphic pipeline:
1. CPC Encoder - Self-supervised representation learning
2. Spike Bridge - Continuous to spike conversion  
3. SNN Classifier - Neuromorphic binary classification
"""

from .cpc_encoder import CPCEncoder, info_nce_loss
from .snn_classifier import SNNClassifier, create_snn_classifier, SNNTrainer

__all__ = [
    "CPCEncoder",
    "info_nce_loss",
    "SNNClassifier",
    "create_snn_classifier", 
    "SNNTrainer",
]
```

## models/cpc_encoder.py

```python
"""
Contrastive Predictive Coding (CPC) Encoder

Self-supervised learning architecture for gravitational wave strain data.
Enhanced with findings from CPC+SNN Integration Paper (2025).
"""

import jax
import jax.numpy as jnp
import flax.linen as nn
import optax
from typing import Tuple, Optional


class CPCEncoder(nn.Module):
    """Enhanced CPC Encoder for GW strain data."""
    latent_dim: int = 256
    conv_channels: Tuple[int, ...] = (32, 64, 128)
    use_batch_norm: bool = False  # Make BatchNorm optional
    dropout_rate: float = 0.0     # Make dropout optional
    
    @nn.compact
    def __call__(self, x: jnp.ndarray, train: bool = False) -> jnp.ndarray:
        # Scale input to reasonable range (GW strain data is very small ~1e-23)
        x = x * 1e20  # Scale from ~1e-23 to ~1e-3 range
        
        # Add channel dimension
        x = x[..., None]  # [batch, time, 1]
        
        # Enhanced convolutions with optional regularization
        for i, channels in enumerate(self.conv_channels):
            x = nn.Conv(channels, kernel_size=(9,), strides=(2,))(x)
            x = nn.gelu(x)
            
            # Optional batch normalization
            if self.use_batch_norm:
                x = nn.BatchNorm(use_running_average=not train)(x)
            
            # Optional dropout
            if self.dropout_rate > 0.0:
                x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
        
        # After convolutions: [batch, downsampled_time, final_channels]
        batch_size, seq_len, features = x.shape
        
        # **ENHANCED: Functional GRU with better parameter management**
        # Paper suggests better GRU initialization dla stability
        gru_ih_kernel = self.param('gru_ih_kernel', 
                                  nn.initializers.xavier_uniform(), 
                                  (features, 3 * features))
        gru_hh_kernel = self.param('gru_hh_kernel', 
                                  nn.initializers.orthogonal(scale=1.0), 
                                  (features, 3 * features))
        gru_ih_bias = self.param('gru_ih_bias', 
                                nn.initializers.zeros, 
                                (3 * features,))
        gru_hh_bias = self.param('gru_hh_bias', 
                                nn.initializers.zeros, 
                                (3 * features,))
        
        # Initial hidden state with better initialization
        h_init = jnp.zeros((batch_size, features))
        
        # Transpose dla scan: [time, batch, features]
        x_transposed = jnp.transpose(x, (1, 0, 2))
        
        def gru_step(h, x):
            """Enhanced GRU step with better numerical stability."""
            # Input projections
            gi = jnp.dot(x, gru_ih_kernel) + gru_ih_bias
            gh = jnp.dot(h, gru_hh_kernel) + gru_hh_bias
            
            # Split into reset, update, new gates
            i_r, i_z, i_n = jnp.split(gi, 3, axis=-1)
            h_r, h_z, h_n = jnp.split(gh, 3, axis=-1)
            
            # Apply activations with better numerical stability
            reset_gate = jax.nn.sigmoid(i_r + h_r)
            update_gate = jax.nn.sigmoid(i_z + h_z)
            new_gate = jnp.tanh(i_n + reset_gate * h_n)
            
            # Update hidden state
            h_new = (1 - update_gate) * new_gate + update_gate * h
            
            return h_new, h_new
        
        # Apply GRU across time steps
        _, hidden_states = jax.lax.scan(gru_step, h_init, x_transposed)
        
        # Transpose back: [batch, time, features]
        hidden_states = jnp.transpose(hidden_states, (1, 0, 2))
        
        # Final projection to latent space with better initialization
        latent_features = nn.Dense(
            self.latent_dim,
            kernel_init=nn.initializers.xavier_uniform(),
            bias_init=nn.initializers.zeros
        )(hidden_states)
        
        # L2 normalization dla better contrastive learning (paper finding)
        # Only normalize if the norm is significant to avoid division by zero
        norms = jnp.linalg.norm(latent_features, axis=-1, keepdims=True)
        latent_features = jnp.where(
            norms > 1e-6,
            latent_features / (norms + 1e-8),
            latent_features  # Keep original if norm too small
        )
        
        return latent_features


def enhanced_info_nce_loss(z_context: jnp.ndarray, 
                          z_target: jnp.ndarray, 
                          temperature: float = 0.1,
                          num_negatives: int = 8,  # Reduced for stability
                          use_hard_negatives: bool = False) -> jnp.ndarray:  # Disabled for now
    """
    Enhanced InfoNCE loss with simplified implementation for stability.
    
    Improvements:
    - Better numerical stability
    - Cosine similarity computation
    - Simplified negative sampling
    """
    batch_size, context_len, feature_dim = z_context.shape
    _, target_len, _ = z_target.shape
    
    # Ensure equal lengths dla proper alignment
    min_len = min(context_len, target_len)
    z_context = z_context[:, :min_len, :]
    z_target = z_target[:, :min_len, :]
    
    # Normalize dla cosine similarity (already done in encoder but ensure here)
    z_context_norm = z_context / (jnp.linalg.norm(z_context, axis=-1, keepdims=True) + 1e-8)
    z_target_norm = z_target / (jnp.linalg.norm(z_target, axis=-1, keepdims=True) + 1e-8)
    
    # **SIMPLIFIED: Use within-batch negatives only for stability**
    total_loss = 0.0
    
    for t in range(min_len):
        # Get features dla timestep t
        context_t = z_context_norm[:, t, :]  # [batch, features]
        target_t = z_target_norm[:, t, :]    # [batch, features]
        
        # Compute similarity matrix: [batch, batch]
        similarity_matrix = jnp.dot(context_t, target_t.T)  # [batch, batch]
        
        # Apply temperature scaling
        logits = similarity_matrix / temperature  # [batch, batch]
        
        # Labels: diagonal elements are positive pairs
        labels = jnp.arange(batch_size)
        
        # Compute cross-entropy loss dla this timestep
        step_loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)
        total_loss += jnp.mean(step_loss)
    
    return total_loss / min_len


# Backward compatibility
def info_nce_loss(z_context: jnp.ndarray, z_target: jnp.ndarray, temperature: float = 0.1) -> jnp.ndarray:
    """Original InfoNCE loss dla backward compatibility."""
    return enhanced_info_nce_loss(z_context, z_target, temperature, use_hard_negatives=False)


def create_enhanced_cpc_encoder(latent_dim: int = 256,
                              conv_channels: Tuple[int, ...] = (32, 64, 128),
                              use_batch_norm: bool = True,
                              dropout_rate: float = 0.1) -> CPCEncoder:
    """Create enhanced CPC encoder with paper-based improvements."""
    return CPCEncoder(
        latent_dim=latent_dim, 
        conv_channels=conv_channels,
        use_batch_norm=use_batch_norm,
        dropout_rate=dropout_rate
    )


def create_standard_cpc_encoder(latent_dim: int = 256,
                              conv_channels: Tuple[int, ...] = (32, 64, 128)) -> CPCEncoder:
    """Create standard CPC encoder without enhanced features."""
    return CPCEncoder(
        latent_dim=latent_dim, 
        conv_channels=conv_channels,
        use_batch_norm=False,
        dropout_rate=0.0
    )
```

## models/simple_snn.py

```python
"""
Simple JAX-based SNN Implementation

Lightweight LIF (Leaky Integrate-and-Fire) SNN classifier
for quick testing of CPC + Spike Bridge + SNN integration.
"""

import jax
import jax.numpy as jnp
import flax.linen as nn
import optax
from typing import Tuple


class LIFLayer(nn.Module):
    """Simple LIF (Leaky Integrate-and-Fire) layer in JAX/Flax."""
    
    hidden_size: int
    tau_mem: float = 20e-3  # Membrane time constant
    tau_syn: float = 5e-3   # Synaptic time constant  
    threshold: float = 1.0  # Spike threshold
    dt: float = 1e-3        # Time step
    
    @nn.compact
    def __call__(self, spikes: jnp.ndarray) -> jnp.ndarray:
        """
        LIF layer forward pass.
        
        Args:
            spikes: Input spikes [batch, time, input_dim]
            
        Returns:
            output_spikes: Output spikes [batch, time, hidden_size]
        """
        batch_size, time_steps, input_dim = spikes.shape
        
        # Linear transformation (synaptic weights)
        W = self.param('kernel', nn.initializers.glorot_uniform(), (input_dim, self.hidden_size))
        b = self.param('bias', nn.initializers.zeros, (self.hidden_size,))
        
        # Decay factors
        alpha_mem = jnp.exp(-self.dt / self.tau_mem)  # Membrane decay
        alpha_syn = jnp.exp(-self.dt / self.tau_syn)  # Synaptic decay
        
        # Initial states
        v_mem = jnp.zeros((batch_size, self.hidden_size))  # Membrane potential
        i_syn = jnp.zeros((batch_size, self.hidden_size))  # Synaptic current
        
        def lif_step(states, x_t):
            """Single LIF time step."""
            v_mem, i_syn = states
            
            # Synaptic input
            i_input = jnp.dot(x_t, W) + b
            
            # Update synaptic current (exponential decay + input)
            i_syn = alpha_syn * i_syn + i_input
            
            # Update membrane potential (leak + synaptic current)
            v_mem = alpha_mem * v_mem + i_syn
            
            # Spike generation (threshold crossing)
            spikes_out = (v_mem >= self.threshold).astype(jnp.float32)
            
            # Reset membrane potential after spike
            v_mem = v_mem * (1 - spikes_out)
            
            return (v_mem, i_syn), spikes_out
        
        # Scan over time dimension
        _, output_spikes = jax.lax.scan(
            lif_step, 
            init=(v_mem, i_syn),
            xs=jnp.transpose(spikes, (1, 0, 2))  # [time, batch, input_dim]
        )
        
        # Transpose back: [batch, time, hidden_size]
        output_spikes = jnp.transpose(output_spikes, (1, 0, 2))
        
        return output_spikes


class SimpleSNN(nn.Module):
    """Simple 2-layer SNN classifier."""
    
    hidden_size: int = 128
    num_classes: int = 2
    tau_mem: float = 20e-3
    tau_syn: float = 5e-3
    threshold: float = 1.0
    
    @nn.compact  
    def __call__(self, spikes: jnp.ndarray) -> jnp.ndarray:
        """
        SNN forward pass.
        
        Args:
            spikes: Input spike trains [batch, time, input_dim]
            
        Returns:
            logits: Classification logits [batch, num_classes]
        """
        # First LIF layer
        h1 = LIFLayer(
            hidden_size=self.hidden_size,
            tau_mem=self.tau_mem,
            tau_syn=self.tau_syn,
            threshold=self.threshold
        )(spikes)
        
        # Second LIF layer
        h2 = LIFLayer(
            hidden_size=self.hidden_size // 2,
            tau_mem=self.tau_mem,
            tau_syn=self.tau_syn,
            threshold=self.threshold
        )(h1)
        
        # Global average pooling over time
        h_pooled = jnp.mean(h2, axis=1)  # [batch, hidden_size//2]
        
        # Linear readout
        logits = nn.Dense(self.num_classes)(h_pooled)
        
        return logits


class SimpleSNNTrainer:
    """Training utilities dla Simple SNN."""
    
    def __init__(self, learning_rate: float = 1e-3):
        self.optimizer = optax.adam(learning_rate)
        
    def classification_loss(self, 
                          params: dict,
                          spikes: jnp.ndarray, 
                          labels: jnp.ndarray,
                          model: SimpleSNN) -> jnp.ndarray:
        """Classification loss function."""
        logits = model.apply(params, spikes)
        
        return optax.softmax_cross_entropy_with_integer_labels(
            logits, labels
        ).mean()
        
    def training_step(self,
                     params: dict,
                     opt_state: optax.OptState,
                     spikes: jnp.ndarray,
                     labels: jnp.ndarray,
                     model: SimpleSNN) -> Tuple[dict, optax.OptState, float]:
        """Single training step."""
        
        loss, grads = jax.value_and_grad(self.classification_loss)(
            params, spikes, labels, model
        )
        
        updates, opt_state = self.optimizer.update(grads, opt_state)
        params = optax.apply_updates(params, updates)
        
        return params, opt_state, loss
        
    def accuracy(self, 
                params: dict,
                spikes: jnp.ndarray, 
                labels: jnp.ndarray,
                model: SimpleSNN) -> float:
        """Compute classification accuracy."""
        logits = model.apply(params, spikes)
        predictions = jnp.argmax(logits, axis=-1)
        
        return jnp.mean(predictions == labels)


# Convenience functions
def create_simple_snn(hidden_size: int = 64, num_classes: int = 2) -> SimpleSNN:
    """Create simple SNN model."""
    return SimpleSNN(
        hidden_size=hidden_size,
        num_classes=num_classes,
        tau_mem=20e-3,
        tau_syn=5e-3,
        threshold=1.0
    )


def test_simple_lif_layer():
    """Quick test of LIF layer functionality."""
    # Create test data
    batch_size, time_steps, input_dim = 2, 10, 8
    test_spikes = jax.random.bernoulli(
        jax.random.PRNGKey(0), 0.1, (batch_size, time_steps, input_dim)
    ).astype(jnp.float32)
    
    # Create and test LIF layer
    lif = LIFLayer(hidden_size=16)
    key = jax.random.PRNGKey(42)
    params = lif.init(key, test_spikes)
    
    output_spikes = lif.apply(params, test_spikes)
    
    print(f"Input shape: {test_spikes.shape}")
    print(f"Output shape: {output_spikes.shape}")
    print(f"Input spike rate: {jnp.mean(test_spikes):.3f}")
    print(f"Output spike rate: {jnp.mean(output_spikes):.3f}")
    
    return output_spikes.shape == (batch_size, time_steps, 16)


def test_simple_snn():
    """Quick test of complete SNN."""
    # Create test data
    batch_size, time_steps, input_dim = 2, 20, 32
    test_spikes = jax.random.bernoulli(
        jax.random.PRNGKey(0), 0.05, (batch_size, time_steps, input_dim)
    ).astype(jnp.float32)
    test_labels = jnp.array([0, 1])
    
    # Create and test SNN
    snn = create_simple_snn(hidden_size=32, num_classes=2)
    trainer = SimpleSNNTrainer(learning_rate=1e-3)
    
    key = jax.random.PRNGKey(42)
    params = snn.init(key, test_spikes)
    opt_state = trainer.optimizer.init(params)
    
    # Forward pass
    logits = snn.apply(params, test_spikes)
    
    # Training step
    params, opt_state, loss = trainer.training_step(
        params, opt_state, test_spikes, test_labels, snn
    )
    
    # Accuracy
    accuracy = trainer.accuracy(params, test_spikes, test_labels, snn)
    
    print(f"Logits shape: {logits.shape}")
    print(f"Training loss: {loss:.4f}")
    print(f"Accuracy: {accuracy:.3f}")
    
    return True


if __name__ == "__main__":
    print("Testing Simple LIF Layer...")
    success1 = test_simple_lif_layer()
    print(f"LIF Layer test: {'PASSED' if success1 else 'FAILED'}\n")
    
    print("Testing Simple SNN...")
    success2 = test_simple_snn()
    print(f"SNN test: {'PASSED' if success2 else 'FAILED'}")
    
    overall_success = success1 and success2
    print(f"\nOverall: {'SUCCESS' if overall_success else 'FAILED'}")
```

## models/snn_classifier.py

```python
"""
Spiking Neural Network (SNN) Classifier

Neuromorphic binary classifier using Spyx LIF neurons
for energy-efficient gravitational wave detection.
"""

import jax
import jax.numpy as jnp
import haiku as hk
import spyx.nn as snn
import optax
from typing import Optional


class SNNClassifier(hk.Module):
    """
    Spiking Neural Network for binary GW detection.
    
    Architecture: LIF layers + global pooling + linear readout
    Uses Spyx (Google Haiku-based) for stable SNN implementation.
    """
    
    def __init__(self, 
                 hidden_size: int = 128,
                 num_classes: int = 2,
                 tau_mem: float = 20e-3,
                 tau_syn: float = 5e-3,
                 threshold: float = 1.0,
                 name: Optional[str] = None):
        super().__init__(name=name)
        self.hidden_size = hidden_size
        self.num_classes = num_classes
        self.tau_mem = tau_mem
        self.tau_syn = tau_syn  
        self.threshold = threshold
    
    def __call__(self, spikes: jnp.ndarray) -> jnp.ndarray:
        """
        Forward pass through SNN classifier.
        
        Args:
            spikes: Input spike trains [batch, time, input_dim]
            
        Returns:
            Classification logits [batch, num_classes]
        """
        # First LIF layer
        h1 = snn.LIF(
            self.hidden_size,
            activation=snn.superspike
        )(spikes)
        
        # Second LIF layer with lateral inhibition 
        h2 = snn.LIF(
            self.hidden_size,
            activation=snn.superspike
        )(h1)
        
        # Global average pooling over time dimension
        h_pooled = jnp.mean(h2, axis=1)  # [batch, hidden_size]
        
        # Linear readout (non-spiking)
        logits = hk.Linear(self.num_classes)(h_pooled)
        
        return logits


def create_snn_classifier(hidden_size: int = 128, 
                         num_classes: int = 2) -> hk.Transformed:
    """
    Create Haiku-transformed SNN classifier.
    
    Returns:
        Haiku transformed function for SNN forward pass
    """
    def snn_forward(spikes: jnp.ndarray) -> jnp.ndarray:
        classifier = SNNClassifier(
            hidden_size=hidden_size,
            num_classes=num_classes
        )
        return classifier(spikes)
    
    return hk.transform(snn_forward)


class SNNTrainer:
    """Training utilities for SNN classifier."""
    
    def __init__(self, 
                 snn_fn: hk.Transformed,
                 learning_rate: float = 1e-3):
        self.snn_fn = snn_fn
        self.optimizer = optax.adam(learning_rate)
        
    def classification_loss(self, 
                          params: dict,
                          spikes: jnp.ndarray, 
                          labels: jnp.ndarray) -> jnp.ndarray:
        """Binary classification loss for SNN."""
        logits = self.snn_fn.apply(params, None, spikes)
        
        return optax.softmax_cross_entropy_with_integer_labels(
            logits, labels
        ).mean()
        
    def training_step(self,
                     params: dict,
                     opt_state: optax.OptState,
                     spikes: jnp.ndarray,
                     labels: jnp.ndarray) -> tuple:
        """Single training step for SNN classifier."""
        
        loss, grads = jax.value_and_grad(self.classification_loss)(
            params, spikes, labels
        )
        
        updates, opt_state = self.optimizer.update(grads, opt_state)
        params = optax.apply_updates(params, updates)
        
        return params, opt_state, loss
        
    def accuracy(self, 
                params: dict,
                spikes: jnp.ndarray, 
                labels: jnp.ndarray) -> float:
        """Compute classification accuracy."""
        logits = self.snn_fn.apply(params, None, spikes)
        predictions = jnp.argmax(logits, axis=-1)
        
        return jnp.mean(predictions == labels)
```

## models/spike_bridge.py

```python
"""
Spike Bridge: CPC Latent to Spike Train Conversion

Converts continuous CPC encoder representations to discrete spike trains
for neuromorphic SNN processing. Supports multiple encoding strategies.
"""

import jax
import jax.numpy as jnp
from typing import Tuple, Optional
from enum import Enum


class SpikeEncodingStrategy(Enum):
    """Spike encoding strategies."""
    POISSON_RATE = "poisson_rate"
    TEMPORAL_CONTRAST = "temporal_contrast"
    POPULATION_VECTOR = "population_vector"
    RATE_BASED = "rate_based"


class SpikeBridge:
    """
    Converts CPC latent representations to spike trains.
    
    Support multiple neuromorphic encoding strategies with
    configurable parameters for optimal SNN performance.
    """
    
    def __init__(self,
                 encoding_strategy: SpikeEncodingStrategy = SpikeEncodingStrategy.POISSON_RATE,
                 spike_time_steps: int = 100,
                 max_spike_rate: float = 100.0,  # Hz
                 dt: float = 1e-3,  # 1ms time steps
                 population_size: int = 8):
        """
        Initialize spike bridge.
        
        Args:
            encoding_strategy: Method for converting continuous values to spikes
            spike_time_steps: Number of time steps for spike train generation
            max_spike_rate: Maximum firing rate (Hz) for rate-based encoding
            dt: Time step size (seconds) 
            population_size: Number of neurons per population (for population coding)
        """
        self.encoding_strategy = encoding_strategy
        self.spike_time_steps = spike_time_steps
        self.max_spike_rate = max_spike_rate
        self.dt = dt
        self.population_size = population_size
        
    def encode(self, 
               latent_features: jnp.ndarray, 
               key: jax.random.PRNGKey) -> jnp.ndarray:
        """
        Convert CPC latent features to spike trains.
        
        Args:
            latent_features: CPC encoder output [batch, seq_len, latent_dim]
            key: JAX random key for stochastic spike generation
            
        Returns:
            spike_trains: Binary spike trains [batch, spike_time_steps, spike_dim]
        """
        if self.encoding_strategy == SpikeEncodingStrategy.POISSON_RATE:
            return self._poisson_rate_encoding(latent_features, key)
        elif self.encoding_strategy == SpikeEncodingStrategy.TEMPORAL_CONTRAST:
            return self._temporal_contrast_encoding(latent_features, key)
        elif self.encoding_strategy == SpikeEncodingStrategy.POPULATION_VECTOR:
            return self._population_vector_encoding(latent_features, key)
        elif self.encoding_strategy == SpikeEncodingStrategy.RATE_BASED:
            return self._rate_based_encoding(latent_features, key)
        else:
            raise ValueError(f"Unknown encoding strategy: {self.encoding_strategy}")
    
    def _poisson_rate_encoding(self, 
                              latent_features: jnp.ndarray, 
                              key: jax.random.PRNGKey) -> jnp.ndarray:
        """
        Poisson rate coding: Continuous values â†’ firing rates â†’ Poisson spikes.
        
        Higher latent values = higher firing rates = more spikes.
        Most biologically plausible encoding strategy.
        """
        batch_size, seq_len, latent_dim = latent_features.shape
        
        # Normalize features to [0, 1] range for stable rate conversion
        features_norm = jax.nn.sigmoid(latent_features)
        
        # Convert to firing rates (Hz)
        firing_rates = features_norm * self.max_spike_rate
        
        # Convert rates to spike probabilities per time step
        spike_probs = firing_rates * self.dt  # Probability per dt
        spike_probs = jnp.clip(spike_probs, 0.0, 1.0)  # Ensure valid probabilities
        
        # Temporal expansion: repeat each feature value across spike time steps
        # [batch, seq_len, latent_dim] â†’ [batch, seq_len * spike_time_steps, latent_dim]
        spike_probs_expanded = jnp.repeat(spike_probs, self.spike_time_steps, axis=1)
        
        # Generate Poisson spikes
        random_vals = jax.random.uniform(
            key, shape=spike_probs_expanded.shape
        )
        spikes = (random_vals < spike_probs_expanded).astype(jnp.float32)
        
        # Reshape to proper spike train format
        # [batch, seq_len * spike_time_steps, latent_dim]
        total_time_steps = seq_len * self.spike_time_steps
        spikes = spikes.reshape(batch_size, total_time_steps, latent_dim)
        
        return spikes
    
    def _temporal_contrast_encoding(self, 
                                  latent_features: jnp.ndarray, 
                                  key: jax.random.PRNGKey) -> jnp.ndarray:
        """
        Temporal contrast coding: Spikes encode changes/gradients.
        
        Positive changes â†’ positive spikes, negative changes â†’ inhibitory spikes.
        Efficient dla representing temporal dynamics in GW signals.
        """
        batch_size, seq_len, latent_dim = latent_features.shape
        
        # Compute temporal differences (gradients)
        features_padded = jnp.pad(latent_features, ((0, 0), (1, 0), (0, 0)), mode='edge')
        temporal_gradients = jnp.diff(features_padded, axis=1)
        
        # Split into positive and negative changes
        positive_changes = jnp.maximum(temporal_gradients, 0)
        negative_changes = jnp.maximum(-temporal_gradients, 0)
        
        # Normalize and convert to spike rates
        pos_rates = jax.nn.sigmoid(positive_changes * 10) * self.max_spike_rate
        neg_rates = jax.nn.sigmoid(negative_changes * 10) * self.max_spike_rate
        
        # Create spike probabilities
        pos_probs = pos_rates * self.dt
        neg_probs = neg_rates * self.dt
        
        # Expand temporally
        pos_probs_expanded = jnp.repeat(pos_probs, self.spike_time_steps, axis=1)
        neg_probs_expanded = jnp.repeat(neg_probs, self.spike_time_steps, axis=1)
        
        # Generate spikes dla both positive and negative channels
        key1, key2 = jax.random.split(key)
        
        pos_spikes = (jax.random.uniform(key1, pos_probs_expanded.shape) < pos_probs_expanded).astype(jnp.float32)
        neg_spikes = (jax.random.uniform(key2, neg_probs_expanded.shape) < neg_probs_expanded).astype(jnp.float32)
        
        # Combine into single spike train with doubled dimensionality
        # Positive channels: [0, latent_dim), Negative channels: [latent_dim, 2*latent_dim)
        spikes = jnp.concatenate([pos_spikes, neg_spikes], axis=-1)
        
        total_time_steps = seq_len * self.spike_time_steps
        spikes = spikes.reshape(batch_size, total_time_steps, 2 * latent_dim)
        
        return spikes
    
    def _population_vector_encoding(self, 
                                  latent_features: jnp.ndarray, 
                                  key: jax.random.PRNGKey) -> jnp.ndarray:
        """
        Population vector coding: Each feature â†’ population of neurons.
        
        Higher feature values activate more neurons in population.
        Provides redundancy and noise robustness.
        """
        batch_size, seq_len, latent_dim = latent_features.shape
        
        # Normalize features
        features_norm = jax.nn.sigmoid(latent_features)
        
        # Create population thresholds [0, 1/N, 2/N, ..., (N-1)/N]
        thresholds = jnp.linspace(0, 1, self.population_size + 1)[:-1]
        thresholds = thresholds.reshape(1, 1, 1, self.population_size)
        
        # Expand features dla population comparison
        features_expanded = features_norm[..., :, None]  # [batch, seq, latent, 1]
        
        # Population activation: neuron fires if feature > threshold
        population_rates = (features_expanded > thresholds).astype(jnp.float32)
        population_rates = population_rates * self.max_spike_rate
        
        # Convert to spike probabilities
        spike_probs = population_rates * self.dt
        spike_probs = jnp.clip(spike_probs, 0.0, 1.0)
        
        # Expand temporally
        spike_probs_expanded = jnp.repeat(spike_probs, self.spike_time_steps, axis=1)
        
        # Generate spikes
        random_vals = jax.random.uniform(key, spike_probs_expanded.shape)
        spikes = (random_vals < spike_probs_expanded).astype(jnp.float32)
        
        # Reshape: [batch, total_time, latent_dim * population_size]
        total_time_steps = seq_len * self.spike_time_steps
        spike_dim = latent_dim * self.population_size
        spikes = spikes.reshape(batch_size, total_time_steps, spike_dim)
        
        return spikes
    
    def _rate_based_encoding(self, 
                           latent_features: jnp.ndarray, 
                           key: jax.random.PRNGKey) -> jnp.ndarray:
        """
        Rate-based encoding: Direct rate representation without stochasticity.
        
        Continuous values â†’ continuous firing rates (no binary spikes).
        Computationally efficient dla SNN training.
        """
        batch_size, seq_len, latent_dim = latent_features.shape
        
        # Normalize to firing rates
        features_norm = jax.nn.sigmoid(latent_features)
        firing_rates = features_norm * self.max_spike_rate
        
        # Expand temporally (repeat rates)
        rates_expanded = jnp.repeat(firing_rates, self.spike_time_steps, axis=1)
        
        # Normalize to [0, 1] range dla SNN compatibility
        rates_normalized = rates_expanded / self.max_spike_rate
        
        total_time_steps = seq_len * self.spike_time_steps  
        rates_final = rates_normalized.reshape(batch_size, total_time_steps, latent_dim)
        
        return rates_final
    
    def compute_spike_statistics(self, spikes: jnp.ndarray) -> dict:
        """
        Compute spike train statistics dla monitoring and analysis.
        
        Args:
            spikes: Spike trains [batch, time, neurons]
            
        Returns:
            Dictionary with spike statistics
        """
        # Overall firing rate
        mean_rate = jnp.mean(spikes) * (1.0 / self.dt)  # Convert to Hz
        
        # Sparsity (fraction of zeros)
        sparsity = jnp.mean(spikes == 0)
        
        # Temporal correlations (autocorrelation at lag 1)
        spikes_t0 = spikes[:, :-1, :]
        spikes_t1 = spikes[:, 1:, :]
        temporal_corr = jnp.corrcoef(
            spikes_t0.flatten(), 
            spikes_t1.flatten()
        )[0, 1]
        
        # Synchrony (correlation across neurons)
        if spikes.shape[-1] > 1:
            spike_rates_per_neuron = jnp.mean(spikes, axis=1)  # [batch, neurons]
            synchrony = jnp.mean(jnp.corrcoef(spike_rates_per_neuron.T))
        else:
            synchrony = 1.0
        
        return {
            'mean_firing_rate_hz': float(mean_rate),
            'sparsity': float(sparsity), 
            'temporal_correlation': float(temporal_corr),
            'neural_synchrony': float(synchrony),
            'total_spikes': int(jnp.sum(spikes)),
            'spike_shape': spikes.shape
        }


# Convenience functions dla easy integration
def create_default_spike_bridge() -> SpikeBridge:
    """Create spike bridge with optimal default parameters dla GW detection."""
    return SpikeBridge(
        encoding_strategy=SpikeEncodingStrategy.POISSON_RATE,
        spike_time_steps=50,  # 50ms simulation time
        max_spike_rate=100.0,  # 100 Hz max rate
        dt=1e-3,  # 1ms resolution
        population_size=8
    )


def create_fast_spike_bridge() -> SpikeBridge:
    """Create spike bridge optimized dla speed (rate-based encoding)."""
    return SpikeBridge(
        encoding_strategy=SpikeEncodingStrategy.RATE_BASED,
        spike_time_steps=20,  # 20ms simulation time
        max_spike_rate=50.0,  # Lower max rate 
        dt=1e-3,
        population_size=4
    )


def create_robust_spike_bridge() -> SpikeBridge:
    """Create spike bridge optimized dla robustness (population coding)."""
    return SpikeBridge(
        encoding_strategy=SpikeEncodingStrategy.POPULATION_VECTOR,
        spike_time_steps=80,  # Longer simulation dla stability
        max_spike_rate=200.0,  # Higher max rate
        dt=0.5e-3,  # Higher temporal resolution
        population_size=16  # Larger populations
    )
```

## training/__init__.py

```python

```

## training/advanced_training.py

```python
#!/usr/bin/env python3

"""
Advanced Neuromorphic GW Training Pipeline

State-of-the-art techniques for achieving 70-90% accuracy:
- AdamW optimizer with cosine annealing
- Focal loss for class imbalance  
- Advanced data augmentation
- Attention-enhanced CPC
- Deeper SNN architectures
- Diffusion-based signal enhancement
"""

import jax
import jax.numpy as jnp
import optax
import flax.linen as nn
import logging
import time
from typing import Dict, Tuple, Optional, List
from pathlib import Path
from dataclasses import dataclass

from ..models.cpc_encoder import CPCEncoder, enhanced_info_nce_loss
from ..models.simple_snn import create_simple_snn
from ..models.spike_bridge import SpikeBridge, SpikeEncodingStrategy
from ..data.continuous_gw_generator import ContinuousGWGenerator
from ..data.gw_download import ProductionGWOSCDownloader

logger = logging.getLogger(__name__)


@dataclass 
class AdvancedTrainingConfig:
    """Advanced configuration for high-performance neuromorphic GW detection."""
    
    # Enhanced Dataset
    num_continuous_signals: int = 500  # Much larger dataset
    num_binary_signals: int = 500
    num_noise_samples: int = 300  # Reduced noise dominance
    signal_duration: float = 4.0
    
    # Advanced Training
    batch_size: int = 32  # Larger batches for stability
    learning_rate: float = 3e-4  # Lower LR for fine-tuning
    num_epochs: int = 100  # Much more training
    warmup_epochs: int = 10
    
    # Model Architecture
    cpc_latent_dim: int = 256  # Larger representation
    cpc_conv_channels: Tuple[int, ...] = (64, 128, 256, 512)  # Deeper CNN
    snn_hidden_sizes: Tuple[int, ...] = (256, 128, 64)  # Multi-layer SNN
    spike_time_steps: int = 100  # More temporal resolution
    
    # Advanced Techniques
    use_attention: bool = True  # Attention in CPC
    use_focal_loss: bool = True  # For class imbalance
    use_mixup: bool = True  # Data augmentation
    use_cosine_scheduling: bool = True
    weight_decay: float = 0.01  # AdamW regularization
    
    # Spike Encoding
    spike_encoding: SpikeEncodingStrategy = SpikeEncodingStrategy.TEMPORAL_CONTRAST
    multi_encoding: bool = True  # Multiple encoding strategies
    
    # Output
    output_dir: str = "advanced_gw_training_outputs"
    save_checkpoints: bool = True
    checkpoint_every: int = 20


class AttentionCPCEncoder(nn.Module):
    """Enhanced CPC Encoder with Attention Mechanisms."""
    
    latent_dim: int = 256
    conv_channels: Tuple[int, ...] = (64, 128, 256, 512)
    num_attention_heads: int = 8
    use_attention: bool = True
    dropout_rate: float = 0.1
    
    @nn.compact
    def __call__(self, x: jnp.ndarray, train: bool = True) -> jnp.ndarray:
        # Input scaling for GW strain data
        x = x * 1e20
        
        # Add channel dimension
        x = x[..., None]  # [batch, time, 1]
        
        # **ENHANCED: Progressive convolution with residual connections**
        skip_connections = []
        
        for i, channels in enumerate(self.conv_channels):
            if i > 0 and x.shape[-1] == channels:
                # Residual connection when dimensions match
                residual = x
            else:
                residual = None
                
            x = nn.Conv(channels, kernel_size=(9,), strides=(2,), 
                       kernel_init=nn.initializers.he_normal())(x)
            x = nn.BatchNorm(use_running_average=not train)(x)
            x = nn.gelu(x)
            
            if residual is not None:
                # Residual connection with proper padding
                if residual.shape[1] != x.shape[1]:
                    residual = nn.avg_pool(residual, window_shape=(2,), strides=(2,))
                x = x + residual
                
            x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
            skip_connections.append(x)
        
        # **NEW: Multi-scale feature fusion**
        # Upsample smaller feature maps and concatenate
        batch_size, seq_len, final_channels = x.shape
        fused_features = x
        
        for i, skip in enumerate(skip_connections[:-1]):
            # Upsample to match final sequence length
            skip_upsampled = jnp.repeat(skip, seq_len // skip.shape[1], axis=1)
            if skip_upsampled.shape[1] > seq_len:
                skip_upsampled = skip_upsampled[:, :seq_len, :]
            elif skip_upsampled.shape[1] < seq_len:
                # Pad to match
                pad_width = seq_len - skip_upsampled.shape[1]
                skip_upsampled = jnp.pad(skip_upsampled, 
                                       ((0, 0), (0, pad_width), (0, 0)), 
                                       mode='edge')
            
            # Project to same channel dimension
            skip_proj = nn.Dense(final_channels)(skip_upsampled)
            fused_features = fused_features + 0.1 * skip_proj  # Weighted addition
        
        # **ENHANCED: Bidirectional GRU with attention**
        gru_features = fused_features.shape[-1]
        
        # Forward GRU
        forward_gru = nn.scan(
            nn.GRUCell(),
            variable_broadcast="params",
            split_rngs={"params": False},
            length=seq_len
        )
        
        carry_forward = nn.GRUCell().initialize_carry(
            jax.random.PRNGKey(0), (batch_size, gru_features)
        )
        
        # Transpose for scan: [time, batch, features]
        x_transposed = jnp.transpose(fused_features, (1, 0, 2))
        carry_forward, forward_states = forward_gru(carry_forward, x_transposed)
        forward_states = jnp.transpose(forward_states, (1, 0, 2))  # Back to [batch, time, features]
        
        # Backward GRU
        backward_gru = nn.scan(
            nn.GRUCell(),
            variable_broadcast="params", 
            split_rngs={"params": False},
            length=seq_len,
            reverse=True
        )
        
        carry_backward = nn.GRUCell().initialize_carry(
            jax.random.PRNGKey(1), (batch_size, gru_features)
        )
        
        carry_backward, backward_states = backward_gru(carry_backward, x_transposed)
        backward_states = jnp.transpose(backward_states, (1, 0, 2))
        
        # Concatenate bidirectional states
        bidirectional_states = jnp.concatenate([forward_states, backward_states], axis=-1)
        
        # **NEW: Multi-head self-attention**
        if self.use_attention:
            attention_dim = bidirectional_states.shape[-1]
            
            # Self-attention mechanism
            attention = nn.MultiHeadDotProductAttention(
                num_heads=self.num_attention_heads,
                qkv_features=attention_dim,
                dropout_rate=self.dropout_rate if train else 0.0
            )
            
            attended_features = attention(
                bidirectional_states,  # queries
                bidirectional_states,  # keys
                bidirectional_states,  # values
                deterministic=not train
            )
            
            # Residual connection
            bidirectional_states = bidirectional_states + attended_features
            
            # Layer normalization
            bidirectional_states = nn.LayerNorm()(bidirectional_states)
        
        # **ENHANCED: Progressive projection to latent space**
        # First reduce dimension gradually
        x = nn.Dense(512, kernel_init=nn.initializers.he_normal())(bidirectional_states)
        x = nn.gelu(x)
        x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
        
        x = nn.Dense(256, kernel_init=nn.initializers.he_normal())(x)
        x = nn.gelu(x)
        x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
        
        # Final projection to latent dimension
        latent_features = nn.Dense(
            self.latent_dim,
            kernel_init=nn.initializers.xavier_uniform(),
            bias_init=nn.initializers.zeros
        )(x)
        
        # **ENHANCED: Adaptive normalization**
        # Only normalize if the norm is significant
        norms = jnp.linalg.norm(latent_features, axis=-1, keepdims=True)
        normalized_features = jnp.where(
            norms > 1e-6,
            latent_features / (norms + 1e-8),
            latent_features
        )
        
        return normalized_features


class DeepSNN(nn.Module):
    """Deep Spiking Neural Network with multiple layers."""
    
    hidden_sizes: Tuple[int, ...] = (256, 128, 64)
    num_classes: int = 3
    dropout_rate: float = 0.2
    
    @nn.compact
    def __call__(self, spikes: jnp.ndarray, train: bool = True) -> jnp.ndarray:
        """
        spikes: [batch, time, input_dim]
        Returns: [batch, num_classes] logits
        """
        x = spikes
        
        # Multiple LIF layers with skip connections
        skip_connections = []
        
        for i, hidden_size in enumerate(self.hidden_sizes):
            # LIF layer (simplified - would use actual SNN implementation)
            x_dense = nn.Dense(hidden_size)(x)
            
            # Apply leaky integration (simplified LIF dynamics)
            # In real implementation, this would be proper LIF neurons with Spyx
            x_lif = nn.tanh(x_dense)  # Placeholder for actual LIF dynamics
            
            # Dropout for regularization
            x_lif = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x_lif)
            
            # Skip connection for deeper networks
            if i > 0 and x.shape[-1] == hidden_size:
                x_lif = x_lif + x  # Residual connection
                
            x = x_lif
            skip_connections.append(x)
        
        # **NEW: Temporal attention pooling**
        # Instead of simple mean pooling, use attention to focus on important time steps
        temporal_weights = nn.Dense(1)(x)  # [batch, time, 1]
        temporal_weights = nn.softmax(temporal_weights, axis=1)
        
        # Weighted average over time
        x_pooled = jnp.sum(x * temporal_weights, axis=1)  # [batch, features]
        
        # **NEW: Multi-scale temporal features**
        # Add features from different temporal scales
        for skip in skip_connections[:-1]:
            skip_weights = nn.Dense(1)(skip)
            skip_weights = nn.softmax(skip_weights, axis=1)
            skip_pooled = jnp.sum(skip * skip_weights, axis=1)
            
            # Project to same dimension and add
            skip_proj = nn.Dense(x_pooled.shape[-1])(skip_pooled)
            x_pooled = x_pooled + 0.1 * skip_proj
        
        # Final classification layers
        x = nn.Dense(128)(x_pooled)
        x = nn.gelu(x)
        x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
        
        x = nn.Dense(64)(x)
        x = nn.gelu(x)
        x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)
        
        # Output layer
        logits = nn.Dense(self.num_classes)(x)
        
        return logits


def focal_loss(logits: jnp.ndarray, labels: jnp.ndarray, 
               alpha: float = 0.25, gamma: float = 2.0) -> jnp.ndarray:
    """
    Focal Loss for addressing class imbalance.
    
    Focuses learning on hard examples and down-weights easy examples.
    """
    # Convert to probabilities
    probs = nn.softmax(logits, axis=-1)
    
    # One-hot encode labels
    num_classes = logits.shape[-1]
    labels_onehot = jax.nn.one_hot(labels, num_classes)
    
    # Get probability of true class
    pt = jnp.sum(probs * labels_onehot, axis=-1)
    
    # Focal loss computation
    focal_weight = alpha * jnp.power(1 - pt, gamma)
    focal_loss_val = -focal_weight * jnp.log(pt + 1e-8)
    
    return jnp.mean(focal_loss_val)


def mixup_data(x: jnp.ndarray, y: jnp.ndarray, alpha: float = 0.2, 
               key: jnp.ndarray = None) -> Tuple[jnp.ndarray, jnp.ndarray, float]:
    """
    Mixup data augmentation for better generalization.
    """
    if key is None:
        key = jax.random.PRNGKey(42)
        
    batch_size = x.shape[0]
    
    # Sample lambda from Beta distribution
    lam = jax.random.beta(key, alpha, alpha)
    
    # Random permutation
    key, subkey = jax.random.split(key)
    indices = jax.random.permutation(subkey, batch_size)
    
    # Mix inputs
    mixed_x = lam * x + (1 - lam) * x[indices]
    
    # Mix labels (soft labels)
    y_onehot = jax.nn.one_hot(y, 3)  # 3 classes
    y_mixed_onehot = lam * y_onehot + (1 - lam) * y_onehot[indices]
    
    return mixed_x, y_mixed_onehot, lam


class AdvancedGWTrainer:
    """Advanced trainer with state-of-the-art techniques."""
    
    def __init__(self, config: AdvancedTrainingConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Enhanced models
        self.cpc_model = AttentionCPCEncoder(
            latent_dim=config.cpc_latent_dim,
            conv_channels=config.cpc_conv_channels,
            use_attention=config.use_attention
        )
        
        self.snn_model = DeepSNN(
            hidden_sizes=config.snn_hidden_sizes,
            num_classes=3
        )
        
        self.spike_bridge = SpikeBridge(
            encoding_strategy=config.spike_encoding,
            spike_time_steps=config.spike_time_steps
        )
        
        # **ADVANCED: AdamW optimizer with cosine annealing**
        if config.use_cosine_scheduling:
            lr_schedule = optax.cosine_decay_schedule(
                init_value=config.learning_rate,
                decay_steps=config.num_epochs * 20,  # Assume ~20 batches per epoch
                alpha=0.01  # Final LR = 1% of initial
            )
        else:
            lr_schedule = config.learning_rate
            
        self.cpc_optimizer = optax.adamw(
            learning_rate=lr_schedule,
            weight_decay=config.weight_decay
        )
        
        self.snn_optimizer = optax.adamw(
            learning_rate=lr_schedule,
            weight_decay=config.weight_decay
        )
        
        # Data generators
        self.continuous_generator = ContinuousGWGenerator(
            base_frequency=50.0,
            freq_range=(20.0, 500.0),  # Wider frequency range
            duration=config.signal_duration
        )
        
        logger.info("Initialized Advanced GW Trainer")
        logger.info(f"  Enhanced architecture: {config.cpc_conv_channels} -> {config.cpc_latent_dim}")
        logger.info(f"  Deep SNN: {config.snn_hidden_sizes}")
        logger.info(f"  Advanced techniques: attention={config.use_attention}, focal_loss={config.use_focal_loss}")
    
    def generate_enhanced_dataset(self) -> Dict:
        """Generate balanced, high-quality dataset."""
        logger.info("Generating advanced multi-signal dataset...")
        
        # **ENHANCED: Balanced dataset with quality control**
        continuous_data = self.continuous_generator.generate_training_dataset(
            num_signals=self.config.num_continuous_signals,
            signal_duration=self.config.signal_duration,
            include_noise_only=False
        )
        
        # Generate more realistic binary signals with parameter sweeps
        binary_data = self._generate_realistic_binary_signals()
        
        # Generate controlled noise samples
        noise_data = self._generate_controlled_noise()
        
        # Combine with careful balancing
        dataset = self._combine_balanced_datasets(continuous_data, binary_data, noise_data)
        
        logger.info(f"Advanced dataset: {dataset['data'].shape}")
        logger.info(f"Class distribution: {jnp.bincount(dataset['labels'])}")
        
        return dataset
    
    def _generate_realistic_binary_signals(self) -> Dict:
        """Generate more realistic binary merger signals."""
        logger.info(f"Generating {self.config.num_binary_signals} realistic binary signals...")
        
        all_data = []
        all_metadata = []
        
        # Parameter ranges from real GW events
        mass_ranges = [(10, 50), (20, 80), (30, 100)]  # Solar masses
        
        for i in range(self.config.num_binary_signals):
            # Varied parameters for diversity
            mass_range = mass_ranges[i % len(mass_ranges)]
            m1 = jax.random.uniform(jax.random.PRNGKey(i), minval=mass_range[0], maxval=mass_range[1])
            m2 = jax.random.uniform(jax.random.PRNGKey(i+1000), minval=mass_range[0], maxval=mass_range[1])
            
            # More realistic frequency evolution
            duration = self.config.signal_duration
            t = jnp.linspace(0, duration, int(duration * 4096))
            
            # Chirp mass calculation
            chirp_mass = jnp.power(m1 * m2, 3/5) / jnp.power(m1 + m2, 1/5)
            
            # More accurate frequency evolution
            tau = duration - t
            f_t = jnp.where(
                tau > 1e-3,
                jnp.power(256 * jnp.pi * chirp_mass * 1.989e30 * 6.674e-11 / (3 * 2.998e8**3), -3/8) * jnp.power(tau, -3/8) / (2 * jnp.pi),
                250.0  # Final frequency
            )
            
            # Amplitude with proper decay
            amplitude = 1e-21 / jnp.sqrt(1 + (t / (duration * 0.1))**2)
            
            # Generate waveform
            phase = jnp.cumsum(2 * jnp.pi * f_t) / 4096
            signal = amplitude * jnp.sin(phase)
            
            # Add realistic noise
            noise = jax.random.normal(jax.random.PRNGKey(i + 2000), signal.shape) * 1e-23
            binary_signal = signal + noise
            
            all_data.append(binary_signal)
            all_metadata.append({
                'signal_type': 'binary_merger',
                'm1': float(m1),
                'm2': float(m2),
                'chirp_mass': float(chirp_mass),
                'detector': 'H1'
            })
        
        return {
            'data': jnp.stack(all_data),
            'metadata': all_metadata
        }
    
    def _generate_controlled_noise(self) -> Dict:
        """Generate controlled noise samples with realistic PSD."""
        num_noise = self.config.num_noise_samples
        logger.info(f"Generating {num_noise} controlled noise samples...")
        
        noise_length = int(self.config.signal_duration * 4096)
        
        all_noise = []
        all_metadata = []
        
        for i in range(num_noise):
            # Generate noise with realistic LIGO PSD characteristics
            key = jax.random.PRNGKey(i + 3000)
            
            # Base Gaussian noise
            noise = jax.random.normal(key, (noise_length,)) * 1e-23
            
            # Add some colored noise characteristics (simplified)
            # Low-frequency noise enhancement
            freqs = jnp.fft.fftfreq(noise_length, 1/4096)
            noise_fft = jnp.fft.fft(noise)
            
            # Simple PSD model (1/f at low frequencies)
            psd_model = jnp.where(
                jnp.abs(freqs) < 20,  # Below 20 Hz
                10.0 / (jnp.abs(freqs) + 1),  # 1/f noise
                1.0  # White noise above 20 Hz
            )
            
            colored_noise_fft = noise_fft * jnp.sqrt(psd_model)
            colored_noise = jnp.real(jnp.fft.ifft(colored_noise_fft))
            
            all_noise.append(colored_noise)
            all_metadata.append({
                'signal_type': 'noise_only',
                'detector': 'H1'
            })
        
        return {
            'data': jnp.stack(all_noise),
            'metadata': all_metadata
        }
    
    def _combine_balanced_datasets(self, continuous_data: Dict, binary_data: Dict, noise_data: Dict) -> Dict:
        """Combine datasets with careful class balancing."""
        # Extract signal data only
        cont_data = continuous_data['data'][continuous_data['labels'] == 1]
        bin_data = binary_data['data']
        noise_data_array = noise_data['data']
        
        # Ensure balanced classes
        min_samples = min(len(cont_data), len(bin_data), len(noise_data_array))
        
        cont_data = cont_data[:min_samples]
        bin_data = bin_data[:min_samples]
        noise_data_array = noise_data_array[:min_samples]
        
        # Combine data
        all_data = jnp.concatenate([noise_data_array, cont_data, bin_data], axis=0)
        
        # Create balanced labels
        noise_labels = jnp.zeros(min_samples, dtype=jnp.int32)
        cont_labels = jnp.ones(min_samples, dtype=jnp.int32)
        bin_labels = jnp.ones(min_samples, dtype=jnp.int32) * 2
        
        all_labels = jnp.concatenate([noise_labels, cont_labels, bin_labels])
        
        # Combine metadata
        all_metadata = (
            noise_data['metadata'][:min_samples] +
            [m for m, l in zip(continuous_data['metadata'], continuous_data['labels']) if l == 1][:min_samples] +
            binary_data['metadata'][:min_samples]
        )
        
        # Shuffle dataset
        key = jax.random.PRNGKey(42)
        indices = jax.random.permutation(key, len(all_data))
        
        return {
            'data': all_data[indices],
            'labels': all_labels[indices],
            'metadata': [all_metadata[i] for i in indices],
            'signal_types': ['noise', 'continuous_gw', 'binary_merger'],
            'num_classes': 3
        }


def run_advanced_training_experiment():
    """Test advanced training pipeline."""
    print("ðŸš€ Advanced Training Experiment")
    
    config = AdvancedTrainingConfig(
        num_continuous_signals=20,
        num_binary_signals=20,
        num_noise_samples=20,
        batch_size=8,
        num_epochs=5,  # Quick test
        use_attention=True,
        use_focal_loss=True
    )
    
    trainer = AdvancedGWTrainer(config)
    dataset = trainer.generate_enhanced_dataset()
    
    print(f"âœ… Advanced dataset: {dataset['data'].shape}")
    print(f"âœ… Class distribution: {jnp.bincount(dataset['labels'])}")
    return True


if __name__ == "__main__":
    import os
    os.environ['JAX_PLATFORM_NAME'] = 'cpu'
    success = run_advanced_training_experiment()
    print("âœ… Advanced training ready!" if success else "âŒ Setup failed!")
```

## training/enhanced_gw_training.py

```python
#!/usr/bin/env python3

"""
Enhanced Gravitational Wave Training Pipeline

Combines continuous GW signals (PyFstat) + binary GW signals (GWOSC)
for comprehensive neuromorphic CPC+SNN detector training.

Represents next-generation multi-signal type GW detection.
"""

import jax
import jax.numpy as jnp
import optax
import logging
import time
from typing import Dict, Tuple, Optional, List
from pathlib import Path
from dataclasses import dataclass

from ..models.cpc_encoder import create_enhanced_cpc_encoder, enhanced_info_nce_loss
from ..models.simple_snn import create_simple_snn, SimpleSNNTrainer
from ..models.spike_bridge import SpikeBridge, SpikeEncodingStrategy
from ..data.continuous_gw_generator import ContinuousGWGenerator, create_mixed_gw_dataset
from ..data.gw_download import ProductionGWOSCDownloader

logger = logging.getLogger(__name__)


@dataclass
class EnhancedTrainingConfig:
    """Configuration for enhanced GW training."""
    # Data configuration
    num_continuous_signals: int = 200
    num_binary_signals: int = 200
    signal_duration: float = 4.0  # seconds
    mix_ratio: float = 0.5  # Ratio continuous:binary
    
    # Training configuration
    batch_size: int = 16
    learning_rate: float = 1e-3
    num_epochs: int = 50
    
    # Model configuration
    cpc_latent_dim: int = 128
    snn_hidden_size: int = 64
    spike_time_steps: int = 50
    
    # Spike encoding
    spike_encoding: SpikeEncodingStrategy = SpikeEncodingStrategy.POISSON_RATE
    spike_rate: float = 100.0  # Hz
    
    # Output
    output_dir: str = "enhanced_gw_training_outputs"
    save_models: bool = True
    

class EnhancedGWTrainer:
    """
    Enhanced Gravitational Wave Trainer.
    
    Trains CPC+SNN neuromorphic detector on combined
    continuous + binary gravitational wave signals.
    """
    
    def __init__(self, config: EnhancedTrainingConfig):
        """
        Initialize enhanced GW trainer.
        
        Args:
            config: Training configuration
        """
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize data generators
        self.continuous_generator = ContinuousGWGenerator(
            base_frequency=50.0,
            freq_range=(20.0, 200.0),
            duration=config.signal_duration
        )
        
        self.binary_downloader = ProductionGWOSCDownloader()
        
        # Initialize models
        self.cpc_model = create_enhanced_cpc_encoder(
            latent_dim=config.cpc_latent_dim,
            use_batch_norm=False,  # Simplified for now
            dropout_rate=0.0
        )
        
        self.snn_model = create_simple_snn(
            hidden_size=config.snn_hidden_size,
            num_classes=3  # 3 classes: noise, continuous, binary
        )
        
        self.spike_bridge = SpikeBridge(
            encoding_strategy=config.spike_encoding,
            spike_time_steps=config.spike_time_steps,
            max_spike_rate=config.spike_rate
        )
        
        # Optimizers
        self.cpc_optimizer = optax.adam(config.learning_rate)
        self.snn_trainer = SimpleSNNTrainer(config.learning_rate)
        
        logger.info("Initialized Enhanced GW Trainer")
        logger.info(f"  Output directory: {self.output_dir}")
        logger.info(f"  Signal types: continuous + binary")
        logger.info(f"  Mix ratio: {config.mix_ratio}")
        
    def generate_enhanced_dataset(self) -> Dict:
        """
        Generate enhanced dataset with continuous + binary + noise signals.
        
        Returns:
            Enhanced dataset with multiple signal types
        """
        logger.info("Generating enhanced multi-signal dataset...")
        
        # Generate continuous signals
        continuous_data = self.continuous_generator.generate_training_dataset(
            num_signals=self.config.num_continuous_signals,
            signal_duration=self.config.signal_duration,
            include_noise_only=False  # Handle noise separately
        )
        
        # Generate synthetic binary signals (simplified)
        binary_data = self._generate_synthetic_binary_signals()
        
        # Generate pure noise samples
        noise_data = self._generate_noise_samples()
        
        # Combine all data
        enhanced_dataset = self._combine_datasets(continuous_data, binary_data, noise_data)
        
        logger.info(f"Enhanced dataset generated:")
        logger.info(f"  Total samples: {enhanced_dataset['data'].shape[0]}")
        logger.info(f"  Signal types: {jnp.unique(enhanced_dataset['labels'])}")
        logger.info(f"  Data shape: {enhanced_dataset['data'].shape}")
        
        return enhanced_dataset
    
    def _generate_synthetic_binary_signals(self) -> Dict:
        """Generate synthetic binary GW signals (simplified chirp model)."""
        logger.info(f"Generating {self.config.num_binary_signals} synthetic binary signals...")
        
        all_data = []
        all_metadata = []
        
        for i in range(self.config.num_binary_signals):
            # Simple chirp parameters
            f_start = jnp.array(35.0 + 10 * jax.random.uniform(jax.random.PRNGKey(i)))
            f_end = jnp.array(250.0)
            duration = self.config.signal_duration
            
            # Generate chirp signal
            t = jnp.linspace(0, duration, int(duration * 4096))
            
            # Frequency evolution (simplified)
            alpha = (f_end - f_start) / duration
            freq_t = f_start + alpha * t
            
            # Chirp signal
            signal = 1e-21 * jnp.sin(2 * jnp.pi * jnp.cumsum(freq_t) / 4096)
            
            # Add noise
            noise = jax.random.normal(jax.random.PRNGKey(i + 1000), signal.shape) * 1e-23
            binary_signal = signal + noise
            
            all_data.append(binary_signal)
            all_metadata.append({
                'signal_type': 'binary_merger',
                'f_start': float(f_start),
                'f_end': float(f_end),
                'detector': 'H1'
            })
        
        return {
            'data': jnp.stack(all_data),
            'metadata': all_metadata
        }
    
    def _generate_noise_samples(self) -> Dict:
        """Generate pure noise samples."""
        num_noise = self.config.num_continuous_signals + self.config.num_binary_signals
        logger.info(f"Generating {num_noise} pure noise samples...")
        
        noise_length = int(self.config.signal_duration * 4096)
        
        all_noise = []
        all_metadata = []
        
        for i in range(num_noise):
            noise = jax.random.normal(jax.random.PRNGKey(i + 2000), (noise_length,)) * 1e-23
            
            all_noise.append(noise)
            all_metadata.append({
                'signal_type': 'noise_only',
                'detector': 'H1'
            })
        
        return {
            'data': jnp.stack(all_noise),
            'metadata': all_metadata
        }
    
    def _combine_datasets(self, continuous_data: Dict, binary_data: Dict, noise_data: Dict) -> Dict:
        """Combine continuous, binary, and noise datasets."""
        # Extract data arrays
        cont_data = continuous_data['data'][continuous_data['labels'] == 1]  # Only signals
        bin_data = binary_data['data']
        noise_data_array = noise_data['data']
        
        # Combine data
        all_data = jnp.concatenate([cont_data, bin_data, noise_data_array], axis=0)
        
        # Create labels: 0=noise, 1=continuous, 2=binary
        cont_labels = jnp.ones(cont_data.shape[0], dtype=jnp.int32)  # Continuous
        bin_labels = jnp.ones(bin_data.shape[0], dtype=jnp.int32) * 2  # Binary
        noise_labels = jnp.zeros(noise_data_array.shape[0], dtype=jnp.int32)  # Noise
        
        all_labels = jnp.concatenate([cont_labels, bin_labels, noise_labels])
        
        # Combine metadata
        all_metadata = (
            [m for m, l in zip(continuous_data['metadata'], continuous_data['labels']) if l == 1] +
            binary_data['metadata'] + 
            noise_data['metadata']
        )
        
        # Shuffle data
        key = jax.random.PRNGKey(42)
        indices = jax.random.permutation(key, len(all_data))
        
        return {
            'data': all_data[indices],
            'labels': all_labels[indices],
            'metadata': [all_metadata[i] for i in indices],
            'signal_types': ['noise', 'continuous_gw', 'binary_merger'],
            'num_classes': 3
        }
    
    def train_cpc_encoder(self, dataset: Dict, num_epochs: int = 20) -> Dict:
        """
        Train CPC encoder on enhanced dataset.
        
        Args:
            dataset: Enhanced multi-signal dataset
            num_epochs: Number of training epochs
            
        Returns:
            Training results and final parameters
        """
        logger.info("Training CPC encoder on enhanced dataset...")
        
        # Initialize parameters
        key = jax.random.PRNGKey(42)
        dummy_input = jnp.ones((1, int(self.config.signal_duration * 4096)))
        cpc_params = self.cpc_model.init(key, dummy_input)
        opt_state = self.cpc_optimizer.init(cpc_params)
        
        # Training loop
        losses = []
        
        for epoch in range(num_epochs):
            epoch_losses = []
            
            # Batch training
            data = dataset['data']
            num_batches = len(data) // self.config.batch_size
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * self.config.batch_size
                end_idx = start_idx + self.config.batch_size
                batch_data = data[start_idx:end_idx]
                
                # Training step
                cpc_params, opt_state, loss = self._cpc_training_step(
                    cpc_params, opt_state, batch_data, jax.random.PRNGKey(epoch * 1000 + batch_idx)
                )
                
                epoch_losses.append(loss)
            
            avg_loss = jnp.mean(jnp.array(epoch_losses))
            losses.append(avg_loss)
            
            if epoch % 5 == 0:
                logger.info(f"CPC Epoch {epoch}: Loss = {avg_loss:.4f}")
        
        logger.info(f"CPC training completed. Final loss: {losses[-1]:.4f}")
        
        return {
            'cpc_params': cpc_params,
            'losses': losses,
            'epochs': num_epochs
        }
    
    def _cpc_training_step(self, params, opt_state, batch, key):
        """Single CPC training step."""
        def loss_fn(params):
            # Encode batch
            encoded = self.cpc_model.apply(params, batch)
            
            # Create context-target pairs for contrastive learning
            context_len = encoded.shape[1] // 2
            context = encoded[:, :context_len]
            targets = encoded[:, context_len:]
            
            # Enhanced InfoNCE loss with proper temperature parameter
            loss = enhanced_info_nce_loss(context, targets, temperature=0.1)
            return loss
        
        loss, grads = jax.value_and_grad(loss_fn)(params)
        updates, opt_state = self.cpc_optimizer.update(grads, opt_state)
        params = optax.apply_updates(params, updates)
        
        return params, opt_state, loss
    
    def train_snn_classifier(self, dataset: Dict, cpc_params: Dict, num_epochs: int = 30) -> Dict:
        """
        Train SNN classifier on CPC-encoded spike trains.
        
        Args:
            dataset: Enhanced dataset
            cpc_params: Trained CPC parameters
            num_epochs: Number of training epochs
            
        Returns:
            Training results and SNN parameters
        """
        logger.info("Training SNN classifier on spike-encoded features...")
        
        # Initialize SNN parameters
        key = jax.random.PRNGKey(123)
        dummy_spikes = jnp.ones((1, self.config.spike_time_steps, self.config.cpc_latent_dim))
        snn_params = self.snn_model.init(key, dummy_spikes)
        opt_state = self.snn_trainer.optimizer.init(snn_params)
        
        # Encode all data through CPC
        logger.info("Encoding data through trained CPC...")
        cpc_features = self.cpc_model.apply(cpc_params, dataset['data'])
        
        # Convert to spike trains
        logger.info("Converting to spike trains...")
        spike_trains = []
        for i in range(len(cpc_features)):
            spikes = self.spike_bridge.encode(
                cpc_features[i:i+1], jax.random.PRNGKey(i)
            )
            spike_trains.append(spikes[0])  # Remove batch dimension
        
        spike_data = jnp.stack(spike_trains)
        labels = dataset['labels']
        
        logger.info(f"Spike data shape: {spike_data.shape}")
        
        # Training loop
        accuracies = []
        losses = []
        
        for epoch in range(num_epochs):
            epoch_losses = []
            epoch_accs = []
            
            # Batch training
            num_batches = len(spike_data) // self.config.batch_size
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * self.config.batch_size
                end_idx = start_idx + self.config.batch_size
                
                batch_spikes = spike_data[start_idx:end_idx]
                batch_labels = labels[start_idx:end_idx]
                
                # Training step
                snn_params, opt_state, loss = self.snn_trainer.training_step(
                    snn_params, opt_state, batch_spikes, batch_labels, self.snn_model
                )
                
                # Accuracy
                acc = self.snn_trainer.accuracy(
                    snn_params, batch_spikes, batch_labels, self.snn_model
                )
                
                epoch_losses.append(loss)
                epoch_accs.append(acc)
            
            avg_loss = jnp.mean(jnp.array(epoch_losses))
            avg_acc = jnp.mean(jnp.array(epoch_accs))
            
            losses.append(avg_loss)
            accuracies.append(avg_acc)
            
            if epoch % 5 == 0:
                logger.info(f"SNN Epoch {epoch}: Loss = {avg_loss:.4f}, Acc = {avg_acc:.3f}")
        
        logger.info(f"SNN training completed. Final accuracy: {accuracies[-1]:.3f}")
        
        return {
            'snn_params': snn_params,
            'losses': losses,
            'accuracies': accuracies,
            'epochs': num_epochs,
            'final_accuracy': float(accuracies[-1])
        }
    
    def evaluate_multi_signal_performance(self, dataset: Dict, cpc_params: Dict, snn_params: Dict) -> Dict:
        """
        Evaluate performance on different signal types.
        
        Args:
            dataset: Test dataset
            cpc_params: Trained CPC parameters  
            snn_params: Trained SNN parameters
            
        Returns:
            Performance metrics by signal type
        """
        logger.info("Evaluating multi-signal performance...")
        
        # Encode through full pipeline
        cpc_features = self.cpc_model.apply(cpc_params, dataset['data'])
        
        spike_trains = []
        for i in range(len(cpc_features)):
            spikes = self.spike_bridge.encode(
                cpc_features[i:i+1], jax.random.PRNGKey(i + 5000)
            )
            spike_trains.append(spikes[0])
        
        spike_data = jnp.stack(spike_trains)
        
        # Get predictions
        logits = self.snn_model.apply(snn_params, spike_data)
        predictions = jnp.argmax(logits, axis=1)
        true_labels = dataset['labels']
        
        # Compute metrics by signal type
        results = {}
        for signal_type_idx, signal_type in enumerate(dataset['signal_types']):
            mask = true_labels == signal_type_idx
            if jnp.sum(mask) > 0:
                type_predictions = predictions[mask]
                type_labels = true_labels[mask]
                accuracy = jnp.mean(type_predictions == type_labels)
                
                results[signal_type] = {
                    'accuracy': float(accuracy),
                    'num_samples': int(jnp.sum(mask)),
                    'true_positives': int(jnp.sum((type_predictions == signal_type_idx) & (type_labels == signal_type_idx)))
                }
        
        # Overall accuracy
        overall_accuracy = jnp.mean(predictions == true_labels)
        results['overall'] = {
            'accuracy': float(overall_accuracy),
            'num_samples': len(true_labels)
        }
        
        logger.info("Multi-signal performance evaluation:")
        for signal_type, metrics in results.items():
            logger.info(f"  {signal_type}: {metrics['accuracy']:.3f} ({metrics['num_samples']} samples)")
        
        return results
    
    def run_enhanced_training(self) -> Dict:
        """
        Run complete enhanced training pipeline.
        
        Returns:
            Complete training results
        """
        logger.info("ðŸš€ Starting Enhanced GW Training Pipeline...")
        start_time = time.time()
        
        # Generate enhanced dataset
        dataset = self.generate_enhanced_dataset()
        
        # Split into train/test (80/20)
        split_idx = int(0.8 * len(dataset['data']))
        train_data = {
            'data': dataset['data'][:split_idx],
            'labels': dataset['labels'][:split_idx],
            'metadata': dataset['metadata'][:split_idx],
            'signal_types': dataset['signal_types'],
            'num_classes': dataset['num_classes']
        }
        
        test_data = {
            'data': dataset['data'][split_idx:],
            'labels': dataset['labels'][split_idx:],
            'metadata': dataset['metadata'][split_idx:],
            'signal_types': dataset['signal_types'],
            'num_classes': dataset['num_classes']
        }
        
        # Train CPC encoder
        cpc_results = self.train_cpc_encoder(train_data, num_epochs=20)
        
        # Train SNN classifier
        snn_results = self.train_snn_classifier(train_data, cpc_results['cpc_params'], num_epochs=30)
        
        # Evaluate performance
        performance = self.evaluate_multi_signal_performance(
            test_data, cpc_results['cpc_params'], snn_results['snn_params']
        )
        
        # Save models if requested
        if self.config.save_models:
            self._save_models(cpc_results['cpc_params'], snn_results['snn_params'])
        
        total_time = time.time() - start_time
        
        final_results = {
            'cpc_training': cpc_results,
            'snn_training': snn_results,
            'performance': performance,
            'training_time': total_time,
            'config': self.config
        }
        
        logger.info(f"ðŸŽ‰ Enhanced Training Completed!")
        logger.info(f"  Total time: {total_time:.1f} seconds")
        logger.info(f"  Overall accuracy: {performance['overall']['accuracy']:.3f}")
        logger.info(f"  CPC final loss: {cpc_results['losses'][-1]:.4f}")
        logger.info(f"  SNN final accuracy: {snn_results['final_accuracy']:.3f}")
        
        return final_results
    
    def _save_models(self, cpc_params: Dict, snn_params: Dict):
        """Save trained model parameters."""
        import pickle
        
        models_dir = self.output_dir / "models"
        models_dir.mkdir(exist_ok=True)
        
        # Save CPC parameters
        with open(models_dir / "cpc_params.pkl", "wb") as f:
            pickle.dump(cpc_params, f)
        
        # Save SNN parameters  
        with open(models_dir / "snn_params.pkl", "wb") as f:
            pickle.dump(snn_params, f)
        
        logger.info(f"Models saved to {models_dir}")


def run_enhanced_gw_training_experiment():
    """Quick test of enhanced training."""
    print("ðŸŒŸ Enhanced GW Training Test")
    
    config = EnhancedTrainingConfig(
        num_continuous_signals=5,  # Small test
        num_binary_signals=5,
        signal_duration=2.0,  # Shorter for testing
        batch_size=4,
        num_epochs=2
    )
    
    trainer = EnhancedGWTrainer(config)
    dataset = trainer.generate_enhanced_dataset()
    
    print(f"âœ… Dataset generated: {dataset['data'].shape}")
    print(f"âœ… Signal types: {dataset['signal_types']}")
    return True


if __name__ == "__main__":
    import os
    os.environ['JAX_PLATFORMS'] = 'cpu'
    success = run_enhanced_gw_training_experiment()
    print("âœ… Test completed!" if success else "âŒ Test failed!")
```

## training/pretrain_cpc.py

```python
#!/usr/bin/env python3
"""
CPC Encoder Pretraining Script

Self-supervised contrastive pretraining dla gravitational wave detection.
Implements InfoNCE loss with gradient accumulation dla Apple Silicon optimization.

Usage:
    python pretrain_cpc.py --config config.yaml --num_steps 100000
"""

import jax
import jax.numpy as jnp
import flax.linen as nn
import optax
import orbax.checkpoint as ocp
import logging
import time
import argparse
from pathlib import Path
from typing import Dict, Any, Tuple, List
from dataclasses import dataclass, asdict
import yaml
import wandb

from ..models.cpc_encoder import CPCEncoder, info_nce_loss
from ..data.gw_download import ProductionGWOSCDownloader, AdvancedDataPreprocessor

logger = logging.getLogger(__name__)


@dataclass
class CPCTrainingConfig:
    """Configuration dla CPC pretraining."""
    # Architecture
    latent_dim: int = 256
    downsample_factor: int = 16
    conv_channels: Tuple[int, ...] = (32, 64, 128)
    
    # Training  
    num_steps: int = 100_000
    batch_size: int = 16
    accumulation_steps: int = 4  # Gradient accumulation dla Metal limitations
    learning_rate: float = 1e-3
    warmup_steps: int = 5_000
    
    # InfoNCE Loss
    context_length: int = 12  # Context windows dla prediction
    prediction_length: int = 4  # Future windows to predict
    num_negatives: int = 128
    temperature: float = 0.1
    
    # Data
    segment_duration: float = 4.0  # seconds
    sample_rate: int = 4096
    detectors: List[str] = None  # Will default to ['H1', 'L1']
    
    # Monitoring
    log_every: int = 100
    eval_every: int = 1000
    save_every: int = 5000
    
    # Paths
    output_dir: str = "experiments/cpc_pretraining"
    wandb_project: str = "ligo-cpc-snn"
    
    def __post_init__(self):
        if self.detectors is None:
            self.detectors = ['H1', 'L1']


class CPCTrainingState:
    """Training state dla CPC pretraining."""
    
    def __init__(self, config: CPCTrainingConfig):
        self.config = config
        self.step = 0
        self.epoch = 0
        self.best_loss = float('inf')
        
        # Initialize model
        self.model = CPCEncoder(
            latent_dim=config.latent_dim,
            conv_channels=config.conv_channels
        )
        
        # Initialize optimizer z warmup schedule
        self.scheduler = optax.warmup_cosine_decay_schedule(
            init_value=0.0,
            peak_value=config.learning_rate,
            warmup_steps=config.warmup_steps,
            decay_steps=config.num_steps - config.warmup_steps
        )
        
        self.optimizer = optax.adam(learning_rate=self.scheduler)
        
        # Initialize data pipeline
        self.downloader = ProductionGWOSCDownloader()
        self.preprocessor = AdvancedDataPreprocessor(
            sample_rate=config.sample_rate,
            apply_whitening=True
        )
        
        # Metrics tracking
        self.metrics = {
            'train_loss': [],
            'learning_rate': [],
            'gradient_norm': [],
            'processing_time': [],
            'examples_per_sec': []
        }
        
    def initialize_params(self, key: jnp.ndarray, input_shape: Tuple[int, ...]) -> Dict[str, Any]:
        """Initialize model parameters."""
        dummy_input = jnp.zeros((1,) + input_shape)
        return self.model.init(key, dummy_input)
        
    def create_training_segments(self, strain_data: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """
        Create context-target pairs dla contrastive learning.
        
        Args:
            strain_data: Input strain timeseries [batch, time]
            
        Returns:
            context_segments: Context windows [batch, context_length, features]
            target_segments: Target windows dla prediction [batch, prediction_length, features]
        """
        batch_size, seq_len = strain_data.shape
        
        # Encode full sequence
        encoded = self.model.apply(self.params, strain_data)  # [batch, downsampled_time, latent_dim]
        _, encoded_len, latent_dim = encoded.shape
        
        # Create sliding windows
        total_length = self.config.context_length + self.config.prediction_length
        
        if encoded_len < total_length:
            # Pad if sequence too short
            pad_length = total_length - encoded_len
            encoded = jnp.pad(encoded, ((0, 0), (0, pad_length), (0, 0)))
            encoded_len = total_length
            
        # Random starting positions
        max_start = encoded_len - total_length
        if max_start <= 0:
            start_idx = 0
        else:
            start_idx = jax.random.randint(
                jax.random.PRNGKey(self.step), (), 0, max_start
            )
        
        # Extract context and target windows
        context_end = start_idx + self.config.context_length
        target_end = context_end + self.config.prediction_length
        
        context_segments = encoded[:, start_idx:context_end, :]  # [batch, context_length, latent_dim]
        target_segments = encoded[:, context_end:target_end, :]  # [batch, prediction_length, latent_dim]
        
        return context_segments, target_segments


@jax.jit
def train_step(state: Dict[str, Any], 
              strain_batch: jnp.ndarray,
              key: jnp.ndarray) -> Tuple[Dict[str, Any], Dict[str, float]]:
    """
    Single training step z gradient accumulation.
    
    Args:
        state: Training state dict {params, opt_state}
        strain_batch: Batch of strain data [batch, time]
        key: Random key dla training
        
    Returns:
        Updated state and metrics
    """
    params, opt_state = state['params'], state['opt_state']
    
    def loss_fn(params, strain_data, key):
        # Forward pass przez CPC encoder
        encoded = model.apply(params, strain_data)
        
        # Create context-target pairs
        batch_size, seq_len, latent_dim = encoded.shape
        context_length = 12
        prediction_length = 4
        
        # Simple sliding window approach
        context = encoded[:, :context_length, :]
        target = encoded[:, context_length:context_length+prediction_length, :]
        
        # InfoNCE loss
        return info_nce_loss(context, target, temperature=0.1)
    
    # Compute loss and gradients
    loss, grads = jax.value_and_grad(loss_fn)(params, strain_batch, key)
    
    # Gradient norm dla monitoring
    grad_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree.leaves(grads)))
    
    # Update parameters
    updates, opt_state = optimizer.update(grads, opt_state)
    params = optax.apply_updates(params, updates)
    
    updated_state = {'params': params, 'opt_state': opt_state}
    metrics = {'loss': loss, 'grad_norm': grad_norm}
    
    return updated_state, metrics


class CPCPretrainer:
    """Main pretraining orchestrator dla CPC encoder."""
    
    def __init__(self, config: CPCTrainingConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup logging
        self._setup_logging()
        
        # Initialize training state
        self.training_state = CPCTrainingState(config)
        
        # Setup checkpointing
        self.checkpointer = ocp.StandardCheckpointer()
        
        # Setup W&B logging
        if config.wandb_project:
            wandb.init(
                project=config.wandb_project,
                config=asdict(config),
                name=f"cpc_pretraining_{int(time.time())}"
            )
            
    def _setup_logging(self):
        """Setup file and console logging."""
        log_file = self.output_dir / "training.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
    def generate_training_data(self) -> List[jnp.ndarray]:
        """
        Generate training dataset from historical GW events.
        
        Returns:
            List of preprocessed strain segments
        """
        logger.info("Generating training dataset from GWOSC...")
        
        # Historical GW events dla training
        training_events = [
            # Major detections with clear signals
            ('H1', 1126259446, 4.0),  # GW150914
            ('L1', 1126259446, 4.0),  # GW150914
            ('H1', 1128678900, 4.0),  # GW151012
            ('L1', 1128678900, 4.0),  # GW151012  
            ('H1', 1135136350, 4.0),  # GW151226
            ('L1', 1135136350, 4.0),  # GW151226
            ('H1', 1167559936, 4.0),  # GW170104
            ('L1', 1167559936, 4.0),  # GW170104
            ('H1', 1180922494, 4.0),  # GW170608
            ('L1', 1180922494, 4.0),  # GW170608
        ]
        
        # Download data in batches
        logger.info(f"Downloading {len(training_events)} training segments...")
        
        strain_segments = self.training_state.downloader.fetch_batch(
            training_events, max_workers=4
        )
        
        # Preprocess all segments
        processed_segments = []
        successful = 0
        
        for i, strain_data in enumerate(strain_segments):
            if strain_data is not None:
                try:
                    result = self.training_state.preprocessor.process(strain_data)
                    
                    if result.quality.is_valid and result.quality.snr_estimate > 5.0:
                        processed_segments.append(result.strain_data)
                        successful += 1
                        
                except Exception as e:
                    logger.warning(f"Failed to process segment {i}: {e}")
                    
        logger.info(f"Successfully processed {successful}/{len(training_events)} segments")
        
        return processed_segments
        
    def train(self):
        """Main training loop dla CPC pretraining."""
        logger.info("ðŸš€ Starting CPC Pretraining")
        logger.info("=" * 60)
        logger.info(f"Config: {asdict(self.config)}")
        
        # Generate training data
        training_data = self.generate_training_data()
        
        if len(training_data) < 4:
            raise ValueError(f"Insufficient training data: {len(training_data)} segments")
            
        # Initialize model parameters
        key = jax.random.PRNGKey(42)
        input_shape = (int(self.config.segment_duration * self.config.sample_rate),)
        
        params = self.training_state.initialize_params(key, input_shape)
        opt_state = self.training_state.optimizer.init(params)
        
        state = {'params': params, 'opt_state': opt_state}
        
        # Training loop
        logger.info(f"Training dla {self.config.num_steps} steps...")
        
        start_time = time.perf_counter()
        
        for step in range(self.config.num_steps):
            step_start = time.perf_counter()
            
            # Sample random batch from training data
            batch_indices = jax.random.choice(
                jax.random.PRNGKey(step), 
                len(training_data), 
                (self.config.batch_size,),
                replace=True
            )
            
            strain_batch = jnp.stack([training_data[i] for i in batch_indices])
            
            # Training step
            key = jax.random.PRNGKey(step)
            state, metrics = train_step(state, strain_batch, key)
            
            step_time = time.perf_counter() - step_start
            
            # Logging
            if step % self.config.log_every == 0:
                lr = self.training_state.scheduler(step)
                examples_per_sec = self.config.batch_size / step_time
                
                logger.info(
                    f"Step {step:6d}: loss={metrics['loss']:.4f}, "
                    f"grad_norm={metrics['grad_norm']:.4f}, "
                    f"lr={lr:.2e}, "
                    f"examples/sec={examples_per_sec:.1f}"
                )
                
                # W&B logging
                if wandb.run:
                    wandb.log({
                        'train/loss': metrics['loss'],
                        'train/grad_norm': metrics['grad_norm'],
                        'train/learning_rate': lr,
                        'train/examples_per_sec': examples_per_sec,
                        'step': step
                    })
                    
            # Checkpointing
            if step % self.config.save_every == 0 and step > 0:
                checkpoint_path = self.output_dir / f"checkpoint_{step}"
                
                self.checkpointer.save(
                    checkpoint_path,
                    {'params': state['params'], 'step': step}
                )
                
                logger.info(f"Saved checkpoint at step {step}")
                
        total_time = time.perf_counter() - start_time
        
        # Final save
        final_path = self.output_dir / "final_checkpoint"
        self.checkpointer.save(
            final_path,
            {'params': state['params'], 'step': self.config.num_steps}
        )
        
        logger.info("ðŸŽ‰ CPC Pretraining completed!")
        logger.info(f"Total training time: {total_time/3600:.2f} hours")
        logger.info(f"Final checkpoint saved to: {final_path}")
        
        if wandb.run:
            wandb.finish()


def main():
    """Command-line interface dla CPC pretraining."""
    parser = argparse.ArgumentParser(description="CPC Encoder Pretraining")
    parser.add_argument("--config", type=str, help="Path to config YAML file")
    parser.add_argument("--num_steps", type=int, default=100_000, help="Number of training steps")
    parser.add_argument("--batch_size", type=int, default=16, help="Batch size")
    parser.add_argument("--learning_rate", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--output_dir", type=str, default="experiments/cpc_pretraining", help="Output directory")
    
    args = parser.parse_args()
    
    # Load config
    if args.config and Path(args.config).exists():
        with open(args.config, 'r') as f:
            config_dict = yaml.safe_load(f)
        config = CPCTrainingConfig(**config_dict)
    else:
        config = CPCTrainingConfig()
        
    # Override z command line args
    if args.num_steps:
        config.num_steps = args.num_steps
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.learning_rate:
        config.learning_rate = args.learning_rate
    if args.output_dir:
        config.output_dir = args.output_dir
        
    # Initialize and run training
    pretrainer = CPCPretrainer(config)
    pretrainer.train()


if __name__ == "__main__":
    main()
```

## utils/__init__.py

```python
"""
Utilities for CPC+SNN Neuromorphic GW Detection

Production-ready utilities following ML4GW standards.
"""

import logging
import sys
from pathlib import Path
from typing import Optional, Union

import yaml
import jax
import jax.numpy as jnp


def setup_logging(
    level: int = logging.INFO,
    log_file: Optional[Union[str, Path]] = None,
    format_string: Optional[str] = None
) -> None:
    """Setup production-ready logging configuration.
    
    Args:
        level: Logging level (e.g., logging.INFO, logging.DEBUG)
        log_file: Optional file path for log output
        format_string: Custom log format string
    """
    if format_string is None:
        format_string = (
            "[%(asctime)s] %(levelname)s [%(name)s:%(lineno)d] %(message)s"
        )
    
    # Create formatter
    formatter = logging.Formatter(format_string)
    
    # Clear any existing handlers
    root_logger = logging.getLogger()
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(level)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # File handler (if specified)
    if log_file is not None:
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        file_handler = logging.FileHandler(log_path)
        file_handler.setLevel(level)
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)
    
    # Set root level
    root_logger.setLevel(level)
    
    # Set JAX logging level to reduce noise
    jax_logger = logging.getLogger("jax")
    jax_logger.setLevel(logging.WARNING)


def load_config(config_path: Union[str, Path]) -> dict:
    """Load YAML configuration file.
    
    Args:
        config_path: Path to YAML configuration file
        
    Returns:
        Configuration dictionary
        
    Raises:
        FileNotFoundError: If config file doesn't exist
        yaml.YAMLError: If config file is invalid
    """
    config_path = Path(config_path)
    
    if not config_path.exists():
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    return config or {}


def save_config(config: dict, output_path: Union[str, Path]) -> None:
    """Save configuration dictionary to YAML file.
    
    Args:
        config: Configuration dictionary to save
        output_path: Output file path
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, sort_keys=False)


def get_jax_device_info() -> dict:
    """Get JAX device information for logging and debugging.
    
    Returns:
        Dictionary with device information
    """
    devices = jax.devices()
    
    device_info = {
        'num_devices': len(devices),
        'devices': [
            {
                'id': i,
                'device_kind': str(device.device_kind),
                'platform': str(device.platform),
            }
            for i, device in enumerate(devices)
        ],
        'default_backend': jax.default_backend(),
    }
    
    # Try to get memory info (may not be available on all platforms)
    try:
        if devices and hasattr(devices[0], 'memory_stats'):
            memory_stats = devices[0].memory_stats()
            device_info['memory_info'] = memory_stats
    except Exception:
        pass
    
    return device_info


def print_system_info() -> None:
    """Print system and JAX configuration information."""
    logger = logging.getLogger(__name__)
    
    # JAX information
    device_info = get_jax_device_info()
    
    logger.info("ðŸ–¥ï¸  System Information:")
    logger.info(f"   JAX backend: {device_info['default_backend']}")
    logger.info(f"   Available devices: {device_info['num_devices']}")
    
    for device in device_info['devices']:
        logger.info(
            f"     Device {device['id']}: {device['device_kind']} "
            f"({device['platform']})"
        )
    
    if 'memory_info' in device_info:
        memory_info = device_info['memory_info'] 
        if 'bytes_in_use' in memory_info:
            memory_gb = memory_info['bytes_in_use'] / (1024**3)
            logger.info(f"   Memory in use: {memory_gb:.2f} GB")


def validate_array_shape(
    array: jnp.ndarray, 
    expected_shape: tuple,
    array_name: str = "array"
) -> None:
    """Validate array shape matches expected shape.
    
    Args:
        array: Input array to validate
        expected_shape: Expected shape tuple (use None for flexible dimensions)
        array_name: Name of array for error messages
        
    Raises:
        ValueError: If shape doesn't match
    """
    actual_shape = array.shape
    
    if len(actual_shape) != len(expected_shape):
        raise ValueError(
            f"{array_name} has {len(actual_shape)} dimensions, "
            f"expected {len(expected_shape)}"
        )
    
    for i, (actual, expected) in enumerate(zip(actual_shape, expected_shape)):
        if expected is not None and actual != expected:
            raise ValueError(
                f"{array_name} dimension {i} has size {actual}, "
                f"expected {expected}"
            )


def create_directory_structure(base_path: Union[str, Path], 
                             subdirs: list[str]) -> Path:
    """Create standardized directory structure for ML4GW projects.
    
    Args:
        base_path: Base directory path
        subdirs: List of subdirectory names to create
        
    Returns:
        Path to created base directory
    """
    base_path = Path(base_path)
    base_path.mkdir(parents=True, exist_ok=True)
    
    for subdir in subdirs:
        (base_path / subdir).mkdir(exist_ok=True)
    
    return base_path


# Standard ML4GW project structure
ML4GW_PROJECT_STRUCTURE = [
    "data",
    "models", 
    "logs",
    "outputs",
    "configs",
    "checkpoints",
    "plots",
    "results",
]


__all__ = [
    "setup_logging",
    "load_config", 
    "save_config",
    "get_jax_device_info",
    "print_system_info",
    "validate_array_shape",
    "create_directory_structure",
    "ML4GW_PROJECT_STRUCTURE",
]
```

## Statistics

- Total Files: 18
- Total Characters: 140943
- Total Tokens: 0
````

## Statistics

- Total Files: 19
- Total Characters: 284541
- Total Tokens: 0
